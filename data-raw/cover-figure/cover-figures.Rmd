
```{r}
library(tidyverse)

update_geom_defaults("point", list(color = "#566675"))

read_rds("cover.rds") +
  theme_minimal() +
  labs(title = NULL,
       subtitle = NULL,
       x = NULL,
       y = NULL) +
  theme(axis.text = element_blank(), panel.grid.major = element_line(colour = "#0e0f1c"), panel.grid.minor = element_line(colour = "#151526"))

ggsave("cover-right.png", bg = "transparent", width = 16, height = 16)
```

```{r}
library(ggpage)

text <- "Modeling as a statistical practice can encompass a wide variety of activities. 
This book focuses on supervised or predictive modeling for text, using text data 
to make predictions about the world around us. We use the tidymodels framework 
for modeling, a consistent and flexible collection of R packages developed to 
encourage good statistical practice.

Supervised machine learning using text data involves building a statistical 
model to estimate some output from input that includes language. The two types 
of models we train in this book are regression and classification. Think of 
regression models as predicting numeric or continuous outputs, such as 
predicting the year of a United States Supreme Court opinion from the text of 
that opinion. Think of classification models as predicting outputs that are 
discrete quantities or class labels, such as predicting whether a GitHub issue 
is about documentation or not from the text of the issue. Models like these can
be used to make predictions for new observations, to understand what features 
or characteristics contribute to differences in the output, and more. We can 
evaluate our models using performance metrics to determine which are best, which 
are acceptable for our specific context, and even which are fair.

Natural language that we as speakers and/or writers use must be dramatically 
transformed to a machine-readable, numeric representation to be ready for 
computation. In this book, we explore typical text preprocessing steps from the 
ground up, and consider the effects of these steps. We also show how to fluently 
use the textrecipes R package to prepare text data within a modeling pipeline.

@Silge2017 provides a practical introduction to text mining with R using tidy 
data principles, based on the [tidytext](https://github.com/juliasilge/tidytext)
package. If you have already started on the path of gaining insight from your 
text data, a next step is using that text directly in predictive modeling. Text
data contains within it latent information that can be used for insight,
understanding, and better decision-making, and predictive modeling with text can 
bring that information and insight to light. If you have already explored how to
analyze text as demonstrated in @Silge2017, this book will move one step further
to show you how to *learn and make predictions* from that text data with 
supervised models. If you are unfamiliar with this previous work, this book will
still provide a robust introduction to how text can be represented in useful 
ways for modeling and a diverse set of supervised modeling approaches for text.

## Outline {-}

The book is divided into three sections. We make a (perhaps arbitrary) 
distinction between *machine learning methods* and *deep learning methods* by 
defining deep learning as any kind of multi-layer neural network (LSTM, bi-LSTM,
CNN) and machine learning as anything else (regularized regression, naive Bayes,
SVM, random forest). We make this distinction both because these different 
methods use separate software packages and modeling infrastructure, and from a 
pragmatic point of view, it is helpful to split up the chapters this way. 

- **Natural language features:** How do we transform text data into a 
representation useful for modeling? In these chapters, we explore the most 
common preprocessing steps for text, when they are helpful, and when they are 
not.

- **Machine learning methods:** We investigate the power of some of the simpler 
and more lightweight models in our toolbox.

- **Deep learning methods:** Given more time and resources, we see what is 
possible once we turn to neural networks. 

Some of the topics in the second and third sections overlap as they provide 
different approaches to the same tasks.

Throughout the book, we will demonstrate with examples and build models using a 
selection of text data sets. A description of these data sets can be found in 
Appendix

## Topics this book will not cover {-}

This book serves as a thorough introduction to prediction and modeling with 
text, along with detailed practical examples, but there are many areas of 
natural language processing we do not cover. The CRAN Task View on Natural 
Language Processing provides details on other ways to use R for computational 
linguistics. Specific topics we do not cover include:

- **Reading text data into memory:** Text data may come to a data practitioner 
in any of a long list of heterogeneous formats. Text data exists in PDFs, 
databases, plain text files (single or multiple for a given project), websites, 
APIs, literal paper, and more. The skills needed to access and sometimes wrangle
text data sets so that they are in memory and ready for analysis are so varied 
and extensive that we cannot hope to cover them in this book. We point readers 
to R packages such as readr [@R-readr], pdftools [@R-pdftools], and httr 
[@R-httr], which we have found helpful in these tasks.

- **Unsupervised machine learning for text:** @Silge2017 provide an introduction
to one method of unsupervised text modeling, and Chapter does
dive deep into word embeddings, which learn from the latent structure in text 
data. However, many more unsupervised machine learning algorithms can be used 
for the goal of learning about the structure or distribution of text data when 
there are no outcome or output variables to predict.

- **Text generation:** The deep learning model architectures we discuss in
Chapters can be used to generate 
new text, as well as to model existing text. @Chollet2018 provide details on 
how to use neural network architectures and training data for text generation.

- **Speech processing:** Models that detect words in audio recordings of speech 
are typically based on many of the principles outlined in this book, but the 
training data is _audio_ rather than written text. R users can access 
pre-trained speech-to-text models via large cloud providers, such as Google"
```

```{r}
library(tidytext)
library(tidyverse)
idf_weight <- fs::dir_ls(regexp = "^[0-9]") %>%
  imap_dfr( ~ tibble(text = read_lines(.x)) %>%
              unnest_tokens(token, text), .id = "chapter") %>%
  count(token, chapter) %>%
  tidytext::bind_tf_idf(token, chapter, n) %>%
  select(word = token, idf) %>%
  distinct()

tibble(text = text) %>%
  unnest_tokens(text, text, token = function(x) str_split(x, "\n")) %>%
  mutate(text = if_else(str_length(text) == 0, "a a", text)) %>%
  ggpage_build() %>%
  left_join(idf_weight, by = "word") %>%
  ggpage_plot(mapping = aes(fill = idf, alpha = 0.5)) +
  theme(axis.text = element_blank()) +
  guides(fill = "none", alpha = "none") +
  scale_fill_viridis_c(option = "G", begin = 0.2, end = 0.8, na.value = viridisLite::viridis(5, option = "G")[2])

ggsave("cover-left.png", bg = "transparent", width = 16, height = 16)
```
