# Dense neural networks {#dldnn}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE, eval = TRUE,
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
options(crayon.enabled = FALSE)
doParallel::registerDoParallel()

## for Julia's local environment
#spacyr::spacy_initialize(condaenv = "r-spacyr", entity = FALSE)
#online <- FALSE

## for GH actions
online <- TRUE
```

These chapters on deep learning are broken up by network architecture instead of by outcome as we did in Chapters \@ref(mlclassification) and \@ref(mlregression).
We'll use Keras [@R-keras] with its Tensorflow backend for these deep learning models; Keras is a well-established framework for deep learning with bindings in Python and, via reticulate [@R-reticulate], R.
Keras provides extensive support for creating and training many kinds of deep learning models, but less support for resampling and preprocessing. Throughout this and the next chapters, we will demonstrate how to use tidymodels packages together with Keras to address these tasks. 

```{block, type = "rmdnote"}
The tidymodels framework is modular, so we can use it for certain tasks without committing to it entirely, when appropriate.
```


This chapter explores one of the most straightforward configurations for a deep learning model, a densely connected neural network. This is typically not a model that will achieve the highest performance on text data, but it is a good place to start to understand the process of building and evaluating deep learning models for text.

## Kickstarter data {#kickstarter}

For all our chapters on deep learning, we will build binary classification models, much like we did in Chapter \@ref(mlclassification), but we will use neural networks instead of shallow learning models. As we discussed in the [preface to these deep learning chapters](https://smltar.com/dlforeword.html), much of the overall model process will look the same, but we will use a different kind of algorithm. We will use a dataset of descriptions or "blurbs" for campaigns from the crowdfunding platform [Kickstarter](https://www.kickstarter.com/).

```{r kickstarter}
library(tidyverse)

kickstarter <- read_csv("data/kickstarter.csv.gz")
kickstarter
```

The text for these models, contained in blurb, are short, less than a few hundred characters. What is the distribution of characters?

```{r kickstartercharhist, dependson="kickstarter", fig.cap="Distribution of character count for Kickstarter campaign blurbs"}
kickstarter %>%
  ggplot(aes(nchar(blurb))) +
  geom_histogram(binwidth = 1, alpha = 0.8) +
  labs(x = "Number of characters per campaign blurb",
       y = "Number of campaign blurbs")
```

Figure \@ref(fig:kickstartercharhist) shows that the distribution of characters per blurb is right-skewed, with thresholds. Individuals creating campaigns don't have much space to make an impression, so most people choose to use most of it! There is an oddity in this chart, a steep drop somewhere between 130 and 140 with another threshold around 150 characters. Let's investigate to see if we can find the reason.

We can use `count()` to find the most common blurb length.

```{r}
kickstarter %>%
  count(nchar(blurb), sort = TRUE)
```

Let's use our own eyes to see what happens around this cutoff point. We can use `slice_sample()` to draw a few random blurbs.

Were the blurbs truncated at 135 characters? Let's look at some blurbs with exactly 135 characters.

```{r}
set.seed(1)
kickstarter %>%
  filter(nchar(blurb) == 135) %>%
  slice_sample(n = 5) %>%
  pull(blurb)
```

All of these blurbs appear coherent and some of them even end with a period to end the sentence. Let's now look at blurbs with more than 135 characters to see if they are different.

```{r}
set.seed(1)
kickstarter %>%
  filter(nchar(blurb) > 135) %>%
  slice_sample(n = 5) %>%
  pull(blurb)
```

All of these blurbs also look fine so the strange distribution doesn't seem like a data collection issue. 

The `kickstarter` dataset also includes a `created_at` variable; let's explore that next. Figure \@ref(fig:kickstarterheatmap) is a heatmap of the lengths of blurbs and the time the campaign was posted.

```{r kickstarterheatmap, dependson="kickstarter", fig.cap="Distribution of character count for Kickstarter campaign blurbs over time"}
kickstarter %>%
  ggplot(aes(created_at, nchar(blurb))) +
  geom_bin2d() +
  labs(x = NULL,
       y = "Number of characters per campaign blurb")
```

That looks like the explanation! It appears that at the end of 2010 there was a policy change in the blurb length, shortening from 150 characters to 135 characters.

```{r}
kickstarter %>%
  filter(nchar(blurb) > 135) %>%
  summarise(max(created_at))
```

We can't say for sure if the change happened on 2010-10-20, but that is the last day a campaign was launched with more than 135 characters.

## A first deep learning model {#firstdlclassification}

Like all our previous modeling, our first step is to split our data into training and testing sets. We will still use our training set to build models and save the testing set for a final estimate of how our model will perform on new data.  

```{block, type = "rmdwarning"}
It is very easy to overfit deep learning models, so an unbiased estimate of future performance from a test set is more important than ever.
```

We use `initial_split()` to define the training and testing splits. We will focus on modeling the blurb alone in these deep learning chapters. Also, we will restrict our modeling analysis to only include blurbs with more than 15 characters, because the shortest blurbs tend to consist of uninformative single words.

```{r}
library(tidymodels)
set.seed(1234)
kickstarter_split <- kickstarter %>%
  filter(nchar(blurb) >= 15) %>%
  initial_split()

kickstarter_train <- training(kickstarter_split)
kickstarter_test <- testing(kickstarter_split)
```

There are `r scales::comma(nrow(kickstarter_train))` blurbs in the training set and `r scales::comma(nrow(kickstarter_test))` in the testing set.

### Preprocessing for deep learning

Preprocessing for deep learning models is different than preprocessing for most other text models. These neural networks model _sequences_, so we have to choose the length of sequences we would like to include. Documents that are longer than this length are truncated (information is thrown away) and documents that are shorter than this length are padded with zeroes (an empty, non-informative value) to get to the chosen sequence length. This sequence length is a hyperparameter of the model and we need to select this value such that we don't: 

- overshoot and introduce a lot of padded zeroes which would make the model hard to train, or 
- undershoot and cut off too much informative text from our documents.

We can use the `count_words()` function from the tokenizers package to calculate the number of words and generate a histogram. Notice how we are only using the training dataset to avoid data leakage when selecting this value.

```{r kickstarterwordlength, fig.cap="Distribution of word count for Kickstarter campaign blurbs"}
kickstarter_train %>% 
  mutate(n_words = tokenizers::count_words(blurb)) %>%
  ggplot(aes(n_words)) +
  geom_bar() +
  labs(x = "Number of words per campaign blurb",
       y = "Number of campaign blurbs")
```

Given that we don't have many words for this particular dataset to begin with, let's err on the side of longer sequences so we don't lose valuable data. Let's try 30 words for our threshold `max_length`, and let's include 20,000 words in our vocabulary.

```{r prepped_recipe}
library(textrecipes)

max_words <- 2e4
max_length <- 30

kick_rec <- recipe(~blurb, data = kickstarter_train) %>%
  step_tokenize(blurb) %>%
  step_tokenfilter(blurb, max_tokens = max_words) %>%
  step_sequence_onehot(blurb, sequence_length = max_length) 

kick_rec
```

This preprocessing recipe tokenizes our text (Chapter \@ref(tokenization)) and filters to keep only the top 20,000 words, but then it transforms the tokenized text in a new way to prepare for deep learning that we have not used in this book before, using `step_sequence_onehot()`.

### One-hot sequence embedding of text

The function `step_sequence_onehot()` transforms tokens into a numeric format appropriate for modeling, like `step_tf()` and `step_tfidf()`. However, it is different in that it takes into account the order of the tokens, unlike `step_tf()` and `step_tfidf()` which do not take order into account. 

```{block, type = "rmdnote"}
Steps like `step_tf()` and `step_tfidf()` are used for approaches called "bag of words", meaning the words are treated like they are just thrown in a bag without attention paid to their order. 
```

Let's take a closer look at how `step_sequence_onehot()` works and how its parameters will change the output.

When we use `step_sequence_onehot()`, two things happen. First, each word is assigned an _integer index_. You can think of this as a key-value pair of the vocabulary. Next, the sequence of tokens is replaced with the corresponding indices; this sequence of integers makes up the final numeric representation. Let's illustrate with a small example:

```{r sequence_onhot_rec}
small_data <- tibble(
  text = c("Adventure Dice Game",
           "Spooky Dice Game",
           "Illustrated Book of Monsters",
           "Monsters, Ghosts, Goblins, Me, Myself and I")
  )

small_spec <- recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "") 

prep(small_spec)
```

```{block, type = "rmdwarning"}
What does the function `prep()` do? Before when we have used recipes, we put them in a `workflow()` which handles low-level processing. The `prep()` will compute or estimate statistics from the training set; the output of `prep()` is a prepped recipe. 
```

Once we have the prepped recipe then we can `tidy()` it to extract the vocabulary, represented in the `vocabulary` and `token` columns^[The `terms` columns refer to the column we have applied `step_sequence_onehot()` to and `id` is its unique identifier. Note that textrecipes allows `step_sequence_onehot()` to be applied to multiple text variables independently and they will have their own vocabularies.].

```{r sequence_onhot_rec_vocab, dependson="sequence_onhot_rec"}
prep(small_spec) %>%
 tidy(2)
```

If we take a look at the resulting matrix, we have one row per observation. The first row starts with some padded zeroes but then contains 1, 4, and 5, which we can use together with the vocabulary to construct the original sentence.

```{r sequence_onhot_rec_matrix1, dependson="sequence_onhot_rec"}
prep(small_spec) %>%
  bake(new_data = NULL, composition = "matrix")
```

```{block, type = "rmdwarning"}
When we `bake()` a prepped recipe, we apply the preprocessing to a dataset. We can get out the training set that we started with by specifying `new_data = NULL` or apply it to another set via `new_data = my_other_data_set`. The output of `bake()` is a dataset like a tibble or a matrix, depending on the `composition` argument.
```

But wait, the 4th line should have started with an 11 since the sentence starts with "monsters"! The entry in `_text_1` is 6 instead. This is happening because the sentence is too long to fit inside the specified sequence length. We must answer three questions before using `step_sequence_onehot()`:

1.  How long should the output sequence be?
2.  What happens to sequences that are too long?
3.  What happens to sequences that are too short?

Choosing the right sequence length is a balancing act. You want the length to be long enough such that you don't truncate too much of your text data, but still short enough to keep the size of the data manageable and to avoid excessive padding. Truncating, having large training data, and excessive padding all lead to worse model performance. This parameter is controlled by the `sequence_length` argument in `step_sequence_onehot()`. 

If the sequence is too long, then it must be truncated. This can be done by removing values from the beginning (`"pre"`) or the end (`"post"`) of the sequence. This choice is mostly influenced by the data, and you need to evaluate where most of the useful information of the text is located. News articles typically start with the main points and then go into detail. If your goal is to detect the broad category, then you may want to keep the beginning of the texts, whereas if you are working with speeches or conversational text, then you might find that the last thing to be said carries more information. 

Lastly, we need to decide how to pad a document that is too short. Pre-padding tends to be more popular, especially when working with RNN and LSTM models (Chapter \@ref(dllstm)) since having post-padding could result in the hidden states getting flushed out by the zeroes before getting to the text itself (Section \@ref(lstmpadding)).

The defaults for `step_sequence_onehot()` are `sequence_length = 100`, `padding = "pre"`, and `truncating = "pre"`. If we change the truncation to happen at the end with:

```{r sequence_onhot_rec_matrix2, dependson="sequence_onhot_rec"}
recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "", 
                       padding = "pre", truncating = "post") %>%
  prep() %>%
  bake(new_data = NULL, composition = "matrix")
```

then we see the 11 at the beginning of the last row representing the "monsters". The starting points are not aligned since we are still padding on the left side. We can left-align all the sequences by setting `padding = "post"`.

```{r sequence_onhot_rec_matrix3, dependson="sequence_onhot_rec"}
recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "", 
                       padding = "post", truncating = "post") %>%
  prep() %>%
  bake(new_data = NULL, composition = "matrix")
```

Now we have that all digits representing the first characters neatly aligned in the first column.

Let's now prepare and apply our feature engineering recipe `kick_rec` so we can use it in for our deep learning model.

```{r}
kick_prep <-  prep(kick_rec)
kick_train <- bake(kick_prep, new_data = NULL, composition = "matrix")
dim(kick_train)
```

The matrix `kick_train` has `r comma(dim(kick_train)[1])` rows for the rows of the training data and `r dim(kick_train)[2]` columns, corresponding to our chosen sequence length.


### Simple Flattened Dense network

The model we will be starting with a model that embeds sentences in sequences of vectors, flattening them, and then trains a dense layer on top.

```{r dense_model}
library(keras)

dense_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, 
                  output_dim = 12,
                  input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

dense_model
```

Let us step through this model specification one layer at a time.
We start the keras model by using `keras_model_sequential()` to indicate that we want to compose a linear stack of layers.
Our first layer is an embedding layer via `layer_embedding()` This layer is e equipped to handle preprocessed data we have in `prepped_training`. It will take each observation/row in `prepped_traning` and embed each token to an embedding vector. This will result in each observation being turned into an (embedding_dim x sequence_length) matrix witch would be a (12 x 30) matrix with our settings, creating a (number of observations x embedding_dim x sequence_length) tensor.
The `layer_flatten()` layer that follows takes the 2-dimensional tensors for each observation and flattens it down into 1 dimension. This will create a `30 * 12 = 360` tensor for each observation. Lastly, we have 2 densely connected layers with the last layer having a sigmoid activation function to give us a number between 0 and 1.

Now that we have specified the architecture of the model we still have a couple of things left to add to the model before we can fit it to the data. A keras model requires an optimizer and a loss function to be able to compile. When the neural network finished passing a batch of data through the network it needs to find a way to use the difference between the predicted values and true values to update the weights. the algorithm that determines those weights is known as the optimization algorithm. keras comes pre-loaded with many optimizers^[https://keras.io/api/optimizers/] and you can even create custom optimizers if what you need isn't on the list. We will start by using the rmsprop optimizer.

```{block, type = "rmdnote"}
An optimizer can either be set with the name of the optimizer as a character or by supplying the function `optimizer_*()` where `*` is the name of the optimizer. If you use the function then you can specify parameters for the optimizer.
```

During training, we need to calculate a quantity that we want to have minimized. This is the loss function, keras comes pre-loaded with many loss functions^[https://keras.io/api/losses/]. These loss function will typically take in two values, typically the true value and the predicted value, and return a measure of how close they are. 
Since we are working on a binary classification task and have the final layer of the network return a probability, then we find binary cross-entropy to be an appropriate loss function. Binary cross-entropy does well at dealing with probabilities as it measures the “distance” between probability distributions. which would in our case be between the ground-truth distribution and the predictions.

We can also add any number of `metrics`^[https://keras.io/api/metrics/] to be calculated and reported during training. These metrics will not affect the training loop which is controlled by the optimizer and loss function. The metrics job is to report back a single number that will inform you of the user how well the model is performing. We will select accuracy as our metric for now. We can now set these 3 options; optimizer, loss, and metrics using the `compile()` function

```{r dense_model_compiled}
dense_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{block, type = "rmdnote"}
Notice how the `compile()` function modifies the network in place. This is different then what is conventionally done in R where a new network object would have been returned.
```

Finally, we can fit the model. When we `fit()` a keras model we need to supply it with the data we are having the model train on. We need to supply this a matrix of predictors `x` and a numeric vector of labels `y`.
This is sufficient information to start training the model. We are going to specify a couple more arguments to get better control of the training loop. First, we set the number of observations to pass through at a time with `batch_size`, and we set `epochs = 20` to tell the model to pass all the data through the training loop 20 times. Lastly, we set `validation_split = 0.2` to specify an internal validation split for when the metrics are calculated.

```{r dense_model_history}
dense_history <- dense_model %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 20,
  validation_split = 0.2
)
```

We can visualize the results of the training loop by `plot()`ing the `dense_history`.

```{r dense_model_history_plot, fig.cap="Training and validation metrics for dense network"}
plot(dense_history)
```

Now that we have the fitted model can we apply the model to our testing dataset to see how well the model performs on data it hasn't seen.

```{r dense_model_evaluate}
dense_model %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

we see that the accuracy very closely resembles the val_accuracy from the training loop, suggesting that we didn't overfit our model.

### Evaluate Keras predictions {#evaluate-keras-predictions}

At the end of the last sub-section we used `evaluate()` to see how well our model is performing.
If you want to use the yardstick package we would have to make some transformations since yardstick needs its input to be data.frames and keras mostly works with matrices and arrays.
The following function create a little brigde between the two frameworks. It allows us to combine a keras model, recipes recipe along with the data, and return the predictions in a tibble format.

```{r}
library(dplyr)
keras_predict <- function(model, recipe, data, response) {
  baked_data <- bake(recipe, data, composition = "matrix")
  prediction <- predict(model, baked_data)[, 1]
  
  data %>%
    mutate(
      .pred_prop = prediction,
      .pred_class = if_else(.pred_prop < 0.5, 0, 1)
    ) %>%
  mutate(across(c({{ response }}, .pred_class), 
                ~ factor(.x, levels = c(1, 0))))
}
```

It is worth noting that this function only works with simple binary classification models that take in its data as a matrix and return a single probability for each observation.
The function is in addition to returning the predicted probability also returning the predicted class based on a 50% cut-off. The function is also making sure that the class and predicted class are both factors with matching levels.

Using this function will allow us to get prediction results that seamlessly connects with yardstick.

```{r}
dense_model_predictions <- keras_predict(
    dense_model, 
    prepped_recipe, 
    kickstarter_test, 
    state
  )

dense_model_predictions
```

We can calculate the standard metrics with `matrics()` 

```{r}
library(yardstick)
metrics(dense_model_predictions, state, .pred_class)
```

and we see that they match the results we got from `evaluate()`. Since we have access to yardsticks full capacities we can also calculates confusion matrices and ROC curves.
Looking at the heatmap in Figure \@ref(fig:dnnheatmap), we are happy to see that there isn't any glaring bias in how the model performs. The model is perfect, it is still only working on around a little over `r scales::percent_format()(floor(100 * accuracy(dense_model_predictions, state, .pred_class)$.estimate) / 100)`, bu is is more or less evenly god at prediction both classes.

```{r dnnheatmap, dependson="nbrs", fig.cap="Confusion matrix for first DNN model predictions of Kickstarter state"}
dense_model_predictions %>%
  conf_mat( state, .pred_class) %>%
  autoplot(type = "heatmap")
```

The ROC curve in Figure \@ref(fig:dnnroccurve) also looks just as good as we would suspect from the other metrics we have calculated so far. Nothing alarming is standing out for us.

```{r dnnroccurve, fig.width=7, fig.height=6, fig.cap="ROC curve for first DNN model predictions of Kickstarter state"}
dense_model_predictions %>%
  roc_curve(truth = state, .pred_prop) %>%
  autoplot() +
  labs(
    title = "Receiver operator curve for Kickstarter blurbs"
  )
```


## Using pre-trained word embeddings

In the last section did we include an embedding layer, and we let the model train the embedding along with it. This is not the only way to handle this task. In chapter \@ref(embeddings) we looked at how embeddings are created and how they are used. Instead of having the embedding layer start at random and have it being trained alongside the other parameters, let us try to supply them our self. 

We start by getting a pre-trained embedding. The glove embedding that we used in section \@ref(glove) will work for now. Setting `dimensions = 50` and only selecting the first 12 dimensions will make it easier for us to compare models.

```{r eval=FALSE}
library(textdata)

glove6b <- embedding_glove6b(dimensions = 50) %>% select(1:13)
```

```{r glove6b12d, echo=FALSE, R.options = list(tibble.max_extra_cols=9, tibble.print_min=10, tibble.width=80)}
load("data/glove6b.rda")
glove6b <- glove6b[, 1:13]
glove6b
```

The `embedding_glove6b()` function returns a tibble which isn't the right format for what keras expects. Also, take notice of how many rows are present in this embedding. Far more than what the trained recipe is expecting to return. The vocabulary can be extracted from the trained recipe using `tidy()`. First, we apply `tidy()` to `prepped_recipe` to get the list of steps that the recipe contains.

```{r, dependson="prepped_recipe"}
tidy(prepped_recipe)
```

We see that the 3rd step is the `sequence_onhot` step, so by setting `number = 3` can we extract the vocabulary of transformation.

```{r, dependson="prepped_recipe"}
tidy(prepped_recipe, number = 3)
```

This list of tokens can then be `left_join()`ed to the `glove6b` embedding tibble to only keep the tokens of interest. Any tokens from the vocabulary not found in `glove6b` is replaced with 0 using `mutate_all()` and `replace_na()`. The results are turned into a matrix, and a row of zeroes is added at the top of the matrix to account for the out-of-vocabulary words.

```{r glove6b_matrix, dependson=c("glove6b12d", "prepped_recipe")}
glove6b_matrix <- tidy(prepped_recipe, 3) %>%
  select(token) %>%
  left_join(glove6b, by = "token") %>%
  mutate_all(replace_na, 0) %>%
  select(-token) %>%
  as.matrix() %>%
  rbind(0, .)
```

The way the model is constructed will remain as unchanged as possible. We make sure that the `output_dim` argument is being set to be equal to` ncol(glove6b_matrix)`, this is a way to make sure that all the dimensions will line up nicely. Everything else stays the same.

```{r dense_model_pte, dependson="glove6b_matrix"}
dense_model_pte <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, 
                output_dim = ncol(glove6b_matrix),
                input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Now we use `get_layer()` to access the first layer which is the embedding layer, then we set the weights with `set_weights()` and lastly we freeze the weights with `freeze_weights()`. 
Freezing the weights stops them from being updated during the training loop.

```{r dense_model_pte_weights, dependson="dense_model_pte"}
dense_model_pte %>%
  get_layer(index = 1) %>%
  set_weights(list(glove6b_matrix)) %>%
  freeze_weights()
```

Now we will compile and fit the model just like the last one we looked at.

```{r dense_pte_history, dependson="dense_model_pte_weights"}
dense_model_pte %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

dense_pte_history <- dense_model_pte %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 20,
  validation_split = 0.2
)
```

This model is not performing as well as the previous model and the evaluation isn't that much better.

```{r dense_pte_evaluate, dependson="dense_pte_history"}
dense_model_pte %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

Why is this happening? Part of the training loop is about adjusting the weights in the network. 
When we froze the weights in this network it appears that we froze them at values that did not perform very well. 
This pre-trained glove embedding[@Pennington2014] we are using have been trained on a Wikipedia dump and [Gigaword 5](https://catalog.ldc.upenn.edu/LDC2011T07) which is a comprehensive archive of newswire text. 
The text contained on Wikipedia and in new articles both follows certain styles and semantics.
Both will tend to be written formally and in the past tense. 
They also contain longer and complete sentences. 
There are many more distinct features of both Wikipedia text and news articles, but the important part is how similar they are to the data we are trying to use.
These text fields are very short, lack punctuation, stop words, narrative, and tense. Many of them simply try to pack as many buzz words in as possible while keeping the sentence readable.
It is not surprising that the word embedding doesn't perform well in this model since the text it is trained on is so far removed from the text is it being applied on.

Although this didn't work that well, doesn't mean that using pre-trained word embeddings are useless.
Sometimes they can perform very well, the important part is how well the embedding fits the data you are using.
there is one more way we can use this embedding in our network, we can load it in as before but not freeze the weights.
This allows the models to still adjust the weights to better fit the data, and the hope is that this pre-trained embedding delivers a better starting point than the randomly generated embedding we get if we don't set the weights.

We specify a new model

```{r dense_model_pte2, dependson="glove6b_matrix"}
dense_model_pte2 <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, 
                output_dim = ncol(glove6b_matrix),
                input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

set the weights with `set_weights()` but we don't freeze them

```{r dense_model_pte_noweights, dependson="dense_model_pte"}
dense_model_pte2 %>%
  get_layer(index = 1) %>%
  set_weights(list(glove6b_matrix))
```

and we compile and fit the model as we did last time

```{r dense_pte_noweights_history, dependson="dense_model_pte_noweights"}
dense_model_pte2 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

dense_pte2_history <- dense_model_pte2 %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 40,
  validation_split = 0.2
)
```

```{r dense_pte_noweights_evaluate, dependson="dense_pte_noweights_history"}
dense_model_pte2 %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

This performs quite a bit better than when we froze the weights.
However, it is training slower than when we didn't set weights since we had to run it for around 40 epochs before we start to overfit.

```{block, type = "rmdnote"}
If you have enough corpus data in the field you are working on, then it would be worth considering training a word embedding that better captures the structure of the domain you are trying to work with.
```

We can combine all the predictions on these last 3 models to more easily compare the results between them

```{r}
all_dense_model_predictions <- bind_rows(
  keras_predict(
    dense_model, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "dense"),
  keras_predict(
    dense_model_pte, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "pte (locked weights)"),
  keras_predict(
    dense_model_pte2, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "pte (not locked weights)")
)
```

Now that the results are combined in `all_dense_model_predictions` we can calculate group-wise evaluation statistics by grouping by the `model` variable.

```{r}
all_dense_model_predictions %>%
  group_by(model) %>%
  metrics(state, .pred_class)
```

We can also do this for ROC-curves. Figure \@ref(fig:alldnnroccurve) shows the 3 different ROC-curves together in 1 chart. As we know the model using pre-trained word embeddings with locked weights didn't perform very well and its ROC-curve is the lowest of the 3. The other two models perform more similar but the self-trained model ends up beating the pre-trained model after all.

```{r alldnnroccurve, fig.width=7, fig.height=6, fig.cap="ROC curve for all DNN models' predictions of Kickstarter state"}
all_dense_model_predictions %>%
  group_by(model) %>%
  roc_curve(truth = state, .pred_prop) %>%
  autoplot() +
  labs(
    title = "Receiver operator curve for Kickstarter blurbs"
  )
```

## Limitations of deep learning

Deep learning models can offer excellent performance on many tasks and the flexibility and potential complexity of their architecture is part of the reason why. One of the main downsides of deep learning models is that the interpretability of the models themselves is rather poor, especially for DNNs.
This means that practitioners that work in fields where interpretability is vital, such as health care, shy away from deep learning models since they are hard to understand and interpret.
Another limitation of deep learning models is that they do not facilitate a comprehensive theoretical understanding of learning or their inner organization[@shwartzziv2017opening].
These two points together lead to what is commonly called "black box" models[@shrikumar2019learning], models where is it hard to peek into the inner working to understand how they work.
The issue of not being able to reason about the inner workings of a model means that we will have a hard time explaining why a model is working well. But it also means it will be hard to remedy a biased model that performs well in some settings but bad in other settings.
This is a problem since it can hide biases from the training set which may lead to unfair, wrong, or even illegal decisions based on protected classes[@guidotti2018survey].
Lastly, deep learning models tend to require more training data than traditional statistical machine learning methods. This means that that it can be hard to train a deep learning model if you have a very small data set[@lampinen2018oneshot].

## Summary {#dldnnsummary}

You can use classification modeling to predict labels or categorical variables from a dataset, including datasets that include text.
Deep neural networks can be used to fit classification models that work on text features.
Although these models have many parameters they are able to adequately train.
We also saw how we can tokenize in a way that depicts the order of the tokens in the text. Doing this can give the model the possibility of finding patterns in the order. This is not possible in the models we saw in chapter \@ref(mlclassification) and \@ref(mlregression) since these models only take a 2 dimensional matrix as input.
We gave up some of the fine control over feature engineering in the hope that the network would be able to learn the features themselves.
The feature engineering is not completely out of our hands since we can still affect the tokenization and normalization before the tokens are being passed into the network.

### In this chapter, you learned:

- how to build and train a deep neural network with keras
- How to tokenize text in a way that retains the order of the tokens
- how to train word embeddings alongside your model
- about resampling strategies for deep learning models
- about how deep learning models have low interpretability
- How to use pre-trained word embeddings in a neural network

```{r echo=FALSE}
knitr::knit_exit()
```

```{r eval = FALSE}
library(hardhat)
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")

kickstarter_train <- kickstarter_train %>%
  mutate(state = as.factor(state))

## baseline lasso model
set.seed(123)
kickstarter_folds <- vfold_cv(kickstarter_train)

kickstarter_rec <- recipe(state ~ blurb, data = kickstarter_train) %>%
  step_tokenize(blurb) %>%
  step_tokenfilter(blurb, max_tokens = 5e3) %>%
  step_tfidf(blurb)

kickstarter_rec

lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
lasso_spec

lambda_grid <- grid_regular(penalty(), levels = 20)
lambda_grid

kickstarter_wf <- workflow() %>%
  add_recipe(kickstarter_rec, blueprint = sparse_bp) %>%
  add_model(lasso_spec)

kickstarter_wf

doParallel::registerDoParallel()
set.seed(2020)
lasso_rs <- tune_grid(
  kickstarter_wf,
  kickstarter_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE, verbose = TRUE)
)

autoplot(lasso_rs)

show_best(lasso_rs, "roc_auc") # 0.7519311

show_best(lasso_rs, "accuracy") # 0.6840861
```