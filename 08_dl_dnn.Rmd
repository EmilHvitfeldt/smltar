# Deep Neural Network {#dldnn}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE, eval = TRUE,
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
options(crayon.enabled = FALSE)
doParallel::registerDoParallel()

## for Julia's local environment
#spacyr::spacy_initialize(condaenv = "r-spacyr", entity = FALSE)
#online <- FALSE

## for GH actions
online <- TRUE
```

This is the first chapter in the deep learning section of this book. These chapters are broken up by model architecture instead of by outcome as we did in chapter \@ref(mlclassification) and \@ref(mlregression).
This chapter will explore the relatively simple deep neural networks. This chapter will be our first look at using keras [@R-keras]. keras is a well-established framework for deep learning with bindings in Python and R.
While keras provides a nice framework for setting up deep learning models, its tools for resampling and preprocessing are not very tidy so we will be using the tidymodels packages for these tasks. This is a good example of the modularity of the tidymodels framework, we can just use it for the tasks we need without having to fully commit.

## Kickstarter data {#kickstarter}

In this chapter, we will predict binary values, much like we did in Chapter \@ref(mlclassification), but we will use deep learning methods instead. We will be using a dataset of fundraising campaigns from [Kickstarter](https://www.kickstarter.com/).

```{r kickstarter}
library(tidyverse)

kickstarter <- read_csv("data/kickstarter.csv.gz")
kickstarter
```

we are working with fairly short texts for this dataset. less than a couple of hundred characters. We can look at the distribution

```{r kickstartercharhist, dependson="kickstarter", fig.cap="Distribution of character count for Kickstarter campaign blurbs"}
kickstarter %>%
  ggplot(aes(nchar(blurb))) +
  geom_histogram(binwidth = 1) +
  labs(x = "Number of characters per campaign blurb",
       y = "Number of campaign blurbs")
```

it is rightly skewed which is to be expected. Since you don't have much space to make your impression most people choose to use most of it. There is one odd thing happening in this chart. There is a drop somewhere between 130 and 140. Let us investigate to see if we can find the reason.

We can use `count()` to find the most common `blurb` length.

```{r}
kickstarter %>%
  count(nchar(blurb), sort = TRUE)
```

it appears to be 135 which in and of itself doesn't tell us much. It might be a glitch in the data collection process. Let us put our own eyes to look at what happens around this cutoff point. We can use `slice_sample()` to draw a random sample of the data.

We start by looking at `blurb`s with exactly 135 characters, this is done so that we can identify if the `blurb`s were cut short at 135 characters.

```{r}
set.seed(1)
kickstarter %>%
  filter(nchar(blurb) == 135) %>%
  slice_sample(n = 5) %>%
  pull(blurb)
```

It doesn't appear to be the case as all of these `blurb`s appear coherent and some of them even end with a period to end the sentence. Let us now look at `blurb`s with more than 135 characters if these are different.

```{r}
set.seed(1)
kickstarter %>%
  filter(nchar(blurb) > 135) %>%
  slice_sample(n = 5) %>%
  pull(blurb)
```

All of these `blurb`s also look good so it doesn't look like a data collection issue. The `kickstarter` dataset also includes a `created_at` variable. Let us see what we can gather with that new information.

Below is a heatmap of the lengths of `blurb`s and the time the campaign was posted.

```{r kickstarterheatmap, dependson="kickstarter", fig.cap="Distribution of character count for Kickstarter campaign blurbs over time"}
kickstarter %>%
  ggplot(aes(created_at, nchar(blurb))) +
  geom_bin2d() +
  labs(x = NULL,
       y = "Number of characters per campaign blurb")
```

We see a trend right here. it appears that at the end of 2010 there was a policy change to have the blurb length shortened from 150 characters to 135 characters.

```{r}
kickstarter %>%
  filter(nchar(blurb) > 135) %>%
  summarise(max(created_at))
```

We can't tell for sure if the change happened in 2010-10-20, but that is the last day a campaign was launched with more than 135 characters.

## A first classification model {#firstdlclassification}

Much like all our previous modeling, our first step is to split our data into training and testing sets. We will still use our training set to build models and save the testing set for a final estimate of how our model will perform on new data. It is very easy to overfit deep learning models, so an unbiased estimate of future performance from a test set is more important than ever. This data will be hard to work with since we don't have much information to work with.

We use `initial_split()` to define the training/testing split. We will focus on modeling the `blurb` alone in this chapter. We will restrict the data to only include `blurb`s with more than 15 characters. The short `blurb`s tend to uninformative single words.

```{r}
library(tidymodels)
set.seed(1234)
kickstarter_split <- kickstarter %>%
  filter(nchar(blurb) >= 15) %>%
  initial_split()

kickstarter_train <- training(kickstarter_split)
kickstarter_test <- testing(kickstarter_split)
```

There are `r scales::comma(nrow(kickstarter_train))` press releases in the training set and `r scales::comma(nrow(kickstarter_test))` in the testing set.

### Preprocessing for deep learning

The way we will be doing preprocessing requires a hyperparameter denoting the length of sequences we would like to include. We need to select this value such that we don't overshoot and introduce a lot of padded zeroes which would make the model hard to train, and we also need to avoid picking too short of a range.

We can use the `count_words()` function from the tokenizers package to calculate the number of words and generate a histogram. Notice how we are only using the training dataset to avoid data leakage when selecting this value.

```{r kickstarterwordlength, fig.cap="Distribution of word count for Kickstarter campaign blurbs"}
kickstarter_train %>% 
  mutate(n_words = tokenizers::count_words(blurb)) %>%
  ggplot(aes(n_words)) +
  geom_bar() +
  labs(x = "Number of words per campaign blurb",
       y = "Number of campaign blurbs")
```

Given that we don't have many words, to begin with, it makes sense to err on the side of longer sequences since we don't want to lose valuable data. I would suppose that 30 would be a good cutoff point.

```{r prepped_recipe}
library(textrecipes)

max_words <- 20000
max_length <- 30

prepped_recipe <- recipe(~blurb, data = kickstarter_train) %>%
  step_tokenize(blurb) %>%
  step_tokenfilter(blurb, max_tokens = max_words) %>%
  step_sequence_onehot(blurb, sequence_length = max_length) %>%
  prep()

prepped_training <- prepped_recipe %>%
  bake(new_data = NULL, composition = "matrix")
```

### One-hot sequence embedding of text

We have used `step_sequence_onehot()` to transforms the tokens into a numerical format, the main difference here is that this format takes into account the order of the tokens, unlike `step_tf()` and `step_tfidf()` which doesn't take order into account. `step_tf()` and `step_tfidf()` are called bag-of-words for this reason. Let us take a closer look at how `step_sequence_onehot()` works and how its parameters will change the output.

When we are using `step_sequence_onehot()` two things are happening. First, each word is being assigned an integer index. You can think of this as key-index pair of the vocabulary. Next, the sequence of tokens will be replaced with their corresponding index and it is this sequence of integers that make up the final numerical representation. To illustrate here is a small example:

```{r sequence_onhot_rec}
small_data <- tibble(
  text = c("Adventure Dice Game",
           "Spooky Dice Game",
           "Illustrated Book of Monsters",
           "Monsters, Ghosts, Goblins, Me, Myself and I")
  )

small_spec <- recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "") %>%
  prep()
```

Once we have the `prep()`ed recipe then we can `tidy()` it to extract the vocabulary. It is being represented in the `vocabulary` and `token` columns.

```{r sequence_onhot_rec_vocab, dependson="sequence_onhot_rec"}
small_spec %>%
 tidy(2)
```

```{block, type = "rmdnote"}
The `terms` columns refer to the column we have applied `step_sequence_onehot()` to and `id` is its unique identifier. Note that textrecipes allow `step_sequence_onehot()` to be applied to multiple text variables independently and they will have their own vocabularies.
```

If we take a look at the resulting matrix we have 1 row per observation. The first row starts with some padded zeroes but turns into 3, 11, 14, which when matched with the vocabulary can construct the original sentence.

```{r sequence_onhot_rec_matrix1, dependson="sequence_onhot_rec"}
small_spec %>%
  juice(composition = "matrix")
```

But wait, the 4th line should have started with a 4 since the sentence starts with "I" but the first number is 13. This is happening because the sentence is too long to fit inside the specified length. This leads us to ask 3 questions before using `step_sequence_onehot()`

1.  How long should the output sequence be?
2.  What happens to too long sequences?
3.  What happens to too short sequences?

Choosing the right length is a balancing act. You want the length to be long enough such that you don't truncate too much of your text data, but still short enough to keep the size of the data down and to avoid excessive padding. Truncating, having large data output and excessive padding all lead to worse model performance. This parameter is controlled by the `sequence_length` argument in `step_sequence_onehot()`. If the sequence is too long then we need to truncate it, this can be done by removing values from the beginning ("pre") or the end ("post") of the sequence. This choice is mostly influenced by the data, and you need to evaluate where most of the extractable information of the text is located. News articles typically start with the main points and then go into detail. If your goal is to detect the broad category then you properly want to keep the beginning of the texts, whereas if you are working with speeches or conversational text, then you might find that the last thing to be said carries more information and this would lead us to truncate from the beginning. Lastly, we need to decide how the padding should be done if the sentence is too short. Pre-padding tends to be more popular, especially when working with RNN and LSTM models since having post-padding could result in the hidden states getting flushed out by the zeroes before getting to the text itself.

`step_sequence_onehot()` defaults to `sequence_length = 100`, `padding = "pre"` and `truncating = "pre"`. If we change the truncation to happen at the end

```{r sequence_onhot_rec_matrix2, dependson="sequence_onhot_rec"}
recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "", 
                       padding = "pre", truncating = "post") %>%
  prep() %>%
  juice(composition = "matrix")
```

then we see the 4 at the beginning of the last row representing the "I". The starting points are not aligned since we are still padding on the left side. We left-align all the sequences by setting `padding = "post"`.

```{r sequence_onhot_rec_matrix3, dependson="sequence_onhot_rec"}
recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "", 
                       padding = "post", truncating = "post") %>%
  prep() %>%
  juice(composition = "matrix")
```

Now we have that all the 4s neatly aligned in the first column.

### Simple Flattened Dense network

The model we will be starting with a model that embeds sentences in sequences of vectors, flattening them, and then trains a dense layer on top.

```{r dense_model}
library(keras)

dense_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, 
                  output_dim = 12,
                  input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

dense_model
```

Let us step through this model specification one layer at a time.
We start the keras model by using `keras_model_sequential()` to indicate that we want to compose a linear stack of layers.
Our first layer is an embedding layer via `layer_embedding()` This layer is e equipped to handle preprocessed data we have in `prepped_training`. It will take each observation/row in `prepped_traning` and embed each token to an embedding vector. This will result in each observation being turned into an (embedding_dim x sequence_length) matrix witch would be a (12 x 30) matrix with our settings, creating a (number of observations x embedding_dim x sequence_length) tensor.
The `layer_flatten()` layer that follows takes the 2-dimensional tensors for each observation and flattens it down into 1 dimension. This will create a `30 * 12 = 360` tensor for each observation. Lastly, we have 2 densely connected layers with the last layer having a sigmoid activation function to give us a number between 0 and 1.

Now that we have specified the architecture of the model we still have a couple of things left to add to the model before we can fit it to the data. A keras model requires an optimizer and a loss function to be able to compile. When the neural network finished passing a batch of data through the network it needs to find a way to use the difference between the predicted values and true values to update the weights. the algorithm that determines those weights is known as the optimization algorithm. keras comes pre-loaded with many optimizers^[https://keras.io/api/optimizers/] and you can even create custom optimizers if what you need isn't on the list. We will start by using the rmsprop optimizer.

```{block, type = "rmdnote"}
An optimizer can either be set with the name of the optimizer as a character or by supplying the function `optimizer_*()` where `*` is the name of the optimizer. If you use the function then you can specify parameters for the optimizer.
```

During training, we need to calculate a quantity that we want to have minimized. This is the loss function, keras comes pre-loaded with many loss functions^[https://keras.io/api/losses/]. These loss function will typically take in two values, typically the true value and the predicted value, and return a measure of how close they are. 
Since we are working on a binary classification task and have the final layer of the network return a probability, then we find binary cross-entropy to be an appropriate loss function. Binary cross-entropy does well at dealing with probabilities as it measures the “distance” between probability distributions. which would in our case be between the ground-truth distribution and the predictions.

We can also add any number of `metrics`^[https://keras.io/api/metrics/] to be calculated and reported during training. These metrics will not affect the training loop which is controlled by the optimizer and loss function. The metrics job is to report back a single number that will inform you of the user how well the model is performing. We will select accuracy as our metric for now. We can now set these 3 options; optimizer, loss, and metrics using the `compile()` function

```{r dense_model_compiled}
dense_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{block, type = "rmdnote"}
Notice how the `compile()` function modifies the network in place. This is different then what is conventionally done in R where a new network object would have been returned.
```

Finally, we can fit the model. When we `fit()` a keras model we need to supply it with the data we are having the model train on. We need to supply this a matrix of predictors `x` and a numeric vector of labels `y`.
This is sufficient information to start training the model. We are going to specify a couple more arguments to get better control of the training loop. First, we set the number of observations to pass through at a time with `batch_size`, and we set `epochs = 20` to tell the model to pass all the data through the training loop 20 times. Lastly, we set `validation_split = 0.2` to specify an internal validation split for when the metrics are calculated.

```{r dense_model_history}
dense_history <- dense_model %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 20,
  validation_split = 0.2
)
```

We can visualize the results of the training loop by `plot()`ing the `dense_history`.

```{r dense_model_history_plot, fig.cap="Training and validation metrics for dense network"}
plot(dense_history)
```

Now that we have the fitted model can we apply the model to our testing dataset to see how well the model performs on data it hasn't seen.

```{r dense_model_evaluate}
dense_model %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

we see that the accuracy very closely resembles the val_accuracy from the training loop, suggesting that we didn't overfit our model.

### Evaluate Keras predictions

At the end of the last sub-section we used `evaluate()` to see how well our model is performing.
If you want to use the yardstick package we would have to make some transformations since yardstick needs its input to be data.frames and keras mostly works with matrices and arrays.
The following function create a little brigde between the two frameworks. It allows us to combine a keras model, recipes recipe along with the data, and return the predictions in a tibble format.

```{r}
library(dplyr)
keras_predict <- function(model, recipe, data, response) {
  baked_data <- bake(recipe, data, composition = "matrix")
  prediction <- predict(model, baked_data)[, 1]
  
  data %>%
    mutate(
      .pred_prop = prediction,
      .pred_class = if_else(.pred_prop < 0.5, 0, 1)
    ) %>%
  mutate(across(c({{ response }}, .pred_class), 
                ~ factor(.x, levels = c(1, 0))))
}
```

It is worth noting that this function only works with simple binary classification models that take in its data as a matrix and return a single probability for each observation.
The function is in addition to returning the predicted probability also returning the predicted class based on a 50% cut-off. The function is also making sure that the class and predicted class are both factors with matching levels.

Using this function will allow us to get prediction results that seamlessly connects with yardstick.

```{r}
dense_model_predictions <- keras_predict(
    dense_model, 
    prepped_recipe, 
    kickstarter_test, 
    state
  )

dense_model_predictions
```

We can calculate the standard metrics with `matrics()` 

```{r}
library(yardstick)
metrics(dense_model_predictions, state, .pred_class)
```

and we see that they match the results we got from `evaluate()`. Since we have access to yardsticks full capacities we can also calculates confusion matrices and ROC curves.
Looking at the heatmap in Figure \@ref(fig:dnnheatmap), we are happy to see that there isn't any glaring bias in how the model performs. The model is perfect, it is still only working on around a little over `r scales::percent_format()(floor(100 * accuracy(dense_model_predictions, state, .pred_class)$.estimate) / 100)`, bu is is more or less evenly god at prediction both classes.

```{r dnnheatmap, dependson="nbrs", fig.cap="Confusion matrix for first DNN model predictions of Kickstarter state"}
dense_model_predictions %>%
  conf_mat( state, .pred_class) %>%
  autoplot(type = "heatmap")
```

The ROC curve in Figure \@ref(fig:dnnroccurve) also looks just as good as we would suspect from the other metrics we have calculated so far. Nothing alarming is standing out for us.

```{r dnnroccurve, fig.width=7, fig.height=6, fig.cap="ROC curve for first DNN model predictions of Kickstarter state"}
dense_model_predictions %>%
  roc_curve(truth = state, .pred_prop) %>%
  autoplot() +
  labs(
    title = "Receiver operator curve for Kickstarter blurbs"
  )
```


## Using pre-trained word embeddings

In the last section did we include an embedding layer, and we let the model train the embedding along with it. This is not the only way to handle this task. In chapter \@ref(embeddings) we looked at how embeddings are created and how they are used. Instead of having the embedding layer start at random and have it being trained alongside the other parameters, let us try to supply them our self. 

We start by getting a pre-trained embedding. The glove embedding that we used in section \@ref(glove) will work for now. Setting `dimensions = 50` and only selecting the first 12 dimensions will make it easier for us to compare models.

```{r eval=FALSE}
library(textdata)

glove6b <- embedding_glove6b(dimensions = 50) %>% select(1:13)
```

```{r glove6b12d, echo=FALSE, R.options = list(tibble.max_extra_cols=9, tibble.print_min=10, tibble.width=80)}
load("data/glove6b.rda")
glove6b <- glove6b[, 1:13]
glove6b
```

The `embedding_glove6b()` function returns a tibble which isn't the right format for what keras expects. Also, take notice of how many rows are present in this embedding. Far more than what the trained recipe is expecting to return. The vocabulary can be extracted from the trained recipe using `tidy()`. First, we apply `tidy()` to `prepped_recipe` to get the list of steps that the recipe contains.

```{r, dependson="prepped_recipe"}
tidy(prepped_recipe)
```

We see that the 3rd step is the `sequence_onhot` step, so by setting `number = 3` can we extract the vocabulary of transformation.

```{r, dependson="prepped_recipe"}
tidy(prepped_recipe, number = 3)
```

This list of tokens can then be `left_join()`ed to the `glove6b` embedding tibble to only keep the tokens of interest. Any tokens from the vocabulary not found in `glove6b` is replaced with 0 using `mutate_all()` and `replace_na()`. The results are turned into a matrix, and a row of zeroes is added at the top of the matrix to account for the out-of-vocabulary words.

```{r glove6b_matrix, dependson=c("glove6b12d", "prepped_recipe")}
glove6b_matrix <- tidy(prepped_recipe, 3) %>%
  select(token) %>%
  left_join(glove6b, by = "token") %>%
  mutate_all(replace_na, 0) %>%
  select(-token) %>%
  as.matrix() %>%
  rbind(0, .)
```

The way the model is constructed will remain as unchanged as possible. We make sure that the `output_dim` argument is being set to be equal to` ncol(glove6b_matrix)`, this is a way to make sure that all the dimensions will line up nicely. Everything else stays the same.

```{r dense_model_pte, dependson="glove6b_matrix"}
dense_model_pte <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, 
                output_dim = ncol(glove6b_matrix),
                input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Now we use `get_layer()` to access the first layer which is the embedding layer, then we set the weights with `set_weights()` and lastly we freeze the weights with `freeze_weights()`. 
Freezing the weights stops them from being updated during the training loop.

```{r dense_model_pte_weights, dependson="dense_model_pte"}
dense_model_pte %>%
  get_layer(index = 1) %>%
  set_weights(list(glove6b_matrix)) %>%
  freeze_weights()
```

Now we will compile and fit the model just like the last one we looked at.

```{r dense_pte_history, dependson="dense_model_pte_weights"}
dense_model_pte %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

dense_pte_history <- dense_model_pte %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 20,
  validation_split = 0.2
)
```

This model is not performing as well as the previous model and the evaluation isn't that much better.

```{r dense_pte_evaluate, dependson="dense_pte_history"}
dense_model_pte %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

Why is this happening? Part of the training loop is about adjusting the weights in the network. 
When we froze the weights in this network it appears that we froze them at values that did not perform very well. 
This pre-trained glove embedding[@Pennington2014] we are using have been trained on a Wikipedia dump and [Gigaword 5](https://catalog.ldc.upenn.edu/LDC2011T07) which is a comprehensive archive of newswire text. 
The text contained on Wikipedia and in new articles both follows certain styles and semantics.
Both will tend to be written formally and in the past tense. 
They also contain longer and complete sentences. 
There are many more distinct features of both Wikipedia text and news articles, but the important part is how similar they are to the data we are trying to use.
These text fields are very short, lack punctuation, stop words, narrative, and tense. Many of them simply try to pack as many buzz words in as possible while keeping the sentence readable.
It is not surprising that the word embedding doesn't perform well in this model since the text it is trained on is so far removed from the text is it being applied on.

Although this didn't work that well, doesn't mean that using pre-trained word embeddings are useless.
Sometimes they can perform very well, the important part is how well the embedding fits the data you are using.
there is one more way we can use this embedding in our network, we can load it in as before but not freeze the weights.
This allows the models to still adjust the weights to better fit the data, and the hope is that this pre-trained embedding delivers a better starting point than the randomly generated embedding we get if we don't set the weights.

We specify a new model

```{r dense_model_pte2, dependson="glove6b_matrix"}
dense_model_pte2 <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, 
                output_dim = ncol(glove6b_matrix),
                input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

set the weights with `set_weights()` but we don't freeze them

```{r dense_model_pte_noweights, dependson="dense_model_pte"}
dense_model_pte2 %>%
  get_layer(index = 1) %>%
  set_weights(list(glove6b_matrix))
```

and we compile and fit the model as we did last time

```{r dense_pte_noweights_history, dependson="dense_model_pte_noweights"}
dense_model_pte2 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

dense_pte2_history <- dense_model_pte2 %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 40,
  validation_split = 0.2
)
```

```{r dense_pte_noweights_evaluate, dependson="dense_pte_noweights_history"}
dense_model_pte2 %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

This performs quite a bit better than when we froze the weights.
However, it is training slower than when we didn't set weights since we had to run it for around 40 epochs before we start to overfit.

```{block, type = "rmdnote"}
If you have enough corpus data in the field you are working on, then it would be worth considering training a word embedding that better captures the structure of the domain you are trying to work with.
```

We can combine all the predictions on these last 3 models to more easily compare the results between them

```{r}
all_dense_model_predictions <- bind_rows(
  keras_predict(
    dense_model, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "dense"),
  keras_predict(
    dense_model_pte, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "pte (locked weights)"),
  keras_predict(
    dense_model_pte2, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "pte (not locked weights)")
)
```

Now that the results are combined in `all_dense_model_predictions` we can calculate group-wise evaluation statistics by grouping by the `model` variable.

```{r}
all_dense_model_predictions %>%
  group_by(model) %>%
  metrics(state, .pred_class)
```

We can also do this for ROC-curves. Figure \@ref(fig:alldnnroccurve) shows the 3 different ROC-curves together in 1 chart. As we know the model using pre-trained word embeddings with locked weights didn't perform very well and its ROC-curve is the lowest of the 3. The other two models perform more similar but the self-trained model ends up beating the pre-trained model after all.

```{r alldnnroccurve, fig.width=7, fig.height=6, fig.cap="ROC curve for all DNN models' predictions of Kickstarter state"}
all_dense_model_predictions %>%
  group_by(model) %>%
  roc_curve(truth = state, .pred_prop) %>%
  autoplot() +
  labs(
    title = "Receiver operator curve for Kickstarter blurbs"
  )
```

## Summary {#dldnnsummary}

You can use classification modeling to predict labels or categorical variables from a dataset, including datasets that include text.
Deep neural networks can be used to fit classification models that work on text features.
Although these models have many parameters they are able to adequately train.
We also saw how we can tokenize in a way that depicts the order of the tokens in the text. Doing this can give the model the possibility of finding patterns in the order. This is not possible in the models we saw in chapter \@ref(mlclassification) and \@ref(mlregression) since these models only take a 2 dimensional matrix as input.
We gave up some of the fine control over feature engineering in the hope that the network would be able to learn the features themselves.
The feature engineering is not completely out of our hands since we can still affect the tokenization and normalization before the tokens are being passed into the network.

### In this chapter, you learned:

- how to build and train a deep neural network with keras
- How to tokenize text in a way that retains the order of the tokens
- how to train word embeddings alongside your model
- about how deep learning models have low interpretability
- How to use pre-trained word embeddings in a neural network

```{r echo=FALSE}
knitr::knit_exit()
```

```{r eval = FALSE}
library(hardhat)
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")

kickstarter_train <- kickstarter_train %>%
  mutate(state = as.factor(state))

## baseline lasso model
set.seed(123)
kickstarter_folds <- vfold_cv(kickstarter_train)

kickstarter_rec <- recipe(state ~ blurb, data = kickstarter_train) %>%
  step_tokenize(blurb) %>%
  step_tokenfilter(blurb, max_tokens = 5e3) %>%
  step_tfidf(blurb)

kickstarter_rec

lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
lasso_spec

lambda_grid <- grid_regular(penalty(), levels = 20)
lambda_grid

kickstarter_wf <- workflow() %>%
  add_recipe(kickstarter_rec, blueprint = sparse_bp) %>%
  add_model(lasso_spec)

kickstarter_wf

doParallel::registerDoParallel()
set.seed(2020)
lasso_rs <- tune_grid(
  kickstarter_wf,
  kickstarter_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE, verbose = TRUE)
)

autoplot(lasso_rs)

show_best(lasso_rs, "roc_auc") # 0.7519311

show_best(lasso_rs, "accuracy") # 0.6840861
```