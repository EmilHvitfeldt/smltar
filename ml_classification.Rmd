# Classification {#mlclassification}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE, 
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())

# Controls caching
online <- TRUE
```

In the previous chapter, we focused on using modeling to predict *continuous values* for documents, such as what year a Supreme Court opinion was published. This is an example of a regression model. We can also use machine learning to predict *labels* on documents using a classification model. For example, let's consider the dataset of consumer complaints submitted to the US Consumer Finance Protecton Bureau. The dataset contains a text field with the complaint along with information regarding what it was for,
how it was filed, and the response. 
In this chapter, we will build models to predict what type of product the complaints are referring to, a label or categorical variable. 


## A first classification model {#classfirstattemptlookatdata}

For most of this chapter, we will focus on binary classification and whether a submitted complaint is about "Credit reporting, credit repair services, or other personal consumer reports" or not.

Let's read in the complaint data (\@ref(us-consumer-finance-complaints)) with `read_csv()`.

```{r complaints, message=FALSE}
library(textrecipes)
library(tidymodels)
library(tidytext)
library(stringr)
library(discrim)
library(readr)

complaints <- read_csv("data/complaints.csv.gz")
```

We can start by taking a quick `glimpse()` at the data to see what we have to work with.

```{r, dependson="complaints"}
glimpse(complaints)
```

The first thing to note is our target variable `product` which we need to trim only display "Credit reporting, credit repair services, or other personal consumer reports" and "Other". We will rename product class to "Credit" for the remainder of the chapter to save space.
and the `consumer_complaint_narrative` variable which contains the complaints.
Here is the first 6 complaints:

```{r, dependson="complaints"}
head(complaints$consumer_complaint_narrative)
```

Throughout the narratives is a series of capital x's. This has been done to hide Personally identifiable information (PII). This is not a universal censoring mechanism and can vary from source to source, hopefully you will be able to get this information in the data dictionary but you should always look at the data yourself to verify. We also see that all monetary amounts are surrounded by curly brackets, this is another step of preprocessing that has been done for us.

We can craft a regular expression to extract all the dollar amounts.

```{r, dependson="complaints"}
complaints$consumer_complaint_narrative %>%
  str_extract_all("\\{\\$[0-9\\.]*\\}") %>%
  compact() %>%
  head()
```


### Building our first classification model {#classfirstmodel}

Since this data is given to us after the fact, 
we need to make sure that only the information that would be available at the time of prediction is included in the model,
otherwise we are going to be very disappointed once the model is pushed to production.
The variables we can use as predictors are

- `date_received`
- `issue`
- `sub_issue`
- `consumer_complaint_narrative`
- `company`
- `state`
- `zip_code`
- `tags`
- `submitted_via`

Many of these have quite a lot of levels.
First we will include `date_received` for further consideration, along with `consumer_complaint_narrative` and `tags`.
`submitted_via` would have been a viable candidate too, but all the entries are "web".
The other variables could be of use too, but they are categorical variables with many values so we will exclude them for now.

We start by splitting the data into a training and testing dataset.
But before we do that we will create a factor variable of `product` with the levels "Credit" and "Other".
Then we will use the `initial_split()` from **rsample** to create a binary split of the data. 
The `strata` argument is used to make sure that the split is created to make sure the distribution of `product` is similar in the training set and testing set. 
Since the split is done using random sampling we set a seed so we can reproduce the results.

```{r, complaintssplit}
set.seed(1234)
complaints2class <- complaints %>%
  mutate(product = factor(if_else(product == "Credit reporting, credit repair services, or other personal consumer reports", "Credit", "Other")))

complaints_split <- initial_split(complaints2class, strata = product)

complaints_train <- training(complaints_split)
complaints_test <- testing(complaints_split)
```

Looking at the dimensions of the two split shows that it worked successfully.

```{r, dependson="complaintssplit"}
dim(complaints_train)
dim(complaints_test)
```

Next we need to do some preprocessing. We need to do this since the models we are trying to use only support all numeric data. 

```{block, type = "rmdnote"}
Some models are able to handle factor variables and missing data. But it is in our best interest to manually deal with these problems so we know how they are handled.
```

The **recipes** package allows us to create a specification of the preprocessing steps we want to perform. Furthermore it contains the transformations we have trained on the training set and apply them in the same way for the testing set.
First off we use the `recipe()` function to initialize a recipe, we use a formula expression to specify the variables we are using along with the dataset.

```{r complaintrec1, dependson="complaintssplit"}
complaints_rec <- 
  recipe(product ~ date_received + tags + consumer_complaint_narrative, 
         data = complaints_train)
```

First will we take a look at the `date_received` variable. We use the `step_date()` to extract the month and day of the week (dow). Then we remove the original variable and dummify the variables created with `step_dummy()`.

```{r complaintrec2, dependson="complaintrec1"}
complaints_rec <- complaints_rec %>%
  step_date(date_received, features = c("month", "dow"), role = "dates") %>%
  step_rm(date_received) %>%
  step_dummy(has_role("dates"))
```

the `tags` variable includes some missing data. We deal with this by using `step_unknown()` to that adds a new level to the factor variable for cases of missing data. Then we dummify the variable with `step_dummy()`

```{r complaintrec3, dependson="complaintrec2"}
complaints_rec <- complaints_rec %>%
  step_unknown(tags) %>%
  step_dummy(tags)
```

Lastly we use **textrecipes** to handle the `consumer_complaint_narrative` variable. First we perform tokenization to words with `step_tokenize()`, by default this is done using `tokenizers::tokenize_words()`.
Next we remove stopwords with `step_stopwords()`, the default choice is the snowball stopword list, but custom lists can be provided too. Before we calculate the tf-idf we use `step_tokenfilter()` to only keep the 500 most frequent tokens, this is to avoid creating too many variables. To end off we use `step_tfidf()` to perform tf-idf calculations.

```{r complaintrec4, dependson="complaintrec3"}
complaints_rec <- complaints_rec %>%
  step_tokenize(consumer_complaint_narrative) %>%
  step_stopwords(consumer_complaint_narrative) %>%
  step_tokenfilter(consumer_complaint_narrative, max_tokens = 500) %>%
  step_tfidf(consumer_complaint_narrative)
```

Now that we have a full specification of the recipe we run `prep()` on it to train each of the steps on the training data.

```{r complaintprep, dependson="complaintrec4"}
complaint_prep <- prep(complaints_rec)
```

We can now extract the transformed training data with `juice()`. To apply the prepped recipe to the testing set we use the `bake()` function.

```{r complaintdata, dependson=c("complaintprep", "complaintssplit")}
train_data <- juice(complaint_prep)
test_data <- bake(complaint_prep, complaints_test)
```

For the modeling we will use a simple Naive Bayes model (TODO add citation to both Naive Bayes and its use in text classification).
One of the main advantages of Naive Bayes is its ability to handle a large number of features that we tend to get when using word count methods.
Here we have only kept the 500 most frequent tokens, we could have kept more tokens and a Naive Bayes model would be able to handle it okay, but we will limit it for this first time.

```{r nbspec}
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")
nb_spec
```

Now we have everything we need to fit our first classification model, we just have to run `fit()` on our model specification and our training data.

```{r nbfit, dependson="nbspec"}
nb_fit <- nb_spec %>%
  fit(product ~ ., data = train_data)
```

We have more successfully fitted out first classification model.

### Evaluation

One option for our evaluating our model is to predict one time on the test set to measure performance. The test set is extremely valuable data, however, and in real world situations, you can only use this precious resource one time (or at most, twice). The purpose of the test data is to estimate how your final model will perform on new data. Often during the process of modeling, we want to compare models or different model parameters. We can't use the test set for this; instead we use **resampling**.

For example, let's estimate the performance of the Naive Bayes classification model we just fit. We can do this using resampled datasets built from the training set. Let's create cross 10-fold cross-validation sets, and use these resampled sets for performance estimates.

```{r complaintsfolds, dependson="complaintssplit"}
complaints_folds <- vfold_cv(complaints_train)

complaints_folds
```

Each of these "splits" contains information about how to create cross-validation folds from the original training data. In this example, 90% of the training data is included in each fold and the other 10% is held out for evaluation.

For convenience, let's use `workflows()` for our resampling estimates of performance. These are convenience functions that fit different modeling functions like recipes, model specifications, etc. together so they are easier to pass around in a modeling project.

```{r nbwf, dependson=c("nbspec", "complaintrec4")}
nb_wf <- workflow() %>%
  add_recipe(complaints_rec) %>%
  add_model(nb_spec)

nb_wf
```

In the last section, we fit one time to the training data as a whole. Now, to estimate how well that model performs, let's fit the model many times, once to each of these resampled folds, and then evaluate on the heldout part of each resampled fold.

```{r nbrs, dependson=c("nbwf", "complaintsfolds"), eval = !online}
nb_rs <- fit_resamples(
  nb_wf,
  complaints_folds,
  control = control_resamples(save_pred = TRUE)
)
```

We can extract the relevant information using `collect_metrics()` and `collect_predictions()`

```{r nbrsmetrics, eval = !online}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)
```

```{r, include = FALSE, eval = !online}
write_rds(nb_rs_metrics, "data/ml_class_first_nb_rs_metrics.rds")
write_rds(nb_rs_predictions, "data/ml_class_first_nb_rs_predictions.rds")
```

```{r, include = FALSE, eval = online}
nb_rs_metrics <- read_rds("data/ml_class_first_nb_rs_metrics.rds")
nb_rs_predictions <- read_rds("data/ml_class_first_nb_rs_predictions.rds")
```

What results do we see, in terms of performance metrics?

```{r}
nb_rs_metrics
```

The default performance parameters for binary classification are accuracy and ROC AUC (area under the receiver operator curve). The accuracy is the percentage of accurate predictions. Which means that for this resample the average accurracy was `r nb_rs_metrics %>% filter(.metric == "accuracy") %>% pull(mean) %>% scales::percent_format(accuracy = 0.1)()`.

```{block, type = "rmdnote"}
For both Accuracy and ROC AUC values closer to 1 are better.
```

The [reciver operator curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) shows the sensitivity at different thresholds. It shows us how well the classification model can distinguish between classes. Figure \@ref(fig:firstroccurve) shows the ROC curve for our first classification model on each of the resampled datasets.

```{r firstroccurve, fig.width=7, fig.height=6, fig.cap="ROC curve for resamples of US Consumer Finance Complaints."}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = product, .pred_Credit) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "Receiver operator curve for US Consumer Finance Complaints",
    subtitle = "Each resample fold is shown in a different color"
  )
```

The area under each of these curves is the `roc_auc` metric we have been collecting. If the curve was going along the diagonal line then the model would be no better then random chance.

Another way to evaluate our model is to look at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). This will tell us how well the model is doing with false positives and false negatives for each class.
there is not a trivial way to visualize multiple confusion matrices, so we will look at them individually for each fold.

```{r firstheatmap, dependson="nbrs", fig.cap="Confusion matrix for first Naive Bayes classifier. It appears to be biased towards predicting 'Credit'"}
nb_rs_predictions %>%
  filter(id == "Fold01") %>%
  conf_mat(product, .pred_class) %>%
  autoplot(type = "heatmap")
```

Figure \@ref(fig:firstheatmap) see that the diagonal squares has darker shades then the off diagonal squares. This is a good sign since we are right more often then not. However the model still appears to be strugling since it is more or less at 50-50 when predicting something from the "Other" class.

```{block, type = "rmdnote"}
One metric can give you a complete picture of how well your classification model is performing. The Confusion matrix is a good starting point to get an overview of your model performance as it includes a lot of information.
```

The following sections will explore different ways we can improve this model.

## Compare to a SVM model {#comparetosvm}

[Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine) are a class of machine learning models that can be used in regression and classification tasks. While they don't see widespread use in the general machine learning space they still have some properties that make them suitable for text classification [@Joachims1998] and can at times give good performance [@Vantu2016].

We create a specification of an SVM with a radial basis function as the kernel. 

```{r svmspec}
svm_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("liquidSVM")

svm_spec
```

Then we create another workflow object with the SVM specification.

```{r svmwf, dependson=c("svmspec", "complaintrec4")}
svm_wf <- workflow() %>%
  add_recipe(complaints_rec) %>%
  add_model(svm_spec)
```

The **liquidSVM** engine doesn't support class probabilities so we have to replace the default metric set with a metric set that doesn't use class probabilities. Here we use accuracy, [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity) and [specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity).

```{r svmrs, dependson=c("svmwf", "complaintsfolds"), eval = !online}
svm_rs <- fit_resamples(
  svm_wf,
  complaints_folds,
  metrics = metric_set(accuracy, sensitivity, specificity),
  control = control_resamples(save_pred = TRUE)
)
```

We will again extract the relevant information using `collect_metrics()` and `collect_predictions()`

```{r svmrsmetrics, eval = !online}
svm_rs_metrics <- collect_metrics(svm_rs)
svm_rs_predictions <- collect_predictions(svm_rs)
```

```{r, include = FALSE, eval = !online}
write_rds(svm_rs_metrics, "data/ml_class_first_svm_rs_metrics.rds")
write_rds(svm_rs_predictions, "data/ml_class_first_svm_rs_predictions.rds")
```

```{r, include = FALSE, eval = online}
svm_rs_metrics <- read_rds("data/ml_class_first_svm_rs_metrics.rds")
svm_rs_predictions <- read_rds("data/ml_class_first_svm_rs_predictions.rds")
```

We can now look at `svm_rs_metrics` to see the formatted  3 performance metrics for the SVM model.

```{r, dependson="svmrsmetrics"}
svm_rs_metrics
```

They look pretty promising considering we didn't do any hyperparameter tuning on the model parameters.
Let us finish by generating a confusion matrix which we see in Figure \@ref(fig:svmheatmap).
It appears to do much better at seperating the classes then the Naive Bayes model and it is much more symmetrical compared to the confusion matrix we saw for the Naive Bayes model in figure \@ref(fig:firstheatmap).

```{r svmheatmap, dependson="svmrsmetrics", fig.cap="Confusion matrix for a simple Support Vector Machine classifier. Yields much more symmetric results."}
svm_rs_predictions %>%
  filter(id == "Fold01") %>%
  conf_mat(product, .pred_class) %>%
  autoplot(type = "heatmap")
```

one of the main benefits of support vector machines is their support for sparse data. We can already see that count based text representations are quite sparse which leads support vector machines to be a great match.
Support vector machines generally also perform well with large datasets (thousands of columns) which are also quite common with text data.

## Two class or multiclass {#mlmulticlass}

We will spend most of this chapter focuses on [binary classification](https://en.wikipedia.org/wiki/Binary_classification).
We have two classes and it can be in one or the other. This is a simpler scenario and the evaluation will be easier to do because you only have a 2 by 2 contingency matrix.
However, it is not always possible to limit yourself to 2 classes. The topic for this section is to explore how we deal with situations where we have more then 2 classes.
The complaints dataset we have been working with so far has 9 different product classes. Here they are in decreasing frequency:

- Credit reporting, credit repair services, or other personal consumer reports
- Debt collection
- Credit card or prepaid card
- Mortgage
- Checking or savings account
- Student loan
- Vehicle loan or lease
- Money transfer, virtual currency, or money service
- Payday loan, title loan, or personal loan

We assume that there is a reason why these product classes have been created in this fashion.
Perhaps complaints from different classes are handled by different people.
In any case, it would be nice to be able to build a multi-class classifier to classify specific product classes.

We need to create a new split of the data using `initial_split()` on the unmodified `complaints` dataset.

```{r, multicomplaintssplit}
set.seed(1234)

multicomplaints_split <- initial_split(complaints, strata = product)

multicomplaints_train <- training(multicomplaints_split)
multicomplaints_test <- testing(multicomplaints_split)
```

Before we go on let us take a look at the number of cases in each of the classes

```{r, dependson="multicomplaintssplit"}
multicomplaints_train %>% 
  count(product, sort = TRUE) %>% 
  select(n, product)
```

There is quite a large imbalance between the classes. Especially since there are over 20 times more cases of the majority class then there is of the smallest class.
We have to deal with this problem, which is not an uncommon problem to have when dealing with multi-class classification since the chance of having a completely balanced dataset is very low.

We have a couple of additional issues we have to keep in mind when working with multi-class classification

- Imbalance of classes
- Many machine learning algorithms do not handle imbalanced data well and is likely to have a hard time predicting minority classes
- Not all machine learning algorithms are built for multi-class classification
- Many evaluation metrics will have to reformulated to be able to describe multi-class predictions

When you have multiple classes in your data, then it is also possible to formulate the multi-class problem in 2 ways. One formulation is that you can put any given observation into multiple classes. The other formulation is that any observations can be put into one and only one class. We will be sticking to a "one class per observation" model formulation in this section.

There are many different ways to deal with unbalanced data.
We will showcase one of the simplest methods, downsampling, which is a method where observations from the majority classes are removed during training to achieve a more balanced class distribution.
We will be using the [themis](https://github.com/tidymodels/themis) add-on package for recipes which provides the [step_downsample()](https://tidymodels.github.io/themis/reference/step_downsample.html) step to perform downsampling.

```{block, type = "rmdnote"}
The themis package provides many more algorithms to help you deal with unbalanced data.
```

We have to create a new recipe specification from scratch since the training data is part of the main specification.
This specification is similar to the \@ref(classfirstattemptlookatdata) with the only chances being the data passed to the `data` argument in `recipe()` have been changed to `multicomplaints_train` and we have added `step_downsample(product)` to the end of the recipe specification to indicate that we want to downsample after doing all the text preprocessing. We want to do the downsampling last such that we still generate features on the full training dataset. The downsampling will then only affect the modeling step and hopefully create a better performing model.

```{r multicomplaintsrec, message=FALSE, dependson="multicomplaintssplit"}
library(themis)

multicomplaints_rec <- 
  recipe(product ~ date_received + tags + consumer_complaint_narrative, 
         data = multicomplaints_train) %>%
  step_date(date_received, features = c("month", "dow"), role = "dates") %>%
  step_rm(date_received) %>%
  step_dummy(has_role("dates")) %>%
  step_unknown(tags) %>%
  step_dummy(tags) %>%
  step_tokenize(consumer_complaint_narrative) %>%
  step_stopwords(consumer_complaint_narrative) %>%
  step_tokenfilter(consumer_complaint_narrative, max_tokens = 500) %>%
  step_tfidf(consumer_complaint_narrative) %>%
  step_downsample(product)
```

We also have to create a new cross-validation object since we are using a different dataset.

```{r multicomplaintsfolds, dependson="complaintssplit"}
multicomplaints_folds <- vfold_cv(multicomplaints_train)
```

We can reuse the Support Vector Machine specification from \@ref(comparetosvm) and create a new workflow object with the new recipe specification.
The SVM algorithm is specified for binary classification only however extensions have been made to generalize to multi-class cases. The liquidSVM method will automatically detect that we are doing multi-class classification and switch to the appropriate extension.

```{r multisvmwf, dependson=c("svmspec", "multicomplaintsrec")}
multi_svm_wf <- workflow() %>%
  add_recipe(multicomplaints_rec) %>%
  add_model(svm_spec)

multi_svm_wf
```

```{block, type = "rmdnote"}
Notice how you don't have to specify that you are doing multi-class classification. The tidymodels packages will infer that from the number of classes in the outcome variable.
```

Now that we have everything we need to put it all into `fit_resamples()` to fit all the models. Note that we are specifying `save_pred = TRUE`. We are doing this so we can create a confusion matrix later afterward which is very beneficial when doing multi-class classification.
We are again specifying the `metric_set()` since liquidSVM doesn't support class probabilities.

```{r multisvmrs, dependson=c("multisvmwf", "multicomplaintsrec"), eval= !online}
multi_svm_rs <- fit_resamples(
  multi_svm_wf,
  multicomplaints_folds,
  metrics = metric_set(accuracy),
  control = control_resamples(save_pred = TRUE)
)

multi_svm_rs
```

We will again extract the relevant information using `collect_metrics()` and `collect_predictions()`

```{r multisvmrsmetrics, dependson="multisvmrs", eval = !online}
multi_svm_rs_metrics <- collect_metrics(multi_svm_rs)
multi_svm_rs_predictions <- collect_predictions(multi_svm_rs)
```

```{r, include = FALSE, eval = !online}
write_rds(multi_svm_rs_metrics, "data/ml_class_first_multi_svm_rs_metrics.rds")
write_rds(multi_svm_rs_predictions, "data/ml_class_first_multi_svm_rs_predictions.rds")
```

```{r, include = FALSE, eval = online}
multi_svm_rs_metrics <- read_rds("data/ml_class_first_multi_svm_rs_metrics.rds")
multi_svm_rs_predictions <- read_rds("data/ml_class_first_multi_svm_rs_predictions.rds")
```

What results do we see, in terms of performance metrics?

```{r}
multi_svm_rs_metrics
```

The accuracy metric naturally extends to multi-class tasks. But it appears quite low at `r multi_svm_rs_metrics %>% filter(.metric == "accuracy") %>% pull(mean) %>% scales::percent_format(accuracy = 0.1)()` which is quite a bit lower then for the binary class in \@ref(#comparetosvm). This is expected since multi-class classification is a harder tasker than binary classification. In binary classification, there is 1 right answer and 1 wrong answer. But in this case, there is 1 right answer and 8 wrong answers.

To get a better look at how our classifier is doing let us look at one of the confusion matrices in figure \@ref(fig:multiheatmap).

```{r multiheatmap, fig.cap="Confusion matrix for Multi-class Support Vector Machine classifier. Most of the classifications appear along the diagonal."}
multi_svm_rs_predictions %>%
  filter(id == "Fold01") %>%
  conf_mat(product, .pred_class) %>%
  autoplot(type = "heatmap")
```

The first thing we see is that the diagonal is fairly well populated with is a good sign. This means that the model predicted the right class.
The off-diagonals numbers are all the failures and where we should direct our focus.
It is a little hard to get a good view since the majority class widens the scale quite a bit.
A trick to deal with this problem is to remove all the correctly predicted observations.

```{r multiheatmapminusdiag, fig.cap="Confusion matrix for Multi-class Support Vector Machine classifier without diagonal."}
multi_svm_rs_predictions %>%
  filter(id == "Fold01") %>%
  filter(.pred_class != product) %>%
  conf_mat(product, .pred_class) %>%
  autoplot(type = "heatmap")
```

Now we can more clearly see in figure \@ref(fig:multiheatmapminusdiag) where our model breaks down. One of the most common errors is "Credit reporting, credit repair services, or other personal consumer reports" complaints being wrongly being predicted as "Credit card or prepaid card" complaints. It is not hard to see why that is happening since they both deal with credit and properly have a great deal of overlap in vocabulary.
Knowing what the problem is, helps us figure out how to improve our model
The first steps for improving our models is to look at the data preprocessing steps and model selection.
We can look at different models or model engines that might be able to more easily separate the classes. The `svm_rbf()` model has a `cost` argument that determines the penalization of wrongly predicted classes that might be worth looking at.
Now that we have an idea of where the model isn't working we can look more closely at the data to find features in the data that would help distinguish between problematic classes. In the later section \@ref(custumfeatures) will we showcase how you can create custom features.

## Case study: What happens if you don't censor your data

The complaints data already have sensitive information censored out with XXXX and XX.
This can be seen as a kind of annotation, we don't get to know the specific account numbers and birthday which would be mostly unique anyways and filtered out.

In figure \@ref(fig:censoredtrigram) below we have is the most frequent trigrams [#tokenizing-by-n-grams] from our training dataset.

```{r censoredtrigram, dependson="complaintssplit", fig.cap="Many of the most frequent trigrams feature censored words."}
complaints_train %>%
  slice(1:1000) %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams", 
                collapse = FALSE) %>%
  count(trigrams, sort = TRUE) %>%
  mutate(censored = str_detect(trigrams, 'xx')) %>%
  slice(1:20) %>%
  ggplot(aes(n, reorder(trigrams, n), fill = censored)) +
  geom_col() +
  scale_fill_manual(values = c("grey40", "firebrick")) +
  labs(y = "Trigrams", x = "Count")
```

As you see the vast majority includes one or more censored words.
Not only does the most used trigrams include some kind of censoring, 
but the censored words include some signal as they are not used uniformly between the products.
In the following figure \@ref(fig:trigram25), we take the top 25 most frequent trigrams that includes one of more censoring,
and plot the proportions of the usage in "Credit" and "Other".

```{r trigram25, fig.cap="Many of the most frequent trigrams feature censored words. There a big difference in how often they are used within each class."}
top_censored_trigrams <- complaints_train %>%
    slice(1:1000) %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams", 
                collapse = FALSE) %>%
  count(trigrams, sort = TRUE) %>%
  filter(str_detect(trigrams, 'xx')) %>%
  slice(1:25)

plot_data <- complaints_train %>%
  unnest_tokens(trigrams, consumer_complaint_narrative, token = "ngrams", 
                collapse = FALSE) %>%
  right_join(top_censored_trigrams, by = "trigrams") %>%
  count(trigrams, product, .drop = FALSE) 

plot_data %>%
  ggplot(aes(n, trigrams, fill = product)) +
  geom_col(position = "fill")
```

There is a good spread in the proportions, tokens like "on xx xx" and "of xx xx" are used when referencing to a date, eg "we had a problem on 06/25 2012".
Remember that the current tokenization engine strips the punctuation before tokenizing. 
This means that the above examples are being turned into "we had a problem on 06 25 2012" before creating n-grams.

We can as a practical example replace all cases of XX and XXXX with random integers to crudely simulate what the data might look like before it was censored. 
This is going a bit overboard since dates will be given values between 00 and 99 which would not be right, 
and that we don't know if only numerics have been censored.
Below is a simple function `uncesor_vec()` that locates all instances of `XX` and replaces them with a number between 11 and 99.
We don't need to handle the special case of `XXXX` as it automatically being handled.

```{r uncensor_vec}
uncensor <- function(n) {
  as.character(sample(seq(10 ^ (n - 1), 10 ^ n - 1), 1))
}

uncensor_vec <- function(x) {
  locs <- str_locate_all(x, "XX")
  
  map2_chr(x, locs, ~ {
    for (i in seq_len(nrow(.y))) {
      str_sub(.x, .y[i, 1], .y[i, 2]) <- uncensor(2)
    }
    .x
  })
}
```

And we can run a quick test to see if it works.

```{r, dependson="uncesor_vec"}
uncensor_vec("In XX/XX/XXXX I leased a XXXX vehicle")
```

Now we try to produce the same chart as \@ref(fig:censoredtrigram) but with the only difference being that we apply our uncensoring function to the text before tokenizing.

```{r uncensoredtrigram, dependson=c("complaintssplit", "uncensor_vec"), fig.cap="Trigrams without numbers flout to the top as the uncensored tokens are too spread out."}
complaints_train %>%
    slice(1:1000) %>%
  mutate(text = uncensor_vec(consumer_complaint_narrative)) %>%
  unnest_tokens(trigrams, text, token = "ngrams", 
                collapse = FALSE) %>%
  count(trigrams, sort = TRUE) %>%
  mutate(censored = str_detect(trigrams, 'xx')) %>%
  slice(1:20) %>%
  ggplot(aes(n, reorder(trigrams, n), fill = censored)) +
  geom_col() +
  scale_fill_manual(values = c("grey40", "firebrick")) +
  labs(y = "Trigrams", x = "Count")
```

The same trigrams that appear in the last chart appeared in this one as well, 
but none of the uncensored words appear in the top which is what is to be expected.
This is expected because while `xx xx 2019` appears towards the top in the first as it indicates a date in the year 2019, having that uncensored would split it into 365 buckets.
Having dates being censored gives more power to pick up the signal of a date as a general construct giving it a higher chance of being important.
But it also blinds us to the possibility that certain dates and months are more prevalent.

We have talked a lot about censoring data in this section.
Another way to look at this is a form of preprocessing in your data pipeline.
It is very unlikely that you want any specific person's social security number, credit card number or any other kind of personally identifiable information ([PII](https://en.wikipedia.org/wiki/Personal_data)) imbedded into your model.
Not only is it likely to provide a useful signal as they appear so rarely and most likely highly correlated with other known variables in your database.
More importantly, that information can become embedded in your model and begin to leak if you are not careful as showcased by @carlini2018secret, @Fredrikson2014 and @Fredrikson2015.
Both of these issues are important, and one of them could land you in a lot of legal trouble if you are not careful. 

If for example, you have a lot of social security numbers you should definitely not pass them on to your model, but there is no hard in annotation the presence of a social security number. 
Since a social security number has a very specific form we can easily construct a regular expression \@ref(regexp) to locate them.

```{block, type = "rmdnote"}
A social security number comes in the form AAA-BB-CCCC where AAA is a number between 001 and 899 excluding 666, BB is a number between 01 and 99 and CCCC is a number between 0001 and 9999. This gives us the following regex

(?!000|666)[0-8][0-9]{2}-(?!00)[0-9]{2}-(?!0000)[0-9]{4}
```

We can use a replace function to replace it with something that can be picked up by later preprocessing steps. 
A good idea is to replace it with a "word" that won't be accidentally broken up by a tokenizer.

```{r}
ssn_text <- c("My social security number is 498-08-6333",
              "No way, mine is 362-60-9159", 
              "My parents numbers are 575-32-6985 and 576-36-5202")

ssn_pattern <-  "(?!000|666)[0-8][0-9]{2}-(?!00)[0-9]{2}-(?!0000)[0-9]{4}"

str_replace_all(string = ssn_text, 
                pattern = ssn_pattern,
                replacement = "ssnindicator")
```

This technique isn't just useful for personally identifiable information but can be used anytime you want to intentionally but similar words in the same bucket, hashtags, emails, and usernames can sometimes also benefit from being annotated.

## Case study: Adding custom features {#custumfeatures}

Most of what we have looked at so far have boiled down to counting occurrences of tokens and weighting them in one way or another.
This approach is quite broad and domain agnostic so it might miss some important parts.
Having domain knowledge over your data allows you to extract, hopefully, more powerful, features from the data that wouldn't come up in the naive search from simple tokens.
As long as you can reasonably formulate what you are trying to count, chances are you can write a function that can detect it.
This is where having a little bit of regular expressions pays off.

```{block, type = "rmdnote"}
A noteable package is [textfeatures](https://github.com/mkearney/textfeatures) which includes many functions to extract all different kinds of metrics. textfeatures can be used in textrecipes with the `step_textfeature()` function.
```

If you have some domain knowledge you might know something that can provide a meaningful signal.
It can be simple things like; the number of URLs and the number of punctuation marks.
But it can also be more tailored such as; the percentage of capitalization, does the text end with a hashtag, or are two people's names both mentioned in this text.

It is clear by looking at the data, that certain patterns repeat that have not adequately been picked up by our model so far.
These are related to the censoring and the annotation regarding monetary amounts that we saw in [#classfirstattemptlookatdata].
In this section, we will walk through how to create functions to extract the following features

- Detect credit cards
- Calculate percentage censoring
- Detect monetary amounts

### Detecting credit cards

We know that the credit card is represented as 4 groups of 4 capital Xs.
Since the data is fairly well processed we are fairly sure that spacing will not be an issue and all credit cards will be represented as "XXXX XXXX XXXX XXXX". 
The first naive attempt is to use str_detect with "XXXX XXXX XXXX XXXX" to find all the credit cards.
It is a good idea to create a small example where you know the answer then prototyping your functions before moving them to the main data.
We start by creating a vector with 2 positives, 1 negative and 1 potential false positive.
The last string is more tricky since it has the same shape as a credit card but has one too many groups.

```{r}
credit_cards <- c("my XXXX XXXX XXXX XXXX balance, and XXXX XXXX XXXX XXXX.",
                  "card with number XXXX XXXX XXXX XXXX.",
                  "at XX/XX 2019 my first",
                  "live at XXXX XXXX XXXX XXXX XXXX SC")


str_detect(credit_cards, "XXXX XXXX XXXX XXXX")
```

And we see what we feared, the last vector got falsely detected to be a credit card.
Sometimes you will have to accept a certain number of false positives and false negatives depending on the data and what you are trying to detect. 
In this case, we can make the regex a little more complicated to avoid that specific false positive.
We need to make sure that the word coming before the X's doesn't end in a capital X and the word following the last X doesn't start with a capital X.
We place spaces around the credit card and use some negated character classes[#character-classes] to detect anything BUT a capital X.

```{r}
str_detect(credit_cards, "[^X] XXXX XXXX XXXX XXXX [^X]")
```

Hurray! This fixed the false positive. 
But it gave us a false negative in return.
Turns out that this regex doesn't allow the credit card to be followed by a period since it requires a space.
We can fix this with an alternation to match for a period or a space and a non X.

```{r}
str_detect(credit_cards, "[^X] +XXXX XXXX XXXX XXXX(\\.| [^X])")
```

Know that we have a regular expression we are happy with we can turn it into a function we can use.
We can extract the presence of a credit card with `str_detect()` and the number of credit cards with `str_count()`.

```{r}
creditcard_indicator <- function(x) {
  str_detect(x, "[^X] +XXXX XXXX XXXX XXXX(\\.| [^X])")
}

creditcard_count <- function(x) {
  str_count(x, "[^X] +XXXX XXXX XXXX XXXX(\\.| [^X])")
}

creditcard_indicator(credit_cards)
creditcard_count(credit_cards)
```

### Calculate percentage censoring

Some of the complaints contain quite a lot of censoring, and we will try to extract the percentage of the text that is censored.
There are often many ways to get to the same solution when working with regular expressions.
I will attack this problem by counting the number of X's in each string, then count the number of alphanumeric characters and divide the two to get a percentage.

```{r}
str_count(credit_cards, "X")
str_count(credit_cards, "[:alnum:]")
str_count(credit_cards, "X") / str_count(credit_cards, "[:alnum:]")
```

And we finish up by creating a function.

```{r}
procent_censoring <- function(x) {
  str_count(x, "X") / str_count(x, "[:alnum:]")
}

procent_censoring(credit_cards)
```

### Detecting monetary amounts

We have already constructed a regular expression that detects the monetary amount from the text.
So we can look at how we can use this information.
Let us start by creating a little example and see what we can extract.

```{r}
dollar_texts <- c("That will be {$20.00}",
                  "{$3.00}, {$2.00} and {$7.00}",
                  "I have no money")

str_extract_all(dollar_texts, "\\{\\$[0-9\\.]*\\}")
```

We can create a function that simply detects the dollar amount, and we can count the number of times each amount appears.
But since each occurrence also has a value, would it be nice to include that information as well, such as the mean, minimum or maximum.

First, let's extract the number from the strings, we could write a regular expression for this, but the `parse_number()` function from the readr package does a really good job of pulling out numbers.

```{r}
str_extract_all(dollar_texts, "\\{\\$[0-9\\.]*\\}") %>%
  map(readr::parse_number)
```

Now that we have the number we can iterate over them with the function of our choice.
Since we are going to have texts with no amounts we need to make sure that we have to handle the case with zero numbers. Defaults for some functions with length 0 vectors can before undesirable as we don't want `-Inf` to be a value. I'm going to extract the maximum value and will denote cases with no values to have a maximum of 0.

```{r}
max_money <- function(x) {
  str_extract_all(x, "\\{\\$[0-9\\.]*\\}") %>%
    map(readr::parse_number) %>%
    map_dbl(~ ifelse(length(.x) == 0, 0, max(.x)))
}

max_money(dollar_texts)
```

know that we have created some feature extraction functions we can use them to hopefully make our classification model better.

## Case Study: feature hashing

So far we have been very specific with creating the tokens to have meaning and be different from each other when they need to be.
A second consideration is how we are going to represent these values in our final data.
We have already looked at how we can use simple counts with word frequencies or weighted counts with tf-idf to represent the data.
One of the main problems with these methods is that output space is vast and dynamic.
You could easily have more than 10,000 features in your training set alone, which very quickly would let you run into memory problems.
Deciding how many tokens to include essentially becomes a trade-off between computational time and information.
Using this approach also doesn't let you take advantage of tokens you didn't see in your training data.

One method that has gained popularity in the machine learning field is the **hashing trick**.
This method provides a couple of benefits:

- Very fast
- Low memory footprint
- No dictionary needed

Before we get into more detail, let up go through the basics of feature hashing.
First proposed by @Weinberger2009, feature hashing was introduced as a dimensionality reduction method with a simple premise.
We start by taking a hashing function which we then apply to our tokens.

```{block, type = "rmdnote"}
A hashing function is a function that takes a variables sized output and maps it to a fixed range. Hashing functions are commonly used in cryptography.
```

We will use the hashFunction package to illustrate the behavior of hashing functions.
Propose we have a lot of country names in a variable and turning them into a one-hot encoding might feel like too much.
We start by applying the hashing function to each of the strings to project it into an integer space defined by the hashing function.
We will be using the 32-bit version of MurmurHash3 [@appleby2008] here.

```{block, type = "rmdnote"}
The choice of hashing function isn't based on cryptographic complexity. Instead, it just needs to fulfill a couple of properties and be fast. Uniformity of the output makes sure that the whole output space is being filled evenly. The Avalanche effect makes it so similar strings are hashed in a way so they won't be similar in out output space.
```

```{r}
library(hashFunction)
countries <- c("Palau", "Luxembourg", "Vietnam", "Guam", "Argentina", "Mayotte", 
               "Bouvet Island", "South Korea", "San Marino", "American Samoa")

map_int(countries, murmur3.32)
```

Since the MurmurHash has 32 bits the number of possible values is `2^32 = 4294967296` which at the moment doesn't feel like a reduction.
We then take the modulo of these big values to project them down to a more manageable space.

```{r}
map_int(countries, murmur3.32) %% 24
```

Now we simply use these as the indices when creating the matrix

```{r, echo=FALSE}
Matrix::sparseMatrix(1:10, map_int(countries, murmur3.32) %% 24, 
                     dims = c(10, 24),
                     dimnames = list(countries, NULL))
```

This method is really fast, both the hashing and modulo can be performed independently for each input since it doesn't need information about the full corpus.
Since we are reducing the space, there is a chance that multiple words are hashed to the same value.
This is called a collision and at first glance, it would appear that this would be a big negative for our model.
However, it is reported that using feature hashing has roughly the same accuracy as a simple bag-of-words model and the effect of collisions is quite minor [@Forman2008].

```{block, type = "rmdnote"}
Another thing that is done to avoid the negative effects of collisions is to have a second hashing function that returns 1 and -1. This will determine if we are adding or subtracting the index we get from the first hashing function. Suppose both the word "outdoor" and "pleasant" hashes to 583. Without the second hashing they would collide to 2. But using signed hashing we have a 50% chance that they will cancel each other out, which tries to stop one feature from growing too much.
```

The main downsides from using feature hashing are:

- Still has one tuning parameter
- Can't be reversed

The number of buckets you have correlates with computation speed and collision rate which in turn affects performance. 
It is your job to find the output that best suits your need.
Increasing the number of buckets will decrease the collision rate but will, in turn, return a larger output data.frame which can make fitting your model take a longer time. 
The number of buckets is tunable in tidymodels using the tune package.

The perhaps greater downside to using feature hashing is that the operation can't be reversed. 
We are not able to detect if a collision occurs and it is going to be hard to figure out the effect of each word in the document.
Remember that we are just left with n columns, so if we find that the 274th column is a highly predictive feature, then we are not certain which tokens contributed to that column. 
We could go back to our training set and create a paired list of the tokens and what they are mapped to find the possible tokens.
And sometimes we might find only 1 token in that list and we know what it is. 
But it might as well have 4 different tokens contributing.
This method is used because of its speed and scalability, not because it is interpretable.

Performing feature hashing on tokens is done using the `step_texthash()` step from textrecipes.

```{r}
complaints_hash <- recipe(product ~ consumer_complaint_narrative, data = complaints_train) %>%
  step_tokenize(consumer_complaint_narrative) %>%
  step_texthash(consumer_complaint_narrative, signed = TRUE, num_terms = 512) %>%
  prep() %>%
  juice()

complaints_hash
```

By using `step_texthash()` are we able to quickly generate machine-ready data with a consistent number of variables.
This will have a slight loss of power compared to using a traditional bag-of-words representation. An example of the loss in performance is illustrated in this [textrecipes blogpost](https://www.hvitfeldt.me/blog/textrecipes-series-featurehashing/).

### text normalization

When working with text you will inevitably run into problems with encodings and irregularities.
One of these problems has a very big influence on how feature hashing is performed.
Consider the German word "sch√∂n".
The o with an umlaut (two dots over it) is a fairly simple character but it can be represented in a couple of different ways in data.
We can either use a single character [\\U00f6](https://www.fileformat.info/info/unicode/char/00f6/index.htm) to represent the letter with umlaut. 
Or we can use two characters, one for the o and one character to denote the presence of two dots over the previous character [\\U0308](https://www.fileformat.info/info/unicode/char/0308/index.htm)

```{r}
s1 <- "sch\U00f6n"
s2 <- "scho\U0308n"
```

These two strings will print the same when we visually expect the data

```{r}
s1
s2
```

But they are not equal.

```{r}
s1 == s2
```

This because a problem with the avalanche effect which is needed for feature hashing to perform correctly. The avalance effect will make it ss these two seamlessly identical words will be hashing to completely different values

```{r}
murmur3.32(s1)
murmur3.32(s2)
```

We can deal with this problem by performing text normalization on our text before feeding it into your preprocessing engine.
One library to perform text normalization is the `stringi` package which comes with many different methods.
How these methods work is beyond the scope of this book. All you need to know is that the text normalization functions will make equivalent text equivalent. We will use `stri_trans_nfc()` for this example, which will perform Canonical Decomposition, followed by Canonical Composition.

```{r}
library(stringi)

stri_trans_nfc(s1) == stri_trans_nfc(s2)

murmur3.32(stri_trans_nfc(s1))
murmur3.32(stri_trans_nfc(s2))
```

And we see that the strings are equal after normalization.

This issue is still important even if you don't use feature hashing. Since these words are different in data then they will be counted separately when we are counting token frequencies. 
Having a token have different representations could effectively split the frequencies, which would introduce noise in the best case and the worst case make some tokens fall below the cutoff when we select the tokens, leading to a loss of a potential informative words.

Luckily this is easily fixed by using `stri_trans_nfc()` on our text columns before starting preprocessing.


## What evaluation metrics are appropriate

We have so far used [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) and the area under the receiver operator curve (roc_auc) for hours models so far, these are the defaults used in classification tasks in tidymodels. These are not the only kind of metrics you can use and the choise will depend on how much you value false positives and false negatives.

If you know before you fit your model that you want to compute one or more of these metrics, you can specify them in a call to `metric_set()`. Let's set up a tuning grid for recall (`recall`) and precision (`precision`).

```{r, eval = FALSE}
nb_rs <- fit_resamples(
  nb_wf,
  complaints_folds,
  metrics = metric_set(recall, precision)
)
```

If you have already fit your model, you can still compute and explore non-default metrics as long as you saved the predictions for your resampled datasets using `control_resamples(save_pred = TRUE)`. 

Let's go back to the lasso model we tuned in Section \@ref(classfirstmodel), with results in `nb_rs`. We can compute the [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall).

```{r}
nb_rs_predictions %>%
  recall(product, .pred_class)
```

We can also compute the recall for each resample by using `group_by()` to calculate inside each resample.

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  recall(product, .pred_class)
```

Many of the matrics used for classification is some function of the true positive, true negative, false positive and false negative rates. 
We can take a look at the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) to look at the contingency table between observed classes and predicted classes.

```{r}
nb_rs_predictions %>%
  filter(id == "Fold01") %>%
  conf_mat(product, .pred_class)
```

It is possible to archive high accuracy with a bad model just by predicting the majority class. This is not favorable as we are not using any information from the data to guide our predictions.

```{block, type = "rmdnote"}
For the full set of regression metric options, see the [yardstick documentation](https://yardstick.tidymodels.org/reference/).
```


## Full game

We have come a long way since our first model \ref(classfirstmodel) and it is time to see how we can use what we have learned about our data to improve it.
We started this chapter with a simple Naive Bayes model and n-gram counts.
Since then have we looked at different models, preprocessing techniques, and domain-specific feature extractions.
For our final model, let's use some of the domain-specific features we developed in \@ref(custumfeatures) along with n-grams. The model we are using doesn't have any significant hyperparameters we can tune but we can tune the number of tokens we will include in our model. For this final model will we:

- train on the same set of cross-validation resamples used throughout this chapter,
- tune the number of tokens used in the model,
- include bigrams,
- include custom made features,
- remove the Snowball stop word lexicon,
- finally evaluate on the testing set, which we have not touched at all yet.

### Feature selection

We start by creating a new recipe. 
We are using the same predictors as the first time and the way we are handling `date_received` and `tags` will stay the same.

```{r complaintrecfinal1, dependson="complaintssplit"}
complaints_rec_v2 <- 
  recipe(product ~ date_received + tags + consumer_complaint_narrative, 
         data = complaints_train) %>%
  step_date(date_received, features = c("month", "dow"), role = "dates") %>%
  step_rm(date_received) %>%
  step_dummy(has_role("dates")) %>%
  step_unknown(tags) %>%
  step_dummy(tags)
```

After exploring the data more in \ref(custumfeatures) it seems appropriate to add these features in our final model.
To do this will we use `step_textfeature()` to calculate custom text features for our data. 
We create a list of the custom text features we want to include and pass it to `step_textfeature()` using the `extract_functions` argument. 
Note how we have to take a copy of `consumer_complaint_narrative` using `step_mutate()` as `step_textfeature()` consumes the column.


```{r complaintrecfinal2, dependson="complaintrecfinal1"}
extract_funs <- list(creditcard_count, procent_censoring, max_money)

complaints_rec_v2 <- complaints_rec_v2 %>%
  step_mutate(narrative_copy = consumer_complaint_narrative) %>%
  step_textfeature(narrative_copy, extract_functions = extract_funs)
```

The tokenization and stop word removal will commence like normal, however, this time will we count bi-grams instead of unigrams.
Last time we only included 100 tokens which aren't that many. 
Let us treat the number of parameters as a hyperparameter that we will vary when we tune the final model.
I have also set the `min_times` argument to 50, this will throw away tokens if they appeared less then 50 times in the entire corpus.
We want the model to be robust and a token needs to appear a certain number of times before it turns from noise to signal.

```{block, type = "rmdnote"}
This dataset has much more than 50 of the most common 1000 tokens. It is still a good practice to specify to be safe. 50 is also an arbitrary choice and needs to be picked depending on your preferences on the robustness of your model.
```

```{r complaintrecfinal3, dependson="complaintrecfinal2"}
complaints_rec_v2 <- complaints_rec_v2 %>%
  step_tokenize(consumer_complaint_narrative) %>%
  step_stopwords(consumer_complaint_narrative) %>%
  step_ngram(consumer_complaint_narrative, num_tokens = 2) %>%
  step_tokenfilter(consumer_complaint_narrative, 
                   max_tokens = tune(), min_times = 250) %>%
  step_tfidf(consumer_complaint_narrative)
```

### Specifying models

We will be using the Support Vector Machine model since it showed some promise in \@ref(comparetosvm).

We can reuse parts of the old workflow and simply update the recipe specificatioon

```{r nbwf2, dependson=c("svmspec", "complaintrecfinal3")}
svm_wf_v2 <- svm_wf %>%
  update_recipe(complaints_rec_v2)

svm_wf_v2
```

We create a grid of possible hyperparameter values using `grid_regular()` from the dials package. 
The levels is set to 5 to give us 5 possible values of the maximum number of tokens.

```{r}
param_grid <- grid_regular(max_tokens(range = c(250, 1500)),
                           levels = 5)

param_grid
```

```{r, message=FALSE, eval=!online}
set.seed(2020)
tune_rs <- tune_grid(
  svm_wf_v2,
  complaints_folds,
  grid = param_grid, 
  metrics = metric_set(accuracy, sensitivity, specificity),
  control = control_resamples(save_pred = TRUE)
)
```

```{r include=FALSE, eval=!online}
tune_rs$splits <- NULL
write_rds(tune_rs, "data/ml_class_final_tune.rds")
```

```{r, include=FALSE, eval=online}
tune_rs <- read_rds("data/ml_class_final_tune.rds")
```

now that the hyper parameter tuning is finished we can take a look at the best performing sets

```{r}
show_best(tune_rs, metric = "accuracy")
```

it appears that 550 tokens is a good middleground before we get to overfitting.
We can extract the best hyperparameters and use it to finalize the workflow by setting the

```{r}
best_accuracy <- select_best(tune_rs, "accuracy")

svm_wf_final <- finalize_workflow(
  svm_wf_v2,
  best_accuracy
)

svm_wf_final
```

lastly, we will do a list fit and see how well the final model did on our testing data:

```{r finalres}
final_res <- svm_wf_final %>%
  last_fit(complaints_split, metrics = metric_set(accuracy))
```

We will again extract the relevant information using `collect_metrics()` and `collect_predictions()`

```{r finalresmetrics, dependson="finalres", eval = !online}
final_res_metrics <- collect_metrics(final_res)
final_res_predictions <- collect_predictions(final_res)
```

```{r, include = FALSE, eval = !online}
write_rds(final_res_metrics, "data/ml_class_final_res_metrics.rds")
write_rds(final_res_predictions, "data/ml_class_final_res_predictions.rds")
```

```{r, include = FALSE, eval = online}
final_res_metrics <- read_rds("data/ml_class_final_res_metrics.rds")
final_res_predictions <- read_rds("data/ml_class_final_res_predictions.rds")
```

If we take a look at the metrics we did pretty good

```{r}
final_res_metrics
```

And it is worth noting that it isn't much different from what we saw in the training dataset. This is good and an indication that our model was able to generalize well enough.

Taking a look at the confusion matrix in figure \@ref(fig:finalheatmap) also yields pleasing results. It appears symmetric with a strong presence in the diagonal showing us that there isn't any strong bias one towards either of the classes.

```{r, finalheatmap, fig.cap="Confusion matrix for final Support Vector Machine classifier."}
final_res_predictions %>% 
  conf_mat(truth = product, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

## Summary

You can use classification modeling to predict classes from a dataset, including datasets that have text in them.
Naive Bayes models perform reasonably well since each feature and token is handled independently and can, therefore, handle many variables with computationally ease.
This is a useful feature as we sometimes want thousands of tokens in our bag-of-word model.
We also saw that Support Vector Machines performed well with keeping the classes separated.
We also saw how handcrafted features can help improve your model.
By using resampling data sets of the training set can we explore the space of hyperparameters to find the most performant model all without using the testing dataset. Keeping the testing dataset for a final evaluation.

### In this chapter, you learned:

- how text can be used to enhance a classification model
- to tune the hyperparameters in the preprocessing stage
- how to compare different model types
- how feature hashing can be used as a fast alternative to bag-of-words
- about how to look at the data and construct custom features
- about performance metrics for classification models
