# Classification {#dlclassification}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE, eval = TRUE,
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
options(crayon.enabled = FALSE)
doParallel::registerDoParallel()

## for Julia's local environment
#spacyr::spacy_initialize(condaenv = "r-spacyr", entity = FALSE)
#online <- FALSE

## for GH actions
online <- TRUE
```

In this chapter, we will predict binary values, much like we did in Chapter \@ref(mlclassification), but we will use deep learning methods instead. We will be using a dataset of fundraising campaigns from [Kickstarter](https://www.kickstarter.com/).

```{r kickstarter}
library(tidyverse)

kickstarter <- read_csv("data/kickstarter.csv.gz")
kickstarter
```

we are working with fairly short texts for this dataset. less than a couple of hundred characters. We can look at the distribution

```{r kickstartercharhist, dependson="kickstarter", fig.cap="Distribution of character count for Kickstarter campaign blurbs"}
kickstarter %>%
  ggplot(aes(nchar(blurb))) +
  geom_histogram(binwidth = 1) +
  labs(x = "Number of characters per campaign blurb",
       y = "Number of campaign blurbs")
```

it is rightly skewed which is to be expected. Since you don't have much space to make your impression most people choose to use most of it. There is one odd thing happening in this chart. There is a drop somewhere between 130 and 140. Let us investigate to see if we can find the reason.

We can use `count()` to find the most common `blurb` length.

```{r}
kickstarter %>%
  count(nchar(blurb), sort = TRUE)
```

it appears to be 135 which in and of itself doesn't tell us much. It might be a glitch in the data collection process. Let us put our own eyes to look at what happens around this cutoff point. We can use `slice_sample()` to draw a random sample of the data.

We start by looking at `blurb`s with exactly 135 characters, this is done so that we can identify if the `blurb`s were cut short at 135 characters.

```{r}
set.seed(1)
kickstarter %>%
  filter(nchar(blurb) == 135) %>%
  slice_sample(n = 5) %>%
  pull(blurb)
```

It doesn't appear to be the case as all of these `blurb`s appear coherent and some of them even end with a period to end the sentence. Let us now look at `blurb`s with more than 135 characters if these are different.

```{r}
set.seed(1)
kickstarter %>%
  filter(nchar(blurb) > 135) %>%
  slice_sample(n = 5) %>%
  pull(blurb)
```

All of these `blurb`s also look good so it doesn't look like a data collection issue. The `kickstarter` dataset also includes a `created_at` variable. Let us see what we can gather with that new information.

Below is a heatmap of the lengths of `blurb`s and the time the campaign was posted.

```{r kickstarterheatmap, dependson="kickstarter", fig.cap="Distribution of character count for Kickstarter campaign blurbs over time"}
kickstarter %>%
  ggplot(aes(created_at, nchar(blurb))) +
  geom_bin2d() +
  labs(x = NULL,
       y = "Number of characters per campaign blurb")
```

We see a trend right here. it appears that at the end of 2010 there was a policy change to have the blurb length shortened from 150 characters to 135 characters.

```{r}
kickstarter %>%
  filter(nchar(blurb) > 135) %>%
  summarise(max(created_at))
```

We can't tell for sure if the change happened in 2010-10-20, but that is the last day a campaign was launched with more than 135 characters.

## A first classification model {#firstdlclassification}

Much like all our previous modeling, our first step is to split our data into training and testing sets. We will still use our training set to build models and save the testing set for a final estimate of how our model will perform on new data. It is very easy to overfit deep learning models, so an unbiased estimate of future performance from a test set is more important than ever. This data will be hard to work with since we don't have much information to work with.

We use `initial_split()` to define the training/testing split. We will focus on modeling the `blurb` alone in this chapter. We will restrict the data to only include `blurb`s with more than 15 characters. The short `blurb`s tend to uninformative single words.

```{r dojsplit, dependson="doj"}
library(tidymodels)
set.seed(1234)
kickstarter_split <- kickstarter %>%
  filter(nchar(blurb) >= 15) %>%
  initial_split()

kickstarter_train <- training(kickstarter_split)
kickstarter_test <- testing(kickstarter_split)
```

There are `r scales::comma(nrow(kickstarter_train))` press releases in the training set and `r scales::comma(nrow(kickstarter_test))` in the testing set.

## Preprocessing for deep learning

The way we will be doing preprocessing requires a hyperparameter denoting the length of sequences we would like to include. We need to select this value such that we don't overshoot and introduce a lot of padded zeroes which would make the model hard to train, and we also need to avoid picking too short of a range.

We can use the `count_words()` function from the tokenizers package to calculate the number of words and generate a histogram. Notice how we are only using the training dataset to avoid data leakage when selecting this value.

```{r kickstarterwordlength, fig.cap="Distribution of word count for Kickstarter campaign blurbs"}
kickstarter_train %>% 
  mutate(n_words = tokenizers::count_words(blurb)) %>%
  ggplot(aes(n_words)) +
  geom_bar() +
  labs(x = "Number of words per campaign blurb",
       y = "Number of campaign blurbs")
```

Given that we don't have many words, to begin with, it makes sense to err on the side of longer sequences since we don't want to lose valuable data. I would suppose that 30 would be a good cutoff point.

```{r}
library(textrecipes)

max_words <- 20000
max_length <- 30

prepped_recipe <- recipe(~blurb, data = kickstarter_train) %>%
  step_tokenize(blurb) %>%
  step_tokenfilter(blurb, max_tokens = max_words) %>%
  step_sequence_onehot(blurb, sequence_length = max_length) %>%
  prep()

prepped_training <- prepped_recipe %>%
  bake(new_data = NULL, composition = "matrix")
```

### One-hot sequence embedding of text

We have used `step_sequence_onehot()` to transforms the tokens into a numerical format, the main difference here is that this format takes into account the order of the tokens, unlike `step_tf()` and `step_tfidf()` which doesn't take order into account. `step_tf()` and `step_tfidf()` are called bag-of-words for this reason. Let us take a closer look at how `step_sequence_onehot()` works and how its parameters will change the output.

When we are using `step_sequence_onehot()` two things are happening. First, each word is being assigned an integer index. You can think of this as key-index pair of the vocabulary. Next, the sequence of tokens will be replaced with their corresponding index and it is this sequence of integers that make up the final numerical representation. To illustrate here is a small example:

```{r sequence_onhot_rec}
small_data <- tibble(
  text = c("Hello there you",
           "I love you very much",
           "I just said hello to you",
           "I have another very very very very very long text")
  )

small_spec <- recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "") %>%
  prep()
```

Once we have the `prep()`ed recipe then we can `tidy()` it to extract the vocabulary. It is being represented in the `vocabulary` and `token` columns.

```{r sequence_onhot_rec_vocab, dependson="sequence_onhot_rec"}
small_spec %>%
 tidy(2)
```

```{block, type = "rmdnote"}
The `terms` columns refer to the column we have applied `step_sequence_onehot()` to and `id` is its unique identifier. Note that textrecipes allow `step_sequence_onehot()` to be applied to multiple text variables independently and they will have their own vocabularies.
```

If we take a look at the resulting matrix we have 1 row per observation. The first row starts with some padded zeroes but turns into 3, 11, 14, which when matched with the vocabulary can construct the original sentence.

```{r sequence_onhot_rec_matrix1, dependson="sequence_onhot_rec"}
small_spec %>%
  juice(composition = "matrix")
```

But wait, the 4th line should have started with a 4 since the sentence starts with "I" but the first number is 13. This is happening because the sentence is too long to fit inside the specified length. This leads us to ask 3 questions before using `step_sequence_onehot()`

1.  How long should the output sequence be?
2.  What happens to too long sequences?
3.  What happens to too short sequences?

Choosing the right length is a balancing act. You want the length to be long enough such that you don't truncate too much of your text data, but still short enough to keep the size of the data down and to avoid excessive padding. Truncating, having large data output and excessive padding all lead to worse model performance. This parameter is controlled by the `sequence_length` argument in `step_sequence_onehot()`. If the sequence is too long then we need to truncate it, this can be done by removing values from the beginning ("pre") or the end ("post") of the sequence. This choice is mostly influenced by the data, and you need to evaluate where most of the extractable information of the text is located. News articles typically start with the main points and then go into detail. If your goal is to detect the broad category then you properly want to keep the beginning of the texts, whereas if you are working with speeches or conversational text, then you might find that the last thing to be said carries more information and this would lead us to truncate from the beginning. Lastly, we need to decide how the padding should be done if the sentence is too short. Pre-padding tends to be more popular, especially when working with RNN and LSTM models since having post-padding could result in the hidden states getting flushed out by the zeroes before getting to the text itself.

`step_sequence_onehot()` defaults to `sequence_length = 100`, `padding = "pre"` and `truncating = "pre"`. If we change the truncation to happen at the end

```{r sequence_onhot_rec_matrix2, dependson="sequence_onhot_rec"}
recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "", 
                       padding = "pre", truncating = "post") %>%
  prep() %>%
  juice(composition = "matrix")
```

then we see the 4 at the beginning of the last row representing the "I". The starting points are not aligned since we are still padding on the left side. We left-align all the sequences by setting `padding = "post"`.

```{r sequence_onhot_rec_matrix3, dependson="sequence_onhot_rec"}
recipe( ~ text, data = small_data) %>%
  step_tokenize(text) %>%
  step_sequence_onehot(text, sequence_length = 6, prefix = "", 
                       padding = "post", truncating = "post") %>%
  prep() %>%
  juice(composition = "matrix")
```

Now we have that all the 4s neatly aligned in the first column.

### Simple Flattened Dense network

The model we will be starting with a model that embeds sentences in sequences of vectors, flattening them, and then trains a dense layer on top.

```{r dense_model}
library(keras)

dense_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, 
                  output_dim = 12,
                  input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

dense_model
```

Let us step through this model specification one layer at a time.
We start the keras model by using `keras_model_sequential()` to indicate that we want to compose a linear stack of layers.
Our first layer is an embedding layer via `layer_embedding()` This layer is e equipped to handle preprocessed data we have in `prepped_training`. It will take each observation/row in `prepped_traning` and embed each token to an embedding vector. This will result in each observation being turned into an (embedding_dim x sequence_length) matrix witch would be a (12 x 30) matrix with our settings, creating a (number of observations x embedding_dim x sequence_length) tensor.
The `layer_flatten()` layer that follows takes the 2-dimensional tensors for each observation and flattens it down into 1 dimension. This will create a `30 * 12 = 360` tensor for each observation. Lastly, we have 2 densely connected layers with the last layer having a sigmoid activation function to give us a number between 0 and 1.

Now that we have specified the architecture of the model we still have a couple of things left to add to the model before we can fit it to the data. A keras model requires an optimizer and a loss function to be able to compile. When the neural network finished passing a batch of data through the network it needs to find a way to use the difference between the predicted values and true values to update the weights. the algorithm that determines those weights is known as the optimization algorithm. keras comes pre-loaded with many optimizers^[https://keras.io/api/optimizers/] and you can even create custom optimizers if what you need isn't on the list. We will start by using the rmsprop optimizer.

```{block, type = "rmdnote"}
An optimizer can either be set with the name of the optimizer as a character or by supplying the function `optimizer_*()` where `*` is the name of the optimizer. If you use the function then you can specify parameters for the optimizer.
```

During training, we need to calculate a quantity that we want to have minimized. This is the loss function, keras comes pre-loaded with many loss functions^[https://keras.io/api/losses/]. These loss function will typically take in two values, typically the true value and the predicted value, and return a measure of how close they are. 
Since we are working on a binary classification task and have the final layer of the network return a probability, then we find binary cross-entropy to be an appropriate loss function. Binary cross-entropy does well at dealing with probabilities as it measures the “distance” between probability distributions. which would in our case be between the ground-truth distribution and the predictions.

We can also add any number of `metrics`^[https://keras.io/api/metrics/] to be calculated and reported during training. These metrics will not affect the training loop which is controlled by the optimizer and loss function. The metrics job is to report back a single number that will inform you of the user how well the model is performing. We will select accuracy as our metric for now. We can now set these 3 options; optimizer, loss, and metrics using the `compile()` function

```{r dense_model_compiled, dependson="dense_model", eval=FALSE}
dense_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{block, type = "rmdnote"}
Notice how the `compile()` function modifies the network in place. This is different then what is conventionally done in R where a new network object would have been returned.
```

Finally, we can fit the model. When we `fit()` a keras model we need to supply it with the data we are having the model train on. We need to supply this a matrix of predictors `x` and a numeric vector of labels `y`.
This is sufficient information to start training the model. We are going to specify a couple more arguments to get better control of the training loop. First, we set the number of observations to pass through at a time with `batch_size`, and we set `epochs = 10` to tell the model to pass all the data through the training loop 10 times. Lastly, we set `validation_split = 0.2` to specify an internal validation split for when the metrics are calculated.

```{r dense_model_history, dependson="dense_model_compiled", eval=FALSE}
history <- dense_model %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 10,
  validation_split = 0.2
)
```

We can visualize the results of the training loop by `plot()`ing the `history.

```{r dense_model_history_plot, dependson="dense_model_history", fig.cap="Training and validation metrics for dense network", eval=FALSE}
plot(history)
```


Now that we have the fitted model can we apply the model to our testing dataset to see how well the model performs on data it hasn't seen.

```{r dense_model_evaluate, dependson="dense_model_compiled", eval=FALSE}
dense_model %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

we see that the accuracy very closely resembles the val_accuracy from the training loop, suggesting that we didn't overfit our model.

### Modeling

### Evaluation

## Preprocessing

Mostly the same, we still want to end with all numerics. Use keras/tensorflow to do preprocessing as an example.

## putting your layers together

## Use embedding

## Model tuning

## Case Study: Vary NN specific parameters

## Look at different deep learning architecture

## Case Study: Applying the wrong model

Here will we demonstrate what happens when the wrong model is used. Model from ML-classification will be used on this dataset.

## Full game

All bells and whistles.
