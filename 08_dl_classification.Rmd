# Classification {#dlclassification}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE, eval = FALSE,
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
options(crayon.enabled = FALSE)
doParallel::registerDoParallel()

## for Julia's local environment
#spacyr::spacy_initialize(condaenv = "r-spacyr", entity = FALSE)
#online <- FALSE

## for GH actions
online <- TRUE
```

## Convolutional Neural Networks

The first networks we have shown in this chapter doesn't take advantage of the sequential patterns.
Text can have patterns of varying length, and this can be hard for a simple dense network to pick up on and learn.
Patterns can be encoded as n-grams\@ref(tokenizingngrams), but this presents problems if you want to encode these n-grams directly since the dimensionality of the vocabulary shoots up even we just try to capture `n = 2` and `n = 3`.

The convolutional neural network (CNN) architecture is the most complicated network architecture we have seen so far, so we will take some time to review the construction, the different features, and the hyperparameters you can tune. The goal of this section is to give you an intuition on how each aspect of the CNN affects the behavior. CNNs are well suited to pick up on spatial structures within the data, this is a powerful feature for working with text since text typically contains a good amount of local structure within the text, especially when characters are used as the token. CNNs become efficient layers by having a small number of weights which is used to scan the input tensor, the output tensor that is produced then hopefully can represent specific structures in the data.

It is worth noting that a CNN isn't trying to learn long term structure, but rather detect local patterns along the sequence.

CNNs can work with 1, 2, 3-dimensional data, but it will mostly involve only 1 dimension when we are using it on text, the following illustrations and explanations will be done in 1 dimension to closely match the use-case we are looking at for this book. 
Figure \@ref(fig:cnn-architecture) illustrates a stereotypical CNN architecture.
You start with your input sequence, this example uses characters as the token, but it could just as well be words.
Then a filter slides along the sequence to produce a new and smaller sequence. This is done multiple times, typically with varying parameters for each layer until we are left with a small tensor which we then transform into our required output shape, 1 value between 0 and 1 in the case of classification.

```{r cnn-architecture, echo= FALSE, fig.cap="A template CNN architecture for 1 dimensional input data. A sequence of consequtive CNN layers will incremently reduce the tensor size, ending up with single value."}
knitr::include_graphics("diagram-files/cnn-architecture.png")
```

This figure lies a little bit since we technically don't feed characters into it, but instead uses sequence one-hot encoding with a possible word embedding.
We will now go through some of the most important concepts about CNNs.

### Filters

The kernel is a small tensor of the same dimensionality as the input tensor that slides along the input tensor. When it is sliding it performs element-wise multiplication of the values in the input tensor and its weights and then summing up the values to get a single value. 
Sometimes an activation function will be applied as well.
It is these weights that are trained with gradient descent to find the best fit.
In keres, the `filters` represent how many different kernels are trained in each layer. You typically start with fewer filters at the beginning of your network and then increase them as you go along. 

### Kernel size

The most prominent hyperparameter is the kernel size. 
The kernel size is the size of the tensor, 1 dimensional is this case, that contains the weights. A kernel with size 5 will have 5 weights. These kernels will similarly capture local information to how n-grams capture location patterns. Increasing the size of the kernel will decrease the size of the output tensor, as we see in figure \@ref(fig:cnn-kernel-size)

```{r cnn-kernel-size, echo= FALSE, fig.cap="The kernel size affects the size of the resulting tensor. A kernel size of 3 uses the information from 3 values to calculate 1 value."}
knitr::include_graphics("diagram-files/cnn-kernel-size.png")
```

Larger kernels will detect larger and less frequent patterns where smaller kernels will find fine-grained features. 
Notice how the choice of the token will affect how we think about kernel size. 
For character level tokens a kernel size of 5 will in early layers find patterns in parts of words more often than patterns across words since 5 characters aren't enough the adequately span multiple words. 
Where on the other hand a kernel size of 5 for word-level tokens will find patterns in parts of sentences instead. Kernels most have an odd length.

### Stride

The stride is the second big hyperparameter that controls the kernels in a CNN. The stride length determines how much the kernel moves along the sequence between each calculation. A stride length of 1 means that the kernel moves over one place at a time, this way we get maximal overlap.

```{r cnn-stride, echo= FALSE, fig.cap="The stride length affects the size of the resulting tensor. When stride = 1 then the window slides along one by one. Increasing the slide length decreases the resulting tensor by skipping windows."}
knitr::include_graphics("diagram-files/cnn-stride.png")
```

In figure \@ref(fig:cnn-stride) we see that if the kernel size and stride length are equal then there is no overlap. We can decrease the size of the output tensor by increasing the stride length. Be careful not to set the stride length to be larger than the kernel size, otherwise, then you will skip over some of the information.

### Dilation

The dilation controls how the kernel is applied to the input tensor.
So far we have shown examples where the dilation is equal to 1. This means that each value from the input tensor will be spaced 1 distance apart from each other.

```{r cnn-dilation, echo= FALSE, fig.cap="The dilation affects the size of the resulting tensor. When dilation = 1 then consecutive values are taking from the input. Increasing the dilation leaves gaps between input values and decreases the resulting tensor."}
knitr::include_graphics("diagram-files/cnn-dilation.png")
```

If we increase the dilation then can see in figure \@ref(fig:cnn-dilation) that there will be spaces or gaps between the input values. This allows the kernel to find large spatial patterns that span many tokens.
This is a useful trick to be able to extract features and structure from long sequences. Dilated convolutional layers when put in succession will be able to find patterns in very large sequences.

### Padding

The last hyperparameter we will talk about is padding.
One of the downsides to how the kernels are being used in the previous figures is how it handles the edge of the sequence.
Padding is the act of putting something before and after the sequence when the convolution is taking place to be able to extract more information from the first and last tokens in the sequence. Padding will lead to larger output tensors since they we let the kernel move more.

### Simple CNN

We will start with a fairly standard CNN specification that closely follows what we saw in figure \@ref(fig:cnn-architecture).
We start with an embedding layer followed by a sequence of 1 dimensional convolution layers `layer_conv_1d()`, followed by a global max pooling layer `layer_global_max_pooling_1d()` and a dense layer with a sigmoid activation function to give us 1 value between 0 and 1 to use in our classification.

```{r simple_cnn_model, eval=FALSE}
simple_cnn_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 16,
                  input_length = max_length) %>%
  layer_conv_1d(filter = 16, kernel_size = 11, activation = "relu") %>%
  layer_conv_1d(filter = 32, kernel_size = 9, activation = "relu") %>%
  layer_conv_1d(filter = 64, kernel_size = 7, activation = "relu") %>%
  layer_conv_1d(filter = 128, kernel_size = 5, activation = "relu") %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 1, activation = "sigmoid")

simple_cnn_model
```

We are using the same embedding layer as we did in the previous networks so there is nothing new there.
We are having 4 convolutional layers. And there are some things to take note off here.
The model is using an increasing number of filters in each layer, doubling the number of filters for each layer. This is to make sure that there are more filter latter on to capture enough of the global information.
The kernel size in this model starts kinda high and then slowly decreases. This model will be able to find quite large patterns in the data.
We use a `layer_global_max_pooling_1d()` layer to transform to collapse the remaining CNN output into 1 dimension and we finish it off with a densely connected layer and a sigmoid activation function.

This might not end up being the best CNN configuration, but it is a good starting point.
One of the challenges when working with CNNS is to make sure that you manage the dimensionality correctly.
You will have to handle the trade-off between having a small number of layers with hyperparameters that are set to decrease the dimensions drastically, or having a larger amount of layers where each output is only slightly smaller then the previous.
Networks with fewer layers can give good performance and fast since there isn't that many weights to train, but you need to be careful that you construct the layers to correctly capture the patterns you want.

The compilation and fitting is the same as we have seen before. 

```{r simple_cnn_model_fit, dependson="simple_cnn_model", eval=FALSE}
simple_cnn_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- simple_cnn_model %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 10,
  validation_split = 0.2
)
```

We are using the `"adam"` optimizer since it performs well for this model. 

```{block, type = "rmdnote"}
You will have to experiment to find the optimizer that works best for your specific model. Different optimizers work differently in different situations and it is up to you to find which works the best for your model and data.
```

Now that the model is done fitting we can evaluate it on testing data set.

```{r simple_cnn_model_evaluation, dependson="simple_cnn_model_fit", eval=FALSE}
simple_cnn_model %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

We are seeing some improvement over the densely connected network. This is a good development, what we hoped to see was an improvement in our model.

## Character level Convolutional Neural Network

In our models so far we have used "words" as the token of interest. Another choice of token could be "character". Since this data set contains very short texts then we don't have many words to work with. We have filtered the data set to have a minimum text length of 15 and while that helps it doesn't stop the fact that many of the texts will have 1 or 2 words in total.

The idea of using character-level CNNs is nothing new, they have been explored by @Zhang2015 and work quite well on small shorter texts such as headlines and tweets[@Vosoughi2016].
These kinds of models will be able to detect patterns of the characters inside the words, which means that these models can have very favorable performance in languages with rich morphology all while having a low number of trainable parameters.

We need to remind ourselves that these models don't contain any linguistic knowledge at all, they only "know" the patterns of sequences of characters in the training set. This is not to say the models are useless, but to set our expectations of what the model is capable of, namely pattern detection.

Since we are using a completely different preprocessing setup we need to specify a new recipe. This recipe should tokenize to characters and instead of specifying the maximal number of tokens we want, we instead specify the tokens directly. These tokens will be our alphabet.
The paper by @Zhang2015 uses an alphabet consisting of 70 characters, including 26 English letters, 10 digits, 33 other characters, and the new line character.

```{text, eval=FALSE}
abcdefghijklmnopqrstuvwxyz0123456789
-,;.!?:’’’/\|_@#$%ˆ&*˜‘+-=<>()[]{}
```

A model using this alphabet would properly work but we should tailor it more to the data we have at hand. Many of the "other" characters are used for punctuation, something that these blurbs don't contain much of. Neither are numbers used much. Let's go simple and only use the 26 letters and spaces.
Before we move on we can double-check that this is a reasonable choice by using the `unnest_characters()` function from the tidytext package to tokenize to characters and then count the different characters.

```{r}
library(tidytext)
kickstarter_train %>%
  unnest_characters(char, blurb, strip_non_alphanum = FALSE) %>%
  count(char, sort = TRUE)
```

If you dig in this list then you find that the frequencies quickly drop off once you get past the letters. We are turning all characters to lowercase, while case does matter some in the text, doubling the alphabet size does not seem worth it.

```{r}
charlevel_recipe <- recipe(~blurb, data = kickstarter_train) %>%
  step_tokenize(blurb, token = "characters", 
                options = list(strip_non_alphanum = FALSE)) %>%
  step_sequence_onehot(blurb, 
                       sequence_length = 70, 
                       vocabulary = c(letters, " "))%>%
  prep()

charlevel_training <- charlevel_recipe %>%
  bake(new_data = NULL, composition = "matrix")
```

We can confirm that this is working by looking at the first observation. 
The leading zeroes are happening because we are pre-padding with zeroes

```{r}
unname(charlevel_training[1, ])
```

to understand what the indices mean we can extract the vocabulary of `step_sequence_onehot()` using `tidy()` on the prepped recipe.

```{r}
tidy(charlevel_recipe, 2)
```

And we are lucky in this case since the vocabulary is in alphabetical order.

## Using different optimizers

## Case Study: Vary NN specific parameters

## Look at different deep learning architecture

## Case Study: Applying the wrong model

Here will we demonstrate what happens when the wrong model is used. Model from ML-classification will be used on this dataset.

## Full game

All bells and whistles.