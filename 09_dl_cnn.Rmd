# Convolutional Neural Network {#dlcnn}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE, eval = TRUE,
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
options(crayon.enabled = FALSE)
doParallel::registerDoParallel()

## for Julia's local environment
#spacyr::spacy_initialize(condaenv = "r-spacyr", entity = FALSE)
#online <- FALSE

## for GH actions
online <- TRUE

keras_predict <- function(model, recipe, data, response) {
  baked_data <- bake(recipe, data, composition = "matrix")
  prediction <- predict(model, baked_data)[, 1]
  
  data %>%
    mutate(
      .pred_prop = prediction,
      .pred_class = if_else(.pred_prop < 0.5, 0, 1)
    ) %>%
    mutate(across(c({{ response }}, .pred_class), 
                  ~ factor(.x, levels = c(1, 0))))
}
```

The first networks\@ref(dldnn) we have shown in this section don't take advantage of the sequential patterns.
Text can have patterns of varying length, and this can be hard for a simple densely connected network to pick up on and learn.
Patterns can be encoded as n-grams\@ref(tokenizingngrams), but this presents problems if you want to encode these n-grams directly since the dimensionality of the vocabulary shoots up even we just try to capture `n = 2` and `n = 3`.

The convolutional neural network (CNN) architecture is the most complicated network architecture we have seen so far, so we will take some time to review the construction, the different features, and the hyperparameters you can tune. The goal of this section is to give you an intuition on how each aspect of the CNN affects the behavior. CNNs are well suited to pick up on spatial structures within the data, this is a powerful feature for working with text since text typically contains a good amount of local structure within the text, especially when characters are used as the token. CNNs become efficient layers by having a small number of weights which is used to scan the input tensor, the output tensor that is produced then hopefully can represent specific structures in the data.

It is worth noting that a CNN isn't trying to learn long term structure, but rather detect local patterns along the sequence.

CNNs can work with 1, 2, 3-dimensional data, but it will mostly involve only 1 dimension when we are using it on text, the following illustrations and explanations will be done in 1 dimension to closely match the use-case we are looking at for this book. 
Figure \@ref(fig:cnn-architecture) illustrates a stereotypical CNN architecture.
You start with your input sequence, this example uses characters as the token, but it could just as well be words.
Then a filter slides along the sequence to produce a new and smaller sequence. This is done multiple times, typically with varying parameters for each layer until we are left with a small tensor which we then transform into our required output shape, 1 value between 0 and 1 in the case of classification.

```{r cnn-architecture, echo= FALSE, fig.cap="A template CNN architecture for 1 dimensional input data. A sequence of consequtive CNN layers will incremently reduce the tensor size, ending up with single value."}
knitr::include_graphics("diagram-files/cnn-architecture.png")
```

This figure lies a little bit since we technically don't feed characters into it, but instead uses sequence one-hot encoding with a possible word embedding.
We will now go through some of the most important concepts about CNNs.

### Filters

The kernel is a small tensor of the same dimensionality as the input tensor that slides along the input tensor. When it is sliding it performs element-wise multiplication of the values in the input tensor and its weights and then summing up the values to get a single value. 
Sometimes an activation function will be applied as well.
It is these weights that are trained with gradient descent to find the best fit.
In keres, the `filters` represent how many different kernels are trained in each layer. You typically start with fewer filters at the beginning of your network and then increase them as you go along. 

### Kernel size

The most prominent hyperparameter is the kernel size. 
The kernel size is the size of the tensor, 1 dimensional is this case, that contains the weights. A kernel with size 5 will have 5 weights. These kernels will similarly capture local information to how n-grams capture location patterns. Increasing the size of the kernel will decrease the size of the output tensor, as we see in figure \@ref(fig:cnn-kernel-size)

```{r cnn-kernel-size, echo= FALSE, fig.cap="The kernel size affects the size of the resulting tensor. A kernel size of 3 uses the information from 3 values to calculate 1 value."}
knitr::include_graphics("diagram-files/cnn-kernel-size.png")
```

Larger kernels will detect larger and less frequent patterns where smaller kernels will find fine-grained features. 
Notice how the choice of the token will affect how we think about kernel size. 
For character level tokens a kernel size of 5 will in early layers find patterns in parts of words more often than patterns across words since 5 characters aren't enough the adequately span multiple words. 
Where on the other hand a kernel size of 5 for word-level tokens will find patterns in parts of sentences instead. Kernels must have an odd length.

### Simple CNN

We will be using the same data and recipe which we looked at and created in section \@ref(kickstarter). This data contains short text blurbs for prospective crowdfunding campaigns and if they were successful or not. Our goal of this modeling is to be able to predict successful campaigns by the text contained in the blurb.

```{r include=FALSE}
library(tidyverse)

kickstarter <- read_csv("data/kickstarter.csv.gz")
kickstarter

library(tidymodels)
set.seed(1234)
kickstarter_split <- kickstarter %>%
  filter(nchar(blurb) >= 15) %>%
  initial_split()

kickstarter_train <- training(kickstarter_split)
kickstarter_test <- testing(kickstarter_split)

library(textrecipes)

max_words <- 20000
max_length <- 30

prepped_recipe <- recipe(~blurb, data = kickstarter_train) %>%
  step_tokenize(blurb) %>%
  step_tokenfilter(blurb, max_tokens = max_words) %>%
  step_sequence_onehot(blurb, sequence_length = max_length) %>%
  prep()

prepped_training <- prepped_recipe %>%
  bake(new_data = NULL, composition = "matrix")
```

We will start with a simple CNN specification that follows what we saw in figure \@ref(fig:cnn-architecture).
We start with an embedding layer followed by one 1 dimensional convolution layers `layer_conv_1d()`, followed by a global max pooling layer `layer_global_max_pooling_1d()` and a densely connected layer and ending with a dense layer with a sigmoid activation function to give us 1 value between 0 and 1 to use in our classification.

```{r simple_cnn_model}
library(keras)
simple_cnn_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 16,
                  input_length = max_length) %>%
  layer_conv_1d(filter = 32, kernel_size = 5, activation = "relu") %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

simple_cnn_model
```

We are using the same embedding layer as we did in the previous networks so there is nothing new there.
We use a `layer_global_max_pooling_1d()` layer to transform to collapse the remaining CNN output into 1 dimension and we finish it off with a densely connected layer and a sigmoid activation function.

This might not end up being the best CNN configuration, but it is a good starting point.
One of the challenges when working with CNNS is to make sure that you manage the dimensionality correctly. The length of the sequence decrease by `(kernel_size - 1)` for each layer. For this input where we have a sequence of length 30, which is decreased by `(5 - 1) = 4` resulting in a sequence of 30. We could in theory have 7 layers with `kernel_size = 5` since we would end with `30 - 4 - 4 - 4 - 4 - 4 - 4 - 4 = 2` elements in the resulting sequence. However, we would not be able to do a network with 3 layers of 
`kernel_size = 7` followed by 3 layers of `kernel_size = 5` since the resulting sequence would be `30 - 6 - 6 - 6 - 4 - 4 - 4 = 0` and we need a positive length in our sequence.
Remember that `kernel_size` is not the only argument that will change the length of the resulting sequence. Constructing a sequence layer by layer while having keras print the configurations is a great idea of how you can make sure your architecture is valid.

The compilation and fitting are the same as we have seen before. 

```{r simple_cnn_model_fit, dependson="simple_cnn_model"}
simple_cnn_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- simple_cnn_model %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 10,
  validation_split = 0.2
)
```

We are using the `"adam"` optimizer since it performs well for this model. 

```{block, type = "rmdnote"}
You will have to experiment to find the optimizer that works best for your specific model. Different optimizers work differently in different situations and it is up to you to find which works the best for your model and data.
```

Now that the model is done fitting we can evaluate it on the testing data set.

```{r simple_cnn_model_evaluation, dependson="simple_cnn_model_fit"}
simple_cnn_model %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

We are seeing some improvement over the densely connected network. This is a good development, what we hoped to see was an improvement in our model.

### Case study: Adding more layers

Now that we have the basic structure of a CNN down we can see what happens when we apply some common modifications to it.
This case study will look at how we can add additional convolutional layers to our base model and how additional dense layers can be added.

We will start by adding another fully connected layer. We take the architecture we used in `simple_cnn_model` and added another `layer_dense()` after the first `layer_dense()` in the model. And we are not going to do more than this.
Increasing the depth of the model over the fully connected layers allows the model to be able to find move complex patterns. There is however a trade-off. Adding more layers will add more weights to the model making it more complex and harder to train. If you don't have enough data or the patterns you are trying to classify aren't that complex then the model performance will suffer since the model will start overfitting since it starts picking up on patterns in the data that aren't there.

When working with CNNs the different layers perform different tasks. The convolutional layers extract local patterns in the data as it slides along the sequences, whereas the fully connected layers find global patterns. We can think of the convolutional layers as doing preprocessing on the text which is then fed into the deep neural network that tries to fit the best curve. Adding more fully connected layers allows the network to create more intricate curves, and adding more convolutional layers gives richer features that are used when fitting the curves. Your job when constructing a CNN is to make the architecture just complex enough to match the data without overfitting. Yoshua Bengio has a simple rule for this: "Just keep adding layers until the test error does not improve anymore"[@bengio2012practical].

```{r}
cnn_double_dense <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 16,
                  input_length = max_length) %>%
  layer_conv_1d(filter = 32, kernel_size = 5, activation = "relu") %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

cnn_double_dense
```

we can compile and fit the new model. We will try to keep as many things as we can constant so we can try to compare the different models.

```{r}
cnn_double_dense %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- cnn_double_dense %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 10,
  validation_split = 0.2
)

cnn_double_dense %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

This model performs well, but it is not entirely clear that it is working better than the first CNN model we tried. This could be an indication that the original model had enough fully connected layers for the amount of training data we had available.
If this is the case and we have two models with identical performance then we should go with the less complex of the two, since they would have faster performance.

We can also try to vary the number of convolutional layers by adding more layers.

```{r}
library(keras)
cnn_double_conv <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 16,
                  input_length = max_length) %>%
  layer_conv_1d(filter = 32, kernel_size = 5, activation = "relu") %>%
  layer_max_pooling_1d(pool_size = 2) %>%
  layer_conv_1d(filter = 64, kernel_size = 3, activation = "relu") %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

cnn_double_conv
```

There are a lot of different ways we can extend the network by adding convolutional layers with `layer_conv_1d()`. We have to consider the individual characteristics of each layer, concerning kernel size, stride, padding, and dilation rate. We also have to consider the progression of these layers within the network itself. 
The model is using an increasing number of filters in each layer, doubling the number of filters for each layer. This is to make sure that there are more filters later on to capture enough of the global information.
This model is using kernel size 5 twice and there aren't any hard rules about how you structure these kernel sizes, but the sizes you choose will change what features the model can detect. The early layers extract general or low-level features whereas the later layers define finer detail or high-level features in the data. The choice of kernel size determines the size of these features. Having a small kernel size in the first layer will let the model detect low-level features locally.

We are also including a max-pooling layer with `layer_max_pooling_1d()` between the convolutional layers. This layer performs a pooling operation that calculates the maximum values in its pooling window which in this model is set to 2.
This is done in the hope that the pooled features will be able to perform better by weeding out the small weights.
This is another thing you can tinker with when you are designing the network concerning the frequency of pooling layers and their parameters.

We compile this model like the other two, again trying to keep as many things as we can constant. The only thing that changed in this model compared to the first is the addition of a `layer_max_pooling_1d()` and a `layer_conv_1d()`.

```{r}
cnn_double_conv %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- cnn_double_conv %>% fit(
  x = prepped_training, 
  y = kickstarter_train$state,
  batch_size = 512,
  epochs = 10,
  validation_split = 0.2
)

cnn_double_conv %>%
  evaluate(
    bake(prepped_recipe, kickstarter_test, composition = "matrix"),
    kickstarter_test$state
  )
```

This model also performs admirably compared to the other models. Let us extract the the prediction using `keras_predict()` we defined in \@ref(evaluate-keras-predictions).

```{r}
all_cnn_model_predictions <- bind_rows(
  keras_predict(
    simple_cnn_model, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "Basic CNN"),
  keras_predict(
    cnn_double_dense, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "Double Dense"),
  keras_predict(
    cnn_double_conv, 
    prepped_recipe, 
    kickstarter_test, 
    state
  ) %>% mutate(model = "Double Conv")
)

all_cnn_model_predictions
```

Now that the results are combined in `all_cnn_model_predictions` we can calculate group-wise evaluation statistics by grouping them by the `model` variable.

```{r}
all_cnn_model_predictions %>%
  group_by(model) %>%
  metrics(state, .pred_class)
```

We can also do this for ROC-curves. Figure \@ref(fig:allcnnroccurve) shows the 3 different ROC-curves together in 1 chart.

```{r allcnnroccurve, fig.width=7, fig.height=6, fig.cap="ROC curve for 3 varients of CNN models' predictions of Kickstarter state"}
all_cnn_model_predictions %>%
  group_by(model) %>%
  roc_curve(truth = state, .pred_prop) %>%
  autoplot() +
  labs(
    title = "Receiver operator curve for Kickstarter blurbs"
  )
```

The curves are very close in this chart. This seems to indicate that we don't have much to gain by adding more layers since they don't add anything to the performance.
This doesn't mean that we are done with this model. There are still some things we can look at such as different tokenization and hyperparameters that can be trained. We will take a look at this in the following sections.

## Character-level Convolutional Neural Network

In our models so far we have used "words" as the token of interest. Another choice of token could be "character". Since this data set contains very short texts then we don't have many words to work with. We have filtered the data set to have a minimum text length of 15 and while that helps it doesn't stop the fact that many of the texts will have 1 or 2 words in total.

The idea of using character-level CNNs is nothing new, they have been explored by @Zhang2015 and work quite well on small shorter texts such as headlines and tweets[@Vosoughi2016].
These kinds of models will be able to detect patterns of the characters inside the words, which means that these models can have very favorable performance in languages with rich morphology all while having a low number of trainable parameters.

We need to remind ourselves that these models don't contain any linguistic knowledge at all, they only "know" the patterns of sequences of characters in the training set. This is not to say the models are useless, but to set our expectations of what the model is capable of, namely pattern detection.

Since we are using a completely different preprocessing setup we need to specify a new recipe. This recipe should tokenize to characters and instead of specifying the maximal number of tokens we want, we instead specify the tokens directly. These tokens will be our alphabet.
The paper by @Zhang2015 uses an alphabet consisting of 70 characters, including 26 English letters, 10 digits, 33 other characters, and the new line character.

```{text, eval=FALSE}
abcdefghijklmnopqrstuvwxyz0123456789
-,;.!?:’’’/\|_@#$%ˆ&*˜‘+-=<>()[]{}
```

A model using this alphabet would properly work but we should tailor it more to the data we have at hand. Many of the "other" characters are used for punctuation, something that these blurbs don't contain much of. Neither are numbers used much. Let's go simple and only use the 26 letters and spaces.
Before we move on we can double-check that this is a reasonable choice by using the `unnest_characters()` function from the tidytext package to tokenize to characters and then count the different characters.

```{r}
library(tidytext)
kickstarter_train %>%
  unnest_characters(char, blurb, strip_non_alphanum = FALSE) %>%
  count(char, sort = TRUE)
```

If you dig in this list then you find that the frequencies quickly drop off once you get past the letters. We are turning all characters to lowercase, while the case does matter some in the text, doubling the alphabet size does not seem worth it.

```{r}
charlevel_recipe <- recipe(~blurb, data = kickstarter_train) %>%
  step_tokenize(blurb, token = "characters", 
                options = list(strip_non_alphanum = FALSE)) %>%
  step_sequence_onehot(blurb, 
                       sequence_length = 70, 
                       vocabulary = c(letters, " "))%>%
  prep()

charlevel_training <- charlevel_recipe %>%
  bake(new_data = NULL, composition = "matrix")
```

We can confirm that this is working by looking at the first observation. 
The leading zeroes are happening because we are pre-padding with zeroes

```{r}
unname(charlevel_training[1, ])
```

to understand what the indices mean we can extract the vocabulary of `step_sequence_onehot()` using `tidy()` on the prepped recipe.

```{r}
tidy(charlevel_recipe, 2)
```

And we are lucky in this case since the vocabulary is in alphabetical order.

## Explainability with lime

## Hyper Parameter search

## Cross validation for evaluation

## Limitations of Covolution Neural Networks

## Summary {#dlcnnsummary}

### In this chapter, you learned:

```{r, include=FALSE}
knitr::knit_exit()
```

### Stride

The stride is the second big hyperparameter that controls the kernels in a CNN. The stride length determines how much the kernel moves along the sequence between each calculation. A stride length of 1 means that the kernel moves over one place at a time, this way we get maximal overlap.

```{r cnn-stride, echo= FALSE, fig.cap="The stride length affects the size of the resulting tensor. When stride = 1 then the window slides along one by one. Increasing the slide length decreases the resulting tensor by skipping windows."}
knitr::include_graphics("diagram-files/cnn-stride.png")
```

In figure \@ref(fig:cnn-stride) we see that if the kernel size and stride length are equal then there is no overlap. We can decrease the size of the output tensor by increasing the stride length. Be careful not to set the stride length to be larger than the kernel size, otherwise, then you will skip over some of the information.

### Dilation

The dilation controls how the kernel is applied to the input tensor.
So far we have shown examples where the dilation is equal to 1. This means that each value from the input tensor will be spaced 1 distance apart from each other.

```{r cnn-dilation, echo= FALSE, fig.cap="The dilation affects the size of the resulting tensor. When dilation = 1 then consecutive values are taking from the input. Increasing the dilation leaves gaps between input values and decreases the resulting tensor."}
knitr::include_graphics("diagram-files/cnn-dilation.png")
```

If we increase the dilation then can see in figure \@ref(fig:cnn-dilation) that there will be spaces or gaps between the input values. This allows the kernel to find large spatial patterns that span many tokens.
This is a useful trick to be able to extract features and structure from long sequences. Dilated convolutional layers when put in succession will be able to find patterns in very large sequences.

### Padding

The last hyperparameter we will talk about is padding.
One of the downsides to how the kernels are being used in the previous figures is how it handles the edge of the sequence.
Padding is the act of putting something before and after the sequence when the convolution is taking place to be able to extract more information from the first and last tokens in the sequence. Padding will lead to larger output tensors since they we let the kernel move more.
