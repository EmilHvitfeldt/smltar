% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{krantz}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmonofont[Scale=0.7]{Source Code Pro}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Supervised Machine Learning for Text Analysis in R},
  pdfauthor={Emil Hvitfeldt and Julia Silge},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage{enumitem}
\usepackage{makeidx}
\makeindex

\setlist[1]{itemsep=10pt}

\usepackage{fontspec} % use fontspec package
\usepackage{xeCJK}    % use xeCJK package

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

% create callout boxes:
\newenvironment{rmdblock}[1]
  {\begin{shaded*}
   %\addtolength{\leftmargini}{-1cm}
  \begin{itemize}[left = -1cm, labelsep = 1cm]
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
 
  \item
  }
  {
  \end{itemize}
  \end{shaded*}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarn}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{rmdpackage}
  {\begin{rmdblock}{package}}
  {\end{rmdblock}}
  
\usepackage{footnote}
\makesavenoteenv{rmdblock}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter


\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Supervised Machine Learning for Text Analysis in R}
\author{Emil Hvitfeldt and Julia Silge}
\date{2021-05-05}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
In loving memory of my mother-in-law Lisa, 
who was the first soul to hear about and fully encourage the idea that eventually became this book
---E.H.
\\[2in]

For Grace, Violet, and Lewis, 
who (thanks to the pandemic and remote school) had a front row seat to most of my work on this book
---J.S.

%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\mainmatter

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


Modeling as a statistical practice can encompass a wide variety of activities. This book focuses on \emph{supervised or predictive modeling for text}, using text data to make predictions about the world around us. We use the \href{https://www.tidymodels.org/}{tidymodels} framework for modeling, a consistent and flexible collection of R packages developed to encourage good statistical practice.

Supervised machine learning using text data involves building a statistical model to estimate some output from input that includes language. The two types of models we train in this book are regression and classification. Think of \index{regression}regression models as predicting numeric or continuous outputs, such as predicting the year of a United States Supreme Court opinion from the text of that opinion. Think of \index{classification}classification models as predicting outputs that are discrete quantities or class labels, such as predicting whether a GitHub issue is about documentation or not from the text of the issue. Models like these can be used to make predictions for new observations, to understand what features or characteristics contribute to differences in the output, and more. We can evaluate our models using performance metrics to determine which are best, which are acceptable for our specific context, and even which are fair.

\begin{rmdnote}
Text data is important for many domains, from healthcare to marketing to
the digital humanities, but specialized approaches are necessary to
create features (predictors) for machine learning from language.
\end{rmdnote}

Natural language that we as speakers and/or writers use must be dramatically transformed to a machine-readable, numeric representation to be ready for computation. In this book, we explore typical text preprocessing steps from the ground up, and consider the effects of these steps. We also show how to fluently use the \textbf{textrecipes} R package \citep{textrecipes} to prepare text data within a modeling pipeline.

\citet{Silge2017} provides a practical introduction to text mining with R using tidy data principles, based on the \textbf{tidytext} package. If you have already started on the path of gaining insight from your text data, a next step is using that text directly in predictive modeling. Text data contains within it latent information that can be used for insight, understanding, and better decision-making, and predictive modeling with text can bring that information and insight to light. If you have already explored how to analyze text as demonstrated in \citet{Silge2017}, this book will move one step further to show you how to \emph{learn and make predictions} from that text data with supervised models. If you are unfamiliar with this previous work, this book will still provide a robust introduction to how text can be represented in useful ways for modeling and a diverse set of supervised modeling approaches for text.

\hypertarget{outline}{%
\section*{Outline}\label{outline}}


The book is divided into three sections. We make a (perhaps arbitrary) distinction between \emph{machine learning methods} and \emph{deep learning methods} by defining deep learning as any kind of multi-layer neural network (LSTM, bi-LSTM, CNN) and machine learning as anything else (regularized regression, naive Bayes, SVM, random forest). We make this distinction both because these different methods use separate software packages and modeling infrastructure, and from a pragmatic point of view, it is helpful to split up the chapters this way.

\begin{itemize}
\item
  \textbf{Natural language features:} How do we transform text data into a representation useful for modeling? In these chapters, we explore the most common preprocessing steps for text, when they are helpful, and when they are not.
\item
  \textbf{Machine learning methods:} We investigate the power of some of the simpler and more lightweight models in our toolbox.
\item
  \textbf{Deep learning methods:} Given more time and resources, we see what is possible once we turn to neural networks.
\end{itemize}

Some of the topics in the second and third sections overlap as they provide different approaches to the same tasks.

Throughout the book, we will demonstrate with examples and build models using a selection of text data sets. A description of these data sets can be found in Appendix \ref{appendixdata}.

\begin{rmdnote}
We use three kinds of info boxes throughout the book to invite attention
to notes and other ideas.
\end{rmdnote}

\begin{rmdwarning}
Some boxes call out warnings or possible problems to watch out for.
\end{rmdwarning}

\begin{rmdpackage}
Boxes marked with hexagons highlight information about specific R
packages and how they are used. We use \textbf{bold} for the names of R
packages.
\end{rmdpackage}

\hypertarget{topics-this-book-will-not-cover}{%
\section*{Topics this book will not cover}\label{topics-this-book-will-not-cover}}


This book serves as a thorough introduction to prediction and modeling with text, along with detailed practical examples, but there are many areas of natural language processing we do not cover. \index{CRAN}The \href{https://cran.r-project.org/web/views/NaturalLanguageProcessing.html}{CRAN Task View on Natural Language Processing} provides details on other ways to use R for computational linguistics. Specific topics we do not cover include:

\begin{itemize}
\item
  \textbf{Reading text data into memory:} Text data may come to a data practitioner in any of a long list of heterogeneous formats. Text data exists in PDFs, databases, plain text files (single or multiple for a given project), websites, APIs, literal paper, and more. The skills needed to access and sometimes wrangle text data sets so that they are in memory and ready for analysis are so varied and extensive that we cannot hope to cover them in this book. We point readers to R packages such as \textbf{readr} \citep{R-readr}, \textbf{pdftools} \citep{R-pdftools}, and \textbf{httr} \citep{R-httr}, which we have found helpful in these tasks.
\item
  \textbf{Unsupervised machine learning for text:} \citet{Silge2017} provide an introduction to one method of unsupervised text modeling\index{machine learning!unsupervised}, and Chapter \ref{embeddings} does dive deep into word embeddings, which learn from the latent structure in text data. However, many more unsupervised machine learning algorithms can be used for the goal of learning about the structure or distribution of text data when there are no outcome or output variables to predict.
\item
  \textbf{Text generation:} The deep learning model architectures we discuss in Chapters \ref{dldnn}, \ref{dllstm}, and \ref{dlcnn} can be used to generate new text\index{text generation}, as well as to model existing text. \citet{Chollet2018} provide details on how to use neural network architectures and training data for text generation.
\item
  \textbf{Speech processing:} Models that detect words in audio recordings of speech\index{speech} are typically based on many of the principles outlined in this book, but the training data is \emph{audio} rather than written text. R users can access pre-trained speech-to-text models via large cloud providers, such as Google Cloud's Speech-to-Text API accessible in R through the \textbf{googleLanguageR} package \citep{R-googleLanguageR}.
\item
  \textbf{Machine translation:} Machine translation\index{translation} of text between languages, based on either older statistical methods or newer neural network methods, is a complex, involved topic. Today, the most successful and well-known implementations of machine translation are proprietary, because large tech companies have access to both the right expertise and enough data in multiple languages to train successful models for general machine translation. Google is one such example, and Google Cloud's Translation API is again available in R through the \textbf{googleLanguageR} package.
\end{itemize}

\hypertarget{who-is-this-book-for}{%
\section*{Who is this book for?}\label{who-is-this-book-for}}


This book is designed to provide practical guidance and directly applicable knowledge for data scientists and analysts who want to integrate text into their modeling pipelines.

We assume that the reader is somewhat familiar with R, predictive modeling concepts for non-text data, and the \href{https://www.tidyverse.org/}{\textbf{tidyverse}} family of packages \citep{Wickham2019}. For users who don't have this background with tidyverse code, we recommend \href{http://r4ds.had.co.nz/}{\emph{R for Data Science}} \citep{Wickham2017}. Helpful resources for getting started with modeling and machine learning include a \href{https://supervised-ml-course.netlify.com/}{free interactive course} developed by one of the authors (JS) and \href{https://bradleyboehmke.github.io/HOML/}{\emph{Hands-On Machine Learning with R}} \citep{Boehmke2019}, as well as \href{http://faculty.marshall.usc.edu/gareth-james/ISL/}{\emph{An Introduction to Statistical Learning}} \citep{James2013}.

We don't assume an extensive background in text analysis, but \href{https://www.tidytextmining.com/}{\emph{Text Mining with R}} \citep{Silge2017}, by one of the authors (JS) and David Robinson, provides helpful skills in exploratory data analysis for text that will promote successful text modeling. This book is more advanced than \emph{Text Mining with R} and will help practitioners use their text data in ways not covered in that book.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}


We are so thankful for the contributions, help, and perspectives of people who have supported us in this project. There are several we would like to thank in particular.

We would like to thank Max Kuhn and Davis Vaughan for their investment in the \textbf{tidymodels} packages, David Robinson for his collaboration on the \textbf{tidytext} package, and Yihui Xie for his work on \textbf{knitr}, \textbf{bookdown}, and the R Markdown ecosystem. Thank you to Desirée De Leon for the site design of the online work and to Sarah Lin for the expert creation of the published work's index. We would also like to thank Carol Haney, Kasia Kulma, David Mimno, Kanishka Misra, and an additional anonymous technical reviewer for their detailed, insightful feedback that substantively improved this book, as well as our editor John Kimmel for his perspective and guidance during the process of writing and publishing.

This book was written in the open, and multiple people contributed via pull requests or issues. Special thanks goes to the 4 people who contributed via GitHub pull requests (in alphabetical order by username): @fellennert, Riva Quiroga (@rivaquiroga), Darrin Speegle (@speegled), Tanner Stauss (@tmstauss).

Note box icons by Smashicons from flaticon.com

\hypertarget{colophon}{%
\section*{Colophon}\label{colophon}}


This book was written in \href{http://www.rstudio.com/ide/}{RStudio} using \href{http://bookdown.org/}{\textbf{bookdown}}. The \href{https://smltar.com/}{website} is hosted via \href{https://pages.github.com/}{GitHub Pages}, and the complete source is available on \href{https://github.com/EmilHvitfeldt/smltar}{GitHub}. We generated all plots in this book using \href{https://ggplot2.tidyverse.org/}{\textbf{ggplot2}} and its light theme (\texttt{theme\_light()}). The \texttt{autoplot()} method for \href{https://yardstick.tidymodels.org/reference/conf_mat.html}{\texttt{conf\_mat()}} have been modified slightly to allow colors; modified code can be found \href{https://github.com/EmilHvitfeldt/smltar/blob/master/_common.R}{online}.

This version of the book was built with R version 4.0.2 (2020-06-22) and the following packages:

\begin{longtable}[]{@{}lll@{}}
\toprule
package & version & source\tabularnewline
\midrule
\endhead
bench & 1.1.1 & CRAN (R 4.0.0)\tabularnewline
bookdown & 0.22 & CRAN (R 4.0.2)\tabularnewline
broom & 0.7.6 & CRAN (R 4.0.2)\tabularnewline
corpus & 0.10.2 & CRAN (R 4.0.2)\tabularnewline
dials & 0.0.9 & CRAN (R 4.0.2)\tabularnewline
discrim & 0.1.1 & CRAN (R 4.0.2)\tabularnewline
doParallel & 1.0.16 & CRAN (R 4.0.2)\tabularnewline
glmnet & 4.1-1 & CRAN (R 4.0.2)\tabularnewline
gt & 0.2.2 & CRAN (R 4.0.2)\tabularnewline
hcandersenr & 0.2.0 & CRAN (R 4.0.0)\tabularnewline
htmltools & 0.5.1.1 & CRAN (R 4.0.2)\tabularnewline
htmlwidgets & 1.5.3 & CRAN (R 4.0.2)\tabularnewline
hunspell & 3.0.1 & CRAN (R 4.0.2)\tabularnewline
irlba & 2.3.3 & CRAN (R 4.0.0)\tabularnewline
jiebaR & 0.11 & CRAN (R 4.0.2)\tabularnewline
jsonlite & 1.7.2 & CRAN (R 4.0.2)\tabularnewline
kableExtra & 1.3.4 & CRAN (R 4.0.2)\tabularnewline
keras & 2.4.0 & CRAN (R 4.0.2)\tabularnewline
klaR & 0.6-15 & CRAN (R 4.0.0)\tabularnewline
LiblineaR & 2.10-12 & CRAN (R 4.0.2)\tabularnewline
lime & 0.5.2 & CRAN (R 4.0.2)\tabularnewline
lobstr & 1.1.1 & CRAN (R 4.0.0)\tabularnewline
naivebayes & 0.9.7 & CRAN (R 4.0.0)\tabularnewline
parsnip & 0.1.5 & CRAN (R 4.0.2)\tabularnewline
prismatic & 1.0.0 & CRAN (R 4.0.2)\tabularnewline
quanteda & 3.0.0 & CRAN (R 4.0.2)\tabularnewline
ranger & 0.12.1 & CRAN (R 4.0.0)\tabularnewline
recipes & 0.1.16 & CRAN (R 4.0.2)\tabularnewline
remotes & 2.3.0 & CRAN (R 4.0.2)\tabularnewline
reticulate & 1.20 & CRAN (R 4.0.2)\tabularnewline
rsample & 0.0.9 & CRAN (R 4.0.2)\tabularnewline
rsparse & 0.4.0 & CRAN (R 4.0.0)\tabularnewline
scico & 1.2.0 & CRAN (R 4.0.2)\tabularnewline
scotus & 1.0.0 & Github (EmilHvitfeldt/scotus@e5ccdea)\tabularnewline
servr & 0.22 & CRAN (R 4.0.2)\tabularnewline
sessioninfo & 1.1.1 & CRAN (R 4.0.0)\tabularnewline
slider & 0.2.1 & CRAN (R 4.0.2)\tabularnewline
SnowballC & 0.7.0 & CRAN (R 4.0.0)\tabularnewline
spacyr & 1.2.1 & CRAN (R 4.0.2)\tabularnewline
stopwords & 2.2 & CRAN (R 4.0.2)\tabularnewline
styler & 1.4.1 & CRAN (R 4.0.2)\tabularnewline
text2vec & 0.6 & CRAN (R 4.0.0)\tabularnewline
textdata & 0.4.1 & CRAN (R 4.0.2)\tabularnewline
textfeatures & 0.3.3 & CRAN (R 4.0.0)\tabularnewline
textrecipes & 0.4.0 & CRAN (R 4.0.2)\tabularnewline
tfruns & 1.5.0 & CRAN (R 4.0.2)\tabularnewline
themis & 0.1.3 & CRAN (R 4.0.2)\tabularnewline
tidymodels & 0.1.3 & CRAN (R 4.0.2)\tabularnewline
tidytext & 0.3.1 & CRAN (R 4.0.2)\tabularnewline
tidyverse & 1.3.1 & CRAN (R 4.0.2)\tabularnewline
tokenizers & 0.2.1 & CRAN (R 4.0.0)\tabularnewline
tokenizers.bpe & 0.1.0 & CRAN (R 4.0.2)\tabularnewline
tufte & 0.9 & CRAN (R 4.0.2)\tabularnewline
tune & 0.1.5 & CRAN (R 4.0.2)\tabularnewline
UpSetR & 1.4.0 & CRAN (R 4.0.2)\tabularnewline
vip & 0.3.2 & CRAN (R 4.0.2)\tabularnewline
widyr & 0.1.3 & CRAN (R 4.0.2)\tabularnewline
workflows & 0.2.2 & CRAN (R 4.0.2)\tabularnewline
yardstick & 0.0.8 & CRAN (R 4.0.2)\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{part-natural-language-features}{%
\part{Natural Language Features}\label{part-natural-language-features}}

\hypertarget{language}{%
\chapter{Language and modeling}\label{language}}

Machine learning and deep learning models for text are executed by computers, but they are designed and created by human beings using language generated by human beings. As natural language processing (NLP) practitioners, we bring our assumptions about what language is and how language works into the task of creating modeling features from natural language and using those features as inputs to statistical models. This is true \emph{even when} we don't think about how language works very deeply or when our understanding is unsophisticated or inaccurate; speaking a language is not the same as having an explicit knowledge of how that language works. We can improve our machine learning models for text by heightening that knowledge.

Throughout the course of this book, we will discuss creating predictors or features from text data, fitting statistical models to those features, and how these tasks are related to language. Data scientists involved in the everyday work of text analysis and text modeling typically don't have formal training in how language works, but there is an entire field focused on exactly that, \emph{linguistics}.\index{linguistics}

\hypertarget{linguistics-for-text-analysis}{%
\section{Linguistics for text analysis}\label{linguistics-for-text-analysis}}

\citet{Briscoe13} provides helpful introductions to what linguistics is and how it intersects with the practical computational field of natural language processing. The broad field of linguistics includes subfields focusing on different aspects of language, which are somewhat hierarchical, as shown in Table \ref{tab:lingsubfields}.

\index{phonetics}
\index{phonology}
\index{morphology}
\index{syntax}
\index{semantics}
\index{pragmatics}

\begin{table}

\caption{\label{tab:lingsubfields}Some subfields of linguistics, moving from smaller structures to broader structures}
\centering
\begin{tabular}[t]{ll}
\toprule
Linguistics subfield & What does it focus on?\\
\midrule
Phonetics & Sounds that people use in language\\
Phonology & Systems of sounds in particular languages\\
Morphology & How words are formed\\
Syntax & How sentences are formed from words\\
Semantics & What sentences mean\\
\addlinespace
Pragmatics & How language is used in context\\
\bottomrule
\end{tabular}
\end{table}

These fields each study a different level at which language exhibits organization. When we build supervised machine learning models for text data, we use these levels of organization to create \emph{natural language features}, i.e., predictors or inputs for our models. These features often depend on the morphological characteristics of language, such as when text is broken into sequences of characters for a recurrent neural network deep learning model\index{neural network!recurrent}. Sometimes these features depend on the syntactic characteristics of language, such as when models use part-of-speech information. These roughly hierarchical levels of organization are key to the process of transforming unstructured language to a mathematical representation that can be used in modeling.

At the same time, \index{linguistics}this organization and the rules of language can be ambiguous; our ability to create text features for machine learning is constrained by the very nature of language. Beatrice Santorini, a linguist at the University of Pennsylvania, compiles examples of linguistic ambiguity from \href{https://www.ling.upenn.edu/~beatrice/humor/headlines.html}{news headlines}:

\begin{itemize}
\item
  Include Your Children When Baking Cookies
\item
  March Planned For Next August
\item
  Enraged Cow Injures Farmer with Ax
\item
  Wives Kill Most Spouses In Chicago
\end{itemize}

If you don't have knowledge about what linguists study and what they know about language, these news headlines are just hilarious. To linguists, these are hilarious because they exhibit certain kinds of semantic ambiguity.

Notice also that the first two subfields on this list are about sounds, i.e., speech.\index{speech} Most linguists view speech as primary, and writing down language as text as a technological step.

\begin{rmdnote}
Remember that some language is signed, not spoken, so the description
laid out here is itself limited.
\end{rmdnote}

\index{language!signed}

Written text is typically less creative and further from the primary language than we would wish. This points out how fundamentally limited modeling from written text is. Imagine that the abstract language data we want exists in some high-dimensional latent space; we would like to extract that information using the text somehow, but it just isn't completely possible. Any features we create or model we build are inherently limited.

\hypertarget{morphology}{%
\section{A glimpse into one area: morphology}\label{morphology}}

\index{morphology}How can a deeper knowledge of how language works inform text modeling? Let's focus on \textbf{morphology}, the study of words' internal structures and how they are formed, to illustrate this. Words are medium to small in length in English; English has a moderately low ratio of morphemes (the smallest unit of language with meaning) to words while other languages like Turkish and Russian have a higher ratio of morphemes to words \citep{Bender13}. Related to this, languages can be either more analytic (like Mandarin or modern English, breaking up concepts into separate words) or synthetic (like Hungarian or Swahili, combining concepts into one word).

Morphology focuses on how morphemes such as prefixes, suffixes, and root words come together to form words. Some languages, like Danish, use many compound words. Danish words such as ``brandbil'' (fire truck), ``politibil'' (police car), and ``lastbil'' (truck) all contain the morpheme ``bil'' (car) and start with prefixes denoting the type of car. Because of these compound words, some nouns seem more descriptive than their English counterpart; ``vaskebjørn'' (raccoon) splits into the morphemes ``vaske'' and ``bjørn'', literally meaning ``washing bear''\footnote{The English word ``raccoon'' derives from an Algonquin word meaning, ``scratches with his hands!''}. When working with Danish and other languages with compound words, such as German, compound splitting to extract more information can be beneficial \citep{Sugisaki2018}. However, even the very question of what a word is turns out to be difficult, and not only for languages other than English. Compound words in English like ``real estate'' and ``dining room'' represent one concept but contain whitespace.

The morphological characteristics of a text data set are deeply connected to preprocessing steps like tokenization (Chapter \ref{tokenization}), removing stop words (Chapter \ref{stopwords}), and even stemming (Chapter \ref{stemming}). These preprocessing steps for creating natural language features, in turn, can have significant effects on model predictions or interpretation.

\hypertarget{different-languages}{%
\section{Different languages}\label{different-languages}}

We believe that most of the readers of this book are probably native English speakers, and certainly most of the text used in training machine learning models is English. However, English is by no means a dominant language globally, especially as a native or first language.\index{language!Non-English} As an example close to home for us, of the two authors of this book, one is a native English speaker and one is not. According to the \href{https://www.ethnologue.com/language/eng}{comprehensive and detailed Ethnologue project}, less than 20\% of the world's population speaks English at all.

\citet{Bender11} provides guidance to computational linguists building models for text, for any language. One specific point she makes is to name the language being studied.\index{language!naming}

\begin{quote}
\textbf{Do} state the name of the language that is being studied, even if it's English. Acknowledging that we are working on a particular language foregrounds the possibility that the techniques may in fact be language-specific. Conversely, neglecting to state that the particular data used were in, say, English, gives {[}a{]} false veneer of language-independence to the work.
\end{quote}

This idea is simple (acknowledge that the models we build are typically language-specific) but the \href{https://twitter.com/search?q=\%23BenderRule}{\#BenderRule} has led to increased awareness of the limitations of the current state of this field. Our book is not geared toward academic NLP researchers developing new methods, but toward data scientists and analysts working with everyday data sets; this issue is relevant even for us. Name the languages used in \index{models!training}training models \citep{bender2019rule}, and think through what that means for their generalizability. We will practice what we preach and tell you that most of the text used for modeling in this book is English, with some text in Danish and a few other languages.

\hypertarget{other-ways-text-can-vary}{%
\section{Other ways text can vary}\label{other-ways-text-can-vary}}

The concept of differences in language is relevant for modeling beyond only the broadest language level (for example, English vs.~Danish vs.~German vs.~Farsi). Language from a specific dialect often cannot be handled well with a model trained on data from the same language but not inclusive of that dialect.\index{models!sensitivity} One dialect used in the United States is \index{dialects!African American Vernacular English}African American Vernacular English (AAVE). Models trained to detect toxic or hate speech are more likely to falsely identify AAVE as hate speech \citep{Sap19}; this is deeply troubling not only because the model is less accurate than it should be, but because it amplifies harm against an already marginalized group.

Language is also changing over time. This is a known characteristic of language; if you notice the evolution of your own language, don't be depressed or angry, because it means that people are using it! Teenage girls are especially effective at language innovation, and have been for centuries \citep{McCulloch15}; innovations spread from groups such as young women to other parts of society. This is another difference that impacts modeling.

\begin{rmdwarning}
Differences in language relevant for models also include the use of
slang, and even the context or medium of that text.
\end{rmdwarning}

Consider two bodies of text, both mostly standard written English, but one made up of tweets and one made up of medical documents. If an NLP practitioner trains a model on the data set of tweets to predict some characteristics of the text, it is very possible (in fact, likely, in our experience) that the model will perform poorly if applied to the data set of medical documents\footnote{Practitioners have built specialized computational resources for medical text \citep{Johnson1999}.}. Like machine learning in general, text modeling is exquisitely sensitive to the data used for training. This is why we are somewhat skeptical of AI products such as sentiment analysis APIs, not because they \emph{never} work well, but because they work well only when the text you need to predict from is a good match to the text such a product was trained on.

\hypertarget{languagesummary}{%
\section{Summary}\label{languagesummary}}

Linguistics is the study of how language works, and while we don't believe real-world NLP practitioners must be experts in linguistics, learning from such domain experts can improve both the accuracy of our models and our understanding of why they do (or don't!) perform well. Predictive models for text reflect the characteristics of their training data, so differences in language over time, between dialects, and in various cultural contexts can prevent a model trained on one data set from being appropriate for application in another. A large amount of the text modeling literature focuses on English, but English is not a dominant language around the world.

\hypertarget{in-this-chapter-you-learned}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned}}

\begin{itemize}
\item
  that areas of linguistics focus on topics from sounds to how language is used
\item
  how a topic like morphology is connected to text modeling steps
\item
  to identify the language you are modeling, even if it is English
\item
  about many ways language can vary and how this can impact model results
\end{itemize}

\hypertarget{tokenization}{%
\chapter{Tokenization}\label{tokenization}}

To build features for supervised machine learning from natural language, we need some way of representing raw text as numbers so we can perform computation on them. Typically, one of the first steps in this transformation from natural language to feature, or any of kind of text analysis, is \emph{tokenization}. Knowing what tokenization and tokens are, along with the related concept of an n-gram, is important for almost any natural language processing task.

\hypertarget{what-is-a-token}{%
\section{What is a token?}\label{what-is-a-token}}

\index{data type!character}In R, text is typically represented with the \emph{character} data type, similar to strings in other languages. Let's explore text from fairy tales written by Hans Christian Andersen, available in the \textbf{hcandersenr} package \citep{R-hcandersenr}. This package stores text as lines such as those you would read in a book; this is just one way that you may find text data in the wild and does allow us to more easily read the text when doing analysis.
If we look at the first paragraph of one story titled ``The Fir Tree'', we find the text of the story is in a character vector: a series of letters, spaces, and punctuation stored as a vector.\index{vector!character}

\begin{rmdpackage}
The \textbf{tidyverse} is a collection of packages for data
manipulation, exploration, and visualization.
\end{rmdpackage}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tokenizers)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidytext)}
\FunctionTok{library}\NormalTok{(hcandersenr)}

\NormalTok{the\_fir\_tree }\OtherTok{\textless{}{-}}\NormalTok{ hcandersen\_en }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(book }\SpecialCharTok{==} \StringTok{"The fir tree"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(text)}

\FunctionTok{head}\NormalTok{(the\_fir\_tree, }\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Far down in the forest, where the warm sun and the fresh air made a
sweet"
#> [2] "resting-place, grew a pretty little fir-tree; and yet it was not happy,
it"
#> [3] "wished so much to be tall like its companions– the pines and firs which
grew"
#> [4] "around it. The sun shone, and the soft air fluttered its leaves, and
the"
#> [5] "little peasant children passed by, prattling merrily, but the fir-tree
heeded"
#> [6] "them not. Sometimes the children would bring a large basket of
raspberries or"
#> [7] "strawberries, wreathed on a straw, and seat themselves near the
fir-tree, and"
#> [8] "say, \"Is it not a pretty little tree?\" which made it feel more
unhappy than"
#> [9] "before."
\end{verbatim}

The first nine lines stores the first paragraph of the story, each line consisting of a series of character symbols.
These elements don't contain any metadata or information to tell us which characters are words and which aren't. Identifying these kinds of boundaries between words is where the process of tokenization comes in.

In tokenization, we take an input (a string) and a token type (a meaningful unit of text, such as a word) and split the input into pieces (tokens) that correspond to the type \citep{Manning:2008:IIR:1394399}. Figure \ref{fig:tokenizationdiag} outlines this process.\index{tokenization!definition}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{diagram-files/tokenization-black-box} 

}

\caption{A black box representation of a tokenizer. The text of these three example text fragments has been converted to lowercase and punctuation has been removed before the text is split.}\label{fig:tokenizationdiag}
\end{figure}

Most commonly, the meaningful unit or type of token that we want to split text into units of is a \textbf{word}. However, it is difficult to clearly define what a word is, for many or even most languages. Many languages, such as Chinese, do not use white space between words at all. Even languages that do use white space, including English, often have particular examples that are ambiguous \citep{Bender13}. Romance languages like Italian and French use pronouns and negation words that may better be considered prefixes with a space, and English contractions like ``didn't'' may more accurately be considered two words with no space.\index{language!Non-English}\index{preprocessing!challenges}

To understand the process of tokenization, let's start with a overly simple definition for a word: any selection of alphanumeric (letters and numbers) symbols. \index{regex}Let's use some regular expressions \index{regular expressions|see {regex}}(or regex for short, see Appendix \ref{regexp}) with \texttt{strsplit()} to split the first two lines of ``The Fir Tree'' by any characters that are not alphanumeric.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{strsplit}\NormalTok{(the\_fir\_tree[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\StringTok{"[\^{}a{-}zA{-}Z0{-}9]+"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>  [1] "Far"    "down"   "in"     "the"    "forest" "where"  "the"    "warm"  
#>  [9] "sun"    "and"    "the"    "fresh"  "air"    "made"   "a"      "sweet" 
#> 
#> [[2]]
#>  [1] "resting" "place"   "grew"    "a"       "pretty"  "little"  "fir"    
#>  [8] "tree"    "and"     "yet"     "it"      "was"     "not"     "happy"  
#> [15] "it"
\end{verbatim}

At first sight, this result looks pretty decent. However, we have lost all punctuation, which may or may not be helpful for our modeling goal, and the hero of this story (\texttt{"fir-tree"}) was split in half. Already it is clear that tokenization is going to be quite complicated. Luckily for us, a lot of work has been invested in this process, and typically it is best to use these existing tools. For example, \textbf{tokenizers} \citep{Mullen18} and \textbf{spaCy} \citep{spacy2} implement fast, consistent tokenizers we can use. Let's demonstrate with the \textbf{tokenizers} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tokenizers)}
\FunctionTok{tokenize\_words}\NormalTok{(the\_fir\_tree[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>  [1] "far"    "down"   "in"     "the"    "forest" "where"  "the"    "warm"  
#>  [9] "sun"    "and"    "the"    "fresh"  "air"    "made"   "a"      "sweet" 
#> 
#> [[2]]
#>  [1] "resting" "place"   "grew"    "a"       "pretty"  "little"  "fir"    
#>  [8] "tree"    "and"     "yet"     "it"      "was"     "not"     "happy"  
#> [15] "it"
\end{verbatim}

We see sensible single-word results here; the \texttt{tokenize\_words()} function uses the \textbf{stringi} package \citep{Gagolewski19} and C++ under the hood, making it very fast. Word-level tokenization is done by finding word boundaries according to the specification from the International Components for Unicode (ICU).\index{Unicode} How does this \href{https://www.unicode.org/reports/tr29/tr29-35.html\#Default_Word_Boundaries}{word boundary algorithm} work? It can be outlined as follows:

\begin{itemize}
\item
  Break at the start and end of text, unless the text is empty.
\item
  Do not break within CRLF (new line characters).
\item
  Otherwise, break before and after new lines (including CR and LF).
\item
  Do not break within emoji zwj sequences.
\item
  Keep horizontal whitespace together.
\item
  Ignore Format and Extend characters, except after sot, CR, LF, and new line.
\item
  Do not break between most letters.
\item
  Do not break letters across certain punctuation.
\item
  Do not break within sequences of digits, or digits adjacent to letters (``3a'', or ``A3'').
\item
  Do not break within sequences, such as ``3.2'' or ``3,456.789''.
\item
  Do not break between Katakana.
\item
  Do not break from extenders.
\item
  Do not break within emoji flag sequences.
\item
  Otherwise, break everywhere (including around ideographs).
\end{itemize}

While we might not understand what each and every step in this algorithm is doing, we can appreciate that it is many times more sophisticated than our initial approach of splitting on non-alphanumeric characters. In most of this book, we will use the \textbf{tokenizers} package as a baseline tokenizer for reference. Your choice of tokenizer will influence your results, so don't be afraid to experiment with different tokenizers or, if necessary, to write your own to fit your problem.

\hypertarget{types-of-tokens}{%
\section{Types of tokens}\label{types-of-tokens}}

Thinking of a token as a word is a useful way to start understanding tokenization, even if it is hard to implement concretely in software. We can generalize the idea of a token beyond only a single word to other units of text. We can tokenize text at a variety of units including:

\begin{itemize}
\item
  characters,
\item
  words,
\item
  sentences,
\item
  lines,
\item
  paragraphs, and
\item
  n-grams.
\end{itemize}

In the following sections, we will explore how to tokenize text using the \textbf{tokenizers} package. These functions take a character vector as the input and return lists of character vectors as output. This same tokenization can also be done using the \textbf{tidytext} \citep{Silge16} package, for workflows using tidy data principles where the input and output are both in a dataframe.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_vector }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Far down in the forest"}\NormalTok{,}
                   \StringTok{"grew a pretty little fir{-}tree"}\NormalTok{)}
\NormalTok{sample\_tibble }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{text =}\NormalTok{ sample\_vector)}
\end{Highlighting}
\end{Shaded}

\begin{rmdpackage}
The \textbf{tokenizers} package offers fast, consistent tokenization in
R for tokens such as words, letters, n-grams, lines, paragraphs, and
more.
\end{rmdpackage}

The tokenization achieved by using \texttt{tokenize\_words()} on \texttt{sample\_vector}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tokenize\_words}\NormalTok{(sample\_vector)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "far"    "down"   "in"     "the"    "forest"
#> 
#> [[2]]
#> [1] "grew"   "a"      "pretty" "little" "fir"    "tree"
\end{verbatim}

will yield the same results as using \texttt{unnest\_tokens()} on \texttt{sample\_tibble}; the only difference is the data structure, and thus how we might use the result moving forward in our analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_tibble }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text, }\AttributeTok{token =} \StringTok{"words"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 11 x 1
#>    word  
#>    <chr> 
#>  1 far   
#>  2 down  
#>  3 in    
#>  4 the   
#>  5 forest
#>  6 grew  
#>  7 a     
#>  8 pretty
#>  9 little
#> 10 fir   
#> 11 tree
\end{verbatim}

\begin{rmdpackage}
The \textbf{tidytext} package provides functions to transform text to
and from tidy formats, allowing us to work seamlessly with other
\textbf{tidyverse} tools.
\end{rmdpackage}

Arguments used in \texttt{tokenize\_words()} can be passed through \texttt{unnest\_tokens()} using the \href{https://adv-r.hadley.nz/functions.html\#fun-dot-dot-dot}{``the dots''}, \texttt{...}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_tibble }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text, }\AttributeTok{token =} \StringTok{"words"}\NormalTok{, }\AttributeTok{strip\_punct =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 12 x 1
#>    word  
#>    <chr> 
#>  1 far   
#>  2 down  
#>  3 in    
#>  4 the   
#>  5 forest
#>  6 grew  
#>  7 a     
#>  8 pretty
#>  9 little
#> 10 fir   
#> 11 -     
#> 12 tree
\end{verbatim}

\hypertarget{character-tokens}{%
\subsection{Character tokens}\label{character-tokens}}

\index{tokenization!character}Perhaps the simplest tokenization is character tokenization, which splits texts into characters. Let's use \texttt{tokenize\_characters()} with its default parameters; this function has arguments to convert to lowercase and to strip all non-alphanumeric characters. These defaults will reduce the number of different tokens that are returned. The \texttt{tokenize\_*()} functions by default return a list of character vectors, one character vector for each string in the input.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tft\_token\_characters }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_characters}\NormalTok{(}\AttributeTok{x =}\NormalTok{ the\_fir\_tree,}
                                            \AttributeTok{lowercase =} \ConstantTok{TRUE}\NormalTok{,}
                                            \AttributeTok{strip\_non\_alphanum =} \ConstantTok{TRUE}\NormalTok{,}
                                            \AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

What do we see if we take a look?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(tft\_token\_characters) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> List of 6
#>  $ : chr [1:57] "f" "a" "r" "d" ...
#>  $ : chr [1:57] "r" "e" "s" "t" ...
#>  $ : chr [1:61] "w" "i" "s" "h" ...
#>  $ : chr [1:56] "a" "r" "o" "u" ...
#>  $ : chr [1:64] "l" "i" "t" "t" ...
#>  $ : chr [1:64] "t" "h" "e" "m" ...
\end{verbatim}

We don't have to stick with the defaults. We can keep the punctuation and spaces by setting \texttt{strip\_non\_alphanum\ =\ FALSE} and now we see that spaces and punctuation are included in the results too.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tokenize\_characters}\NormalTok{(}\AttributeTok{x =}\NormalTok{ the\_fir\_tree,}
                    \AttributeTok{strip\_non\_alphanum =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> List of 6
#>  $ : chr [1:73] "f" "a" "r" " " ...
#>  $ : chr [1:74] "r" "e" "s" "t" ...
#>  $ : chr [1:76] "w" "i" "s" "h" ...
#>  $ : chr [1:72] "a" "r" "o" "u" ...
#>  $ : chr [1:77] "l" "i" "t" "t" ...
#>  $ : chr [1:77] "t" "h" "e" "m" ...
\end{verbatim}

The results have more elements because the spaces and punctuation have not been removed.

Sometimes you run into problems where what a ``character'' is can be ambiguous. Depending on the format you have the data in, it might contain ligatures.\index{tokenization!ligatures} Ligatures are when multiple graphemes or letters are combined as a single character The graphemes ``f'' and ``l'' are combined into ``ﬂ'', or ``s'' and ``t'' into ``ﬆ''. When we apply normal tokenization rules the ligatures will not be split up.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tokenize\_characters}\NormalTok{(}\StringTok{"ﬂowers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "ﬂ" "o" "w" "e" "r" "s"
\end{verbatim}

We might want to have these ligatures separated back into separate characters, but first, we need to consider a couple of things. \index{tokenization!ligatures}First, we need to consider if the presence of ligatures is a meaningful feature to the question we are trying to answer. Second, there are two main types of ligatures, stylistic and functional. Stylistic ligatures are when two characters are combined because the spacing between the characters has been deemed unpleasant. Functional ligatures like the German Eszett (also called the scharfes S, meaning sharp s) ß, is an official letter of the German alphabet. It is described as a long S and Z and historically has never gotten an uppercase character. This has led the typesetters to use SZ or SS as a replacement when writing a word in uppercase. Additionally, ß is omitted entirely in German writing in Switzerland and is replaced with ss. Other examples include the ``W'' in the Latin alphabet (two ``v'' or two ``u'' joined together), and æ, ø, and å in the Nordic languages. Some place names for historical reasons use the old spelling ``aa'' instead of å. In Section \ref{text-normalization} we will discuss text normalization approaches to deal with ligatures.

Newcomers in terms of characters are emojis.\index{tokenization!emojis} While they do look whimsical, various tricks have been used to more effectively store them in Unicode. Let's first consider the flag emoji. As of the time of writing, 261 different flags have emoji representation. However, they do not have individual Unicode characters. Let's take a look:

\begin{center}\includegraphics[width=1\linewidth]{images/tokenization-emoji} \end{center}

\begin{verbatim}
#> [[1]]
#> [1] "\U0001f1e8\U0001f1e6" "\U0001f1e6\U0001f1f6" "\U0001f1ea\U0001f1fa"
#> [4] "\U0001f1ef\U0001f1f5"
\end{verbatim}

When tokenizing here we get to see the bare Unicode characters.\index{Unicode} Notice how each flag has two characters. If you were to look up the characters for the Canadian flag you will find that they are ``REGIONAL INDICATOR SYMBOLS'', \href{https://codepoints.net/U+1F1E8?lang=en}{``C''} and \href{https://codepoints.net/U+1F1E6?lang=en}{``A''} respectively. With this approach, all the flags can be represented using only 26 Unicode symbols. The same approach is used with the job emojis and gender modifiers, and general emojis with hairstyle modifiers, hair color modifiers, and skin tone modifiers\footnote{Full list of emoji modifiers here: \url{https://unicode.org/emoji/charts/full-emoji-modifiers.html}}. There is a lot of information packed into emojis, and it is useful to remember to check that your tokenizer is treating them the way you would expect.

\hypertarget{word-tokens}{%
\subsection{Word tokens}\label{word-tokens}}

Tokenizing at the word level is perhaps the most common and widely used tokenization. We started our discussion in this chapter with this kind of tokenization, and as we described before, this is the procedure of splitting text into words. To do this, let's use the \texttt{tokenize\_words()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tft\_token\_words }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_words}\NormalTok{(}\AttributeTok{x =}\NormalTok{ the\_fir\_tree,}
                                  \AttributeTok{lowercase =} \ConstantTok{TRUE}\NormalTok{,}
                                  \AttributeTok{stopwords =} \ConstantTok{NULL}\NormalTok{,}
                                  \AttributeTok{strip\_punct =} \ConstantTok{TRUE}\NormalTok{,}
                                  \AttributeTok{strip\_numeric =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The results show us the input text split into individual words.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(tft\_token\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> List of 6
#>  $ : chr [1:16] "far" "down" "in" "the" ...
#>  $ : chr [1:15] "resting" "place" "grew" "a" ...
#>  $ : chr [1:15] "wished" "so" "much" "to" ...
#>  $ : chr [1:14] "around" "it" "the" "sun" ...
#>  $ : chr [1:12] "little" "peasant" "children" "passed" ...
#>  $ : chr [1:13] "them" "not" "sometimes" "the" ...
\end{verbatim}

We have already seen \texttt{lowercase\ =\ TRUE}, and \texttt{strip\_punct\ =\ TRUE} and \texttt{strip\_numeric\ =\ FALSE} control whether we remove punctuation and numeric characters respectively. We also have \texttt{stopwords\ =\ NULL}, which we will talk about in more depth in Chapter \ref{stopwords}.

Let's create a tibble with two fairy tales, ``The Fir Tree'' and ``The Little Mermaid''. Then we can use \texttt{unnest\_tokens()} together with some \textbf{dplyr} verbs to find the most commonly used words in each.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hcandersen\_en }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(book }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"The fir tree"}\NormalTok{, }\StringTok{"The little mermaid"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(book, word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(book) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 3
#> # Groups:   book [2]
#>    book               word      n
#>    <chr>              <chr> <int>
#>  1 The fir tree       the     278
#>  2 The fir tree       and     161
#>  3 The fir tree       tree     76
#>  4 The fir tree       it       66
#>  5 The fir tree       a        56
#>  6 The little mermaid the     817
#>  7 The little mermaid and     398
#>  8 The little mermaid of      252
#>  9 The little mermaid she     240
#> 10 The little mermaid to      199
\end{verbatim}

The five most common words in each fairy tale are fairly uninformative, with the exception being \texttt{"tree"} in the ``The Fir Tree''.

\begin{rmdwarning}
These uninformative words are called \textbf{stop words} and will be explored in-depth in Chapter \ref{stopwords}.
\end{rmdwarning}

\hypertarget{tokenizingngrams}{%
\subsection{Tokenizing by n-grams}\label{tokenizingngrams}}

\index{tokenization!n-gram}An n-gram (sometimes written ``ngram'') is a term in linguistics for a contiguous sequence of \(n\) items from a given sequence of text or speech. The item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, they mean a group of \(n\) words. In this book, we will use n-gram to denote word n-grams unless otherwise stated.

\begin{rmdnote}
We use Latin prefixes so that a 1-gram is called a unigram, a 2-gram is
called a bigram, a 3-gram called a trigram, and so on.
\end{rmdnote}

Some example n-grams are:

\begin{itemize}
\item
  \textbf{unigram:} ``Hello'', ``day'', ``my'', ``little''
\item
  \textbf{bigram:} ``fir tree'', ``fresh air'', ``to be'', ``Robin Hood''
\item
  \textbf{trigram:} ``You and I'', ``please let go'', ``no time like'', ``the little mermaid''
\end{itemize}

The benefit of using n-grams compared to words is that n-grams capture word order which would otherwise be lost. Similarly, when we use character n-grams, we can model the beginning and end of words, because a space will be located at the end of an n-gram for the end of a word and at the beginning of an n-gram of the beginning of a word.

To split text into word n-grams, we can use the function \texttt{tokenize\_ngrams()}. It has a few more arguments, so let's go over them one by one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tft\_token\_ngram }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_ngrams}\NormalTok{(}\AttributeTok{x =}\NormalTok{ the\_fir\_tree,}
                                   \AttributeTok{lowercase =} \ConstantTok{TRUE}\NormalTok{,}
                                   \AttributeTok{n =}\NormalTok{ 3L,}
                                   \AttributeTok{n\_min =}\NormalTok{ 3L,}
                                   \AttributeTok{stopwords =} \FunctionTok{character}\NormalTok{(),}
                                   \AttributeTok{ngram\_delim =} \StringTok{" "}\NormalTok{,}
                                   \AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have seen the arguments \texttt{lowercase}, \texttt{stopwords}, and \texttt{simplify} before; they work the same as for the other tokenizers. We also have \texttt{n}, the argument to determine which degree of n-gram to return. Using \texttt{n\ =\ 1} returns unigrams, \texttt{n\ =\ 2} bigrams, \texttt{n\ =\ 3} gives trigrams, and so on. Related to \texttt{n} is the \texttt{n\_min} argument, which specifies the minimum number of n-grams to include. By default both \texttt{n} and \texttt{n\_min} are set to 3 making \texttt{tokenize\_ngrams()} return only trigrams. By setting \texttt{n\ =\ 3} and \texttt{n\_min\ =\ 1}, we will get all unigrams, bigrams, and trigrams of a text. Lastly, we have the \texttt{ngram\_delim} argument, which specifies the separator between words in the n-grams; notice that this defaults to a space.

Let's look at the result of n-gram tokenization for the first line of ``The Fir Tree''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tft\_token\_ngram[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "far down in"      "down in the"      "in the forest"    "the forest where"
#>  [5] "forest where the" "where the warm"   "the warm sun"     "warm sun and"    
#>  [9] "sun and the"      "and the fresh"    "the fresh air"    "fresh air made"  
#> [13] "air made a"       "made a sweet"
\end{verbatim}

Notice how the words in the trigrams overlap so that the word ``down'' appears in the middle of the first trigram and beginning of the second trigram. N-gram tokenization slides along the text to create overlapping sets of tokens.

It is important to choose the right value for \texttt{n} when using n-grams for the question we want to answer. Using unigrams is faster and more efficient, but we don't capture information about word order. Using a higher value for \texttt{n} keeps more information, but the vector space of tokens increases dramatically, corresponding to a reduction in token counts. A sensible starting point in most cases is three. However, if you don't have a large vocabulary in your data set, consider starting at two instead of three and experimenting from there. Figure \ref{fig:ngramtokens} demonstrates how token frequency starts to decrease dramatically for trigrams and higher-order n-grams.

\begin{figure}

{\centering \includegraphics{02_tokenization_files/figure-latex/ngramtokens-1} 

}

\caption{Using longer n-grams results in a higher number of unique tokens with fewer counts. Note that the color maps to counts on a logarithmic scale.}\label{fig:ngramtokens}
\end{figure}

We are not limited to use only one degree of n-grams. We can, for example, combine unigrams and bigrams in an analysis or model. Getting multiple degrees of n-grams is a little different depending on what package you are using; using \texttt{tokenize\_ngrams()} you can specify \texttt{n} and \texttt{n\_min}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tft\_token\_ngram }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_ngrams}\NormalTok{(}\AttributeTok{x =}\NormalTok{ the\_fir\_tree,}
                                   \AttributeTok{n =}\NormalTok{ 2L,}
                                   \AttributeTok{n\_min =}\NormalTok{ 1L)}
\NormalTok{tft\_token\_ngram[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "far"          "far down"     "down"         "down in"      "in"          
#>  [6] "in the"       "the"          "the forest"   "forest"       "forest where"
#> [11] "where"        "where the"    "the"          "the warm"     "warm"        
#> [16] "warm sun"     "sun"          "sun and"      "and"          "and the"     
#> [21] "the"          "the fresh"    "fresh"        "fresh air"    "air"         
#> [26] "air made"     "made"         "made a"       "a"            "a sweet"     
#> [31] "sweet"
\end{verbatim}

Combining different degrees of n-grams can allow you to extract different levels of detail from text data. Unigrams tell you which individual words have been used a lot of times; some of these words could be overlooked in bigram or trigram counts if they don't co-appear with other words often. Consider a scenario where every time the word ``dog'' was used it came after an adjective: ``happy dog'', ``sad dog'', ``brown dog'', ``white dog'', ``playful dog'', etc. If this is fairly consistent and the adjectives varied enough, then bigrams would not be able to detect that this story is about dogs. Similarly ``very happy'' and ``not happy'' will be recognized as different from bigrams and not with unigrams alone.

\hypertarget{lines-sentence-and-paragraph-tokens}{%
\subsection{Lines, sentence, and paragraph tokens}\label{lines-sentence-and-paragraph-tokens}}

Tokenizers to split text into larger units of text like lines, sentences, and paragraphs are rarely used directly for modeling purposes, as the tokens produced tend to be fairly unique. It is very uncommon for multiple sentences in a text to be identical! However, these tokenizers are useful for preprocessing and labeling.\index{preprocessing}

For example, Jane Austen's novel \emph{Northanger Abbey} (as available in the \textbf{janeaustenr} package) is already preprocessed with each line being at most 80 characters long. However, it might be useful to split the data into chapters and paragraphs instead.

Let's create a function that takes a dataframe containing a variable called \texttt{text} and turns it into a dataframe where the text is transformed into paragraphs. First, we can collapse the text into one long string using \texttt{collapse\ =\ "\textbackslash{}n"} to denote line breaks, and then next we can use \texttt{tokenize\_paragraphs()} to identify the paragraphs and put them back into a dataframe. We can add a paragraph count with \texttt{row\_number()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{add\_paragraphs }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
  \FunctionTok{pull}\NormalTok{(data, text) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{paste}\NormalTok{(}\AttributeTok{collapse =} \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tokenize\_paragraphs}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unlist}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{tibble}\NormalTok{(}\AttributeTok{text =}\NormalTok{ .) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{paragraph =} \FunctionTok{row\_number}\NormalTok{())}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we take the raw text data and add the chapter count by detecting when the characters \texttt{"CHAPTER"} appears at the beginning of a line. Then we \texttt{nest()} the text column, apply our \texttt{add\_paragraphs()} function, and then \texttt{unnest()} again.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janeaustenr)}

\NormalTok{northangerabbey\_paragraphed }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{text =}\NormalTok{ northangerabbey) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{chapter =} \FunctionTok{cumsum}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(text, }\StringTok{"\^{}CHAPTER "}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(chapter }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{,}
         \SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(text, }\StringTok{"\^{}CHAPTER "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest}\NormalTok{(}\AttributeTok{data =}\NormalTok{ text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{map}\NormalTok{(data, add\_paragraphs)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(data))}

\FunctionTok{glimpse}\NormalTok{(northangerabbey\_paragraphed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 1,020
#> Columns: 3
#> $ chapter   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, ~
#> $ text      <chr> "No one who had ever seen Catherine Morland in her infancy w~
#> $ paragraph <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1~
\end{verbatim}

Now we have 1020 separate paragraphs we can analyze. Similarly, we could go a step further to split these chapters into sentences, lines, or words.

It can be useful to be able to reshape text data to get a different observational unit. As an example, if you wanted to build a sentiment classifier that would classify sentences as hostile or not, then you need to work with and train your model on sentences of text.\index{sentiment classifier} Turning pages or paragraphs into sentences is a necessary step in your workflow.

Let us look at how we can turn \texttt{the\_fir\_tree} from a ``one line per element'' vector to a ``one sentence per element''. \texttt{the\_fir\_tree} comes as a vector so we start by using \texttt{paste()} to combine the lines back together. We use a space as the separator and then we pass it to the \texttt{tokenize\_sentences()} function from the tokenizers package which will perform sentence splitting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{the\_fir\_tree\_sentences }\OtherTok{\textless{}{-}}\NormalTok{ the\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{paste}\NormalTok{(}\AttributeTok{collapse =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tokenize\_sentences}\NormalTok{()}


\FunctionTok{head}\NormalTok{(the\_fir\_tree\_sentences[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Far down in the forest, where the warm sun and the fresh air made a
sweet resting-place, grew a pretty little fir-tree; and yet it was not happy,
it wished so much to be tall like its companions– the pines and firs which grew
around it."
#> [2] "The sun shone, and the soft air fluttered its leaves, and the little
peasant children passed by, prattling merrily, but the fir-tree heeded them
not."
#> [3] "Sometimes the children would bring a large basket of raspberries or
strawberries, wreathed on a straw, and seat themselves near the fir-tree, and
say, \"Is it not a pretty little tree?\""
#> [4] "which made it feel more unhappy than before."
#> [5] "And yet all this while the tree grew a notch or joint taller every
year; for by the number of joints in the stem of a fir-tree we can discover its
age."
#> [6] "Still, as it grew, it complained."
\end{verbatim}

If you have lines from different categories as we have in the \texttt{hcandersen\_en} dataframe, which contains all the lines of the fairy tales in English, then we would like to be able to turn these lines into sentences while preserving the \texttt{book} column in the data set.
To do this we use \texttt{nest()} and \texttt{map\_chr()} to create a dataframe where each fairy tale is its own element and then we use the \texttt{unnest\_sentences()} function from the tidytext package to split the text into sentences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hcandersen\_sentences }\OtherTok{\textless{}{-}}\NormalTok{ hcandersen\_en }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest}\NormalTok{(}\AttributeTok{data =} \FunctionTok{c}\NormalTok{(text)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{map\_chr}\NormalTok{(data, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{paste}\NormalTok{(.x}\SpecialCharTok{$}\NormalTok{text, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_sentences}\NormalTok{(sentences, data)}
\end{Highlighting}
\end{Shaded}

Now that we have turned the text into ``one sentence per element'', we can analyze on the sentence level.

\hypertarget{where-does-tokenization-break-down}{%
\section{Where does tokenization break down?}\label{where-does-tokenization-break-down}}

Tokenization will generally be one of the first steps when building a model or any kind of text analysis, so it is important to consider carefully what happens in this step of data preprocessing.\index{preprocessing} As with most software, there is a trade-off between speed and customizability, as demonstrated in Section \ref{tokenization-benchmark}. The fastest tokenization methods give us less control over how it is done.

While the defaults work well in many cases, we encounter situations where we want to impose stricter rules to get better or different tokenized results. Consider the following sentence.

\begin{quote}
``Don't forget you owe the bank \$1 million for the house.''
\end{quote}

This sentence has several interesting aspects which we need to decide whether to keep or to ignore when tokenizing. The first issue is the contraction in \texttt{"Don\textquotesingle{}t"} which presents us with several possible options. The fastest option is to keep this as one word, but it could also be split up into \texttt{"do"} and \texttt{"n\textquotesingle{}t"}.

\index{tokenization!punctuation}The next issue at hand is how to deal with \texttt{"\$1"}; the dollar sign is an important part of this sentence as it denotes a kind of currency. We could either remove or keep this punctuation symbol, and if we keep the dollar sign, we can choose between keeping one or two tokens, \texttt{"\$1"} or \texttt{"\$"} and \texttt{"1"}. If we look at the default for \texttt{tokenize\_words()}, we notice that it defaults to removing most punctuation including \$.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tokenize\_words}\NormalTok{(}\StringTok{"$1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "1"
\end{verbatim}

We can keep the dollar sign if we don't strip punctuation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tokenize\_words}\NormalTok{(}\StringTok{"$1"}\NormalTok{, }\AttributeTok{strip\_punct =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "$" "1"
\end{verbatim}

When dealing with this sentence, we also need to decide whether to keep the final period as a token or not. If we remove it, we will not be able to locate the last word in a sentence using n-grams.

\index{preprocessing!challenges}Information lost to tokenization (especially default tokenization) occurs more frequently in online and more casual text. Multiple spaces, extreme use of exclamation characters, and deliberate use of capitalization can be completely lost depending on our choice of tokenizer and tokenization parameters. At the same time, it is not always worth keeping that kind of information about how text is being used. If we are studying trends in disease epidemics using Twitter data, the style the tweets are written with is likely not nearly as important as what words are used. However, if we are trying to model social groupings, language style and how individuals use language toward each other becomes much more important.

Another thing to consider is the degree of compression each type of tokenization provides. The choice of tokenization results in a different pool of possible tokens, and can influence performance. By choosing a method that gives fewer possible tokens you allow later computational tasks to be performed faster. However, that comes with the risk of collapsing together categories of a different meaning. It is also worth noting that the spread of the number of different tokens varies with your choice of tokenizer.

Figure \ref{fig:tokendists} illustrates these points. Each of the fairy tales from \textbf{hcandersenr} has been tokenized in five different ways and the number of distinct tokens has been plotted along the x-axis (note that the x-axis is logarithmic). We see that the number of distinct tokens decreases if we convert words to lowercase or extract word stems (see Chapter \ref{stemming} for more on stemming). Second, notice that the distributions of distinct tokens for character tokenizers are quite narrow; these texts use all or most of the letters in the English alphabet.

\begin{figure}

{\centering \includegraphics{02_tokenization_files/figure-latex/tokendists-1} 

}

\caption{The number of distinct tokens can vary enormously for different tokenizers}\label{fig:tokendists}
\end{figure}

\hypertarget{building-your-own-tokenizer}{%
\section{Building your own tokenizer}\label{building-your-own-tokenizer}}

\index{tokenization!specialty}Sometimes the out-of-the-box tokenizers won't be able to do what you need them to do. In this case, we will have to wield \textbf{stringi}/\textbf{stringr} and regular expressions (see Appendix \ref{regexp}).

There are two main approaches to tokenization.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Split} the string up according to some rule.
\item
  \emph{Extract} tokens based on some rule.
\end{enumerate}

The number and complexity of our rules are determined by our desired outcome. We can reach complex outcomes by chaining together many smaller rules. In this section, we will implement a couple of specialty tokenizers to showcase these techniques.

\hypertarget{tokenize-to-characters-only-keeping-letters}{%
\subsection{Tokenize to characters, only keeping letters}\label{tokenize-to-characters-only-keeping-letters}}

Here we want to modify what \texttt{tokenize\_characters()} does such that we only keep letters. There are two main options. We can use \texttt{tokenize\_characters()} and remove anything that is not a letter, or we can extract the letters one by one. Let's try the latter option. This is an \textbf{extract} task and we will use \texttt{str\_extract\_all()} as each string has the possibility of including more than 1 token. Since we want to extract letters we can use the letters character class \texttt{{[}:alpha:{]}} to match letters and the quantifier \texttt{\{1\}} to only extract the first one.

\begin{rmdnote}
In this example, leaving out the quantifier yields the same result as
including it. However, for more complex regular expressions, specifying
the quantifier allows the string handling to run faster.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{letter\_tokens }\OtherTok{\textless{}{-}} \FunctionTok{str\_extract\_all}\NormalTok{(}
  \AttributeTok{string =} \StringTok{"This sentence include 2 numbers and 1 period."}\NormalTok{,}
  \AttributeTok{pattern =} \StringTok{"[:alpha:]\{1\}"}
\NormalTok{)}
\NormalTok{letter\_tokens}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>  [1] "T" "h" "i" "s" "s" "e" "n" "t" "e" "n" "c" "e" "i" "n" "c" "l" "u" "d" "e"
#> [20] "n" "u" "m" "b" "e" "r" "s" "a" "n" "d" "p" "e" "r" "i" "o" "d"
\end{verbatim}

We may be tempted to specify the character class as something like \texttt{{[}a-zA-Z{]}\{1\}}. This option would run faster, but we would lose non-English letter characters.\index{language!Non-English} This is a design choice we have to make depending on the goals of our specific problem.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{danish\_sentence }\OtherTok{\textless{}{-}} \StringTok{"Så mødte han en gammel heks på landevejen"}

\FunctionTok{str\_extract\_all}\NormalTok{(danish\_sentence, }\StringTok{"[:alpha:]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>  [1] "S" "å" "m" "ø" "d" "t" "e" "h" "a" "n" "e" "n" "g" "a" "m" "m" "e" "l" "h"
#> [20] "e" "k" "s" "p" "å" "l" "a" "n" "d" "e" "v" "e" "j" "e" "n"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(danish\_sentence, }\StringTok{"[a{-}zA{-}Z]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>  [1] "S" "m" "d" "t" "e" "h" "a" "n" "e" "n" "g" "a" "m" "m" "e" "l" "h" "e" "k"
#> [20] "s" "p" "l" "a" "n" "d" "e" "v" "e" "j" "e" "n"
\end{verbatim}

\begin{rmdwarning}
Choosing between \texttt{{[}:alpha:{]}} and \texttt{{[}a-zA-Z{]}} may
seem quite similar, but the resulting differences can have a big impact
on your analysis.
\end{rmdwarning}

\hypertarget{allow-for-hyphenated-words}{%
\subsection{Allow for hyphenated words}\label{allow-for-hyphenated-words}}

In our examples so far, we have noticed that the string ``fir-tree'' is typically split into two tokens. Let's explore two different approaches for how to handle this hyphenated word as one token. First, let's split on white space; this is a decent way to identify words in English and some other languages, and it does not split hyphenated words as the hyphen character isn't considered a white-space. \index{regex}Second, let's find a regex to match words with a hyphen and extract those.

Splitting by white-space is not too difficult because we can use character classes, as shown in Table \ref{tab:characterclasses}. We will use the white space character class \texttt{{[}:space:{]}} to split our sentence.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_split}\NormalTok{(}\StringTok{"This isn\textquotesingle{}t a sentence with hyphenated{-}words."}\NormalTok{, }\StringTok{"[:space:]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "This"              "isn't"             "a"                
#> [4] "sentence"          "with"              "hyphenated-words."
\end{verbatim}

This worked pretty well. This version doesn't drop punctuation, but we can achieve this by removing punctuation characters at the beginning and end of words.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_split}\NormalTok{(}\StringTok{"This isn\textquotesingle{}t a sentence with hyphenated{-}words."}\NormalTok{, }\StringTok{"[:space:]"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{str\_remove\_all}\NormalTok{(.x, }\StringTok{"\^{}[:punct:]+|[:punct:]+$"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "This"             "isn't"            "a"                "sentence"        
#> [5] "with"             "hyphenated-words"
\end{verbatim}

This regex used to remove the punctuation is a little complicated so let's discuss it, piece by piece.

\begin{itemize}
\item
  The regex \texttt{\^{}{[}:punct:{]}+} will look at the beginning of the string (\texttt{\^{}}) to match any punctuation characters (\texttt{{[}:punct:{]}}) where it will select one or more (\texttt{+}).
\item
  The other regex \texttt{{[}:punct:{]}+\$} will look for punctuation characters (\texttt{{[}:punct:{]}}) that appear one or more times (\texttt{+}) at the end of the string (\texttt{\$}).
\item
  These will alternate (\texttt{\textbar{}}) so that we get matches from both sides of the words.
\item
  The reason we use the quantifier \texttt{+} is that there are cases where a word is followed by multiple characters we don't want, such as \texttt{"okay..."} and \texttt{"Really?!!!"}.
\end{itemize}

We are using \texttt{map()} since \texttt{str\_split()} returns a list, and we want \texttt{str\_remove\_all()} to be applied to each element in the list. (The example here only has one element.)

Now let's see if we can get the same result using extraction. We will start by constructing a \index{regex}regular expression that will capture hyphenated words; our definition here is a word with one hyphen located inside it. Since we want the hyphen to be inside the word, we will need to have a non-zero number of characters on either side of the hyphen.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(}
  \AttributeTok{string =} \StringTok{"This isn\textquotesingle{}t a sentence with hyphenated{-}words."}\NormalTok{,}
  \AttributeTok{pattern =} \StringTok{"[:alpha:]+{-}[:alpha:]+"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "hyphenated-words"
\end{verbatim}

Wait, this only matched the hyphenated word! This happened because we are only matching words with hyphens. If we add the quantifier \texttt{?} then we can match 0 or 1 occurrences.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(}
  \AttributeTok{string =} \StringTok{"This isn\textquotesingle{}t a sentence with hyphenated{-}words."}\NormalTok{,}
  \AttributeTok{pattern =} \StringTok{"[:alpha:]+{-}?[:alpha:]+"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "This"             "isn"              "sentence"         "with"            
#> [5] "hyphenated-words"
\end{verbatim}

Now we are getting more words, but the ending of \texttt{"isn\textquotesingle{}t"} isn't there anymore and we lost the word \texttt{"a"}. We can get matches for the whole contraction by expanding the character class \texttt{{[}:alpha:{]}} to include the character \texttt{\textquotesingle{}}. We do that by using \texttt{{[}{[}:alpha:{]}\textquotesingle{}{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(}
  \AttributeTok{string =} \StringTok{"This isn\textquotesingle{}t a sentence with hyphenated{-}words."}\NormalTok{,}
  \AttributeTok{pattern =} \StringTok{"[[:alpha:]\textquotesingle{}]+{-}?[[:alpha:]\textquotesingle{}]+"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "This"             "isn't"            "sentence"         "with"            
#> [5] "hyphenated-words"
\end{verbatim}

Next, we need to find out why \texttt{"a"} wasn't matched. If we look at the regular expression, we remember that we imposed the restriction that a non-zero number of characters needed to surround the hyphen to avoid matching words that start or end with a hyphen. This means that the smallest possible pattern matched is 2 characters long. We can fix this by using an alternation with \texttt{\textbar{}}. We will keep our previous match on the left-hand side, and include \texttt{{[}:alpha:{]}\{1\}} on the right-hand side to match the single length words that won't be picked up by the left-hand side. Notice how we aren't using \texttt{{[}{[}:alpha:{]}\textquotesingle{}{]}} since we are not interested in matching single \texttt{\textquotesingle{}} characters.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(}
  \AttributeTok{string =} \StringTok{"This isn\textquotesingle{}t a sentence with hyphenated{-}words."}\NormalTok{,}
  \AttributeTok{pattern =} \StringTok{"[[:alpha:]\textquotesingle{}]+{-}?[[:alpha:]\textquotesingle{}]+|[:alpha:]\{1\}"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "This"             "isn't"            "a"                "sentence"        
#> [5] "with"             "hyphenated-words"
\end{verbatim}

That is getting to be quite a complex regex\index{regex}, but we are now getting the same answer as before.

\hypertarget{wrapping-it-in-a-function}{%
\subsection{Wrapping it in a function}\label{wrapping-it-in-a-function}}

We have shown how we can use regular expressions to extract the tokens we want, perhaps to use in modeling. So far, the code has been rather unstructured. We would ideally wrap these tasks into functions\index{functions} that can be used the same way \texttt{tokenize\_words()} is used.

Let's start with the example with hyphenated words. To make the function a little more flexible, let's add an option to transform all the output to lowercase.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tokenize\_hyphenated\_words }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{lowercase =} \ConstantTok{TRUE}\NormalTok{) \{}
  \ControlFlowTok{if}\NormalTok{ (lowercase)}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{str\_to\_lower}\NormalTok{(x)}

  \FunctionTok{str\_split}\NormalTok{(x, }\StringTok{"[:space:]"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{map}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{str\_remove\_all}\NormalTok{(.x, }\StringTok{"\^{}[:punct:]+|[:punct:]+$"}\NormalTok{))}
\NormalTok{\}}

\FunctionTok{tokenize\_hyphenated\_words}\NormalTok{(the\_fir\_tree[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>  [1] "far"    "down"   "in"     "the"    "forest" "where"  "the"    "warm"  
#>  [9] "sun"    "and"    "the"    "fresh"  "air"    "made"   "a"      "sweet" 
#> 
#> [[2]]
#>  [1] "resting-place" "grew"          "a"             "pretty"       
#>  [5] "little"        "fir-tree"      "and"           "yet"          
#>  [9] "it"            "was"           "not"           "happy"        
#> [13] "it"           
#> 
#> [[3]]
#>  [1] "wished"     "so"         "much"       "to"         "be"        
#>  [6] "tall"       "like"       "its"        "companions" "the"       
#> [11] "pines"      "and"        "firs"       "which"      "grew"
\end{verbatim}

Notice how we transformed to lowercase first because the rest of the operations are case insensitive.

Next let's turn our character n-gram tokenizer into a function\index{functions}, with a variable \texttt{n} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tokenize\_character\_ngram }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, n) \{}
\NormalTok{  ngram\_loc }\OtherTok{\textless{}{-}} \FunctionTok{str\_locate\_all}\NormalTok{(x, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"(?=(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w\{"}\NormalTok{, n, }\StringTok{"\}))"}\NormalTok{))}

  \FunctionTok{map2}\NormalTok{(ngram\_loc, x, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_sub}\NormalTok{(.y, .x[, }\DecValTok{1}\NormalTok{], .x[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\NormalTok{\}}

\FunctionTok{tokenize\_character\_ngram}\NormalTok{(the\_fir\_tree[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>  [1] "Far" "dow" "own" "the" "for" "ore" "res" "est" "whe" "her" "ere" "the"
#> [13] "war" "arm" "sun" "and" "the" "fre" "res" "esh" "air" "mad" "ade" "swe"
#> [25] "wee" "eet"
#> 
#> [[2]]
#>  [1] "res" "est" "sti" "tin" "ing" "pla" "lac" "ace" "gre" "rew" "pre" "ret"
#> [13] "ett" "tty" "lit" "itt" "ttl" "tle" "fir" "tre" "ree" "and" "yet" "was"
#> [25] "not" "hap" "app" "ppy"
#> 
#> [[3]]
#>  [1] "wis" "ish" "she" "hed" "muc" "uch" "tal" "all" "lik" "ike" "its" "com"
#> [13] "omp" "mpa" "pan" "ani" "nio" "ion" "ons" "the" "pin" "ine" "nes" "and"
#> [25] "fir" "irs" "whi" "hic" "ich" "gre" "rew"
\end{verbatim}

We can use \texttt{paste0()} in this function to construct an actual regex\index{regex}.

\hypertarget{tokenization-for-non-latin-alphabets}{%
\section{Tokenization for non-Latin alphabets}\label{tokenization-for-non-latin-alphabets}}

\index{language!Non-Latin}Our discussion of tokenization so far has focused on text where words are separated by white space and punctuation. For such text, even a quite basic tokenizer can give decent results. However, many written languages don't separate words in this way.

One of these languages is Chinese where each ``word'' can be represented by one or more consecutive characters.
Splitting Chinese text into words is called ``word segmentation''\index{word segmentation} and is still an active area of research \citep{ma-etal-2018-state, Huang2019}.

We are not going to go into depth in this area, but we want to showcase that word segmentation is indeed possible with R as well. We use the \textbf{jiebaR} package \citep{R-jiebaR}. It is conceptually similar to the tokenizers package, but we need to create a worker that is passed into \texttt{segment()} along with the string we want to segment.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(jiebaR)}
\NormalTok{words }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"下面是不分行输出的结果"}\NormalTok{, }\StringTok{"下面是不输出的结果"}\NormalTok{)}

\NormalTok{engine1 }\OtherTok{\textless{}{-}} \FunctionTok{worker}\NormalTok{(}\AttributeTok{bylines =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{segment}\NormalTok{(words, engine1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "下面" "是"   "不"   "分行" "输出" "的"   "结果"
#> 
#> [[2]]
#> [1] "下面" "是"   "不"   "输出" "的"   "结果"
\end{verbatim}

\hypertarget{tokenization-benchmark}{%
\section{Tokenization benchmark}\label{tokenization-benchmark}}

Not all tokenization packages are the same. Most open source tokenizers in R are well-designed but they are designed to serve different purposes. Some have a multitude of arguments to allow you to customize your tokenizer for greater flexibility, but this flexibility comes at a price; they tend to have relatively slower performance.

While we can't easily quantify flexibility, it is straightforward to benchmark some of the tokenizers\index{tokenization!package} available in R so you can pick the one that best suits your needs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bench}\SpecialCharTok{::}\FunctionTok{mark}\NormalTok{(}\AttributeTok{check =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{iterations =} \DecValTok{10}\NormalTok{,}
  \StringTok{\textasciigrave{}}\AttributeTok{corpus}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ corpus}\SpecialCharTok{::}\FunctionTok{text\_tokens}\NormalTok{(hcandersen\_en}\SpecialCharTok{$}\NormalTok{text),}
  \StringTok{\textasciigrave{}}\AttributeTok{tokenizers}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ tokenizers}\SpecialCharTok{::}\FunctionTok{tokenize\_words}\NormalTok{(hcandersen\_en}\SpecialCharTok{$}\NormalTok{text),}
  \StringTok{\textasciigrave{}}\AttributeTok{text2vec}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ text2vec}\SpecialCharTok{::}\FunctionTok{word\_tokenizer}\NormalTok{(hcandersen\_en}\SpecialCharTok{$}\NormalTok{text),}
  \StringTok{\textasciigrave{}}\AttributeTok{quanteda}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ quanteda}\SpecialCharTok{::}\FunctionTok{tokenize\_word}\NormalTok{(hcandersen\_en}\SpecialCharTok{$}\NormalTok{text),}
  \StringTok{\textasciigrave{}}\AttributeTok{base R}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{strsplit}\NormalTok{(hcandersen\_en}\SpecialCharTok{$}\NormalTok{text, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 5 x 6
#>   expression      min   median `itr/sec` mem_alloc `gc/sec`
#>   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>
#> 1 corpus       76.2ms   85.2ms     11.7     4.59MB    0    
#> 2 tokenizers   95.2ms  104.4ms      9.65    1.01MB    1.07 
#> 3 text2vec     77.4ms     85ms     11.9    19.21MB    0    
#> 4 quanteda    154.8ms  168.5ms      5.86     8.7MB    1.46 
#> 5 base R      296.9ms  305.2ms      2.99   10.51MB    0.747
\end{verbatim}

The corpus package \citep{Perry2020} offers excellent performance for tokenization, and other options are not much worse. One exception is using a base R function as a tokenizer; you will see significant performance gains by instead using a package built specifically for text tokenization.

\hypertarget{tokensummary}{%
\section{Summary}\label{tokensummary}}

To build a predictive model, text data needs to be split into meaningful units, called tokens. These tokens range from individual characters to words to n-grams and even more complex structures, and the particular procedure used to identify tokens from text can be important to your results. Fast and consistent tokenizers are available, but understanding how they behave and in what circumstances they work best will set you up for success. It's also possible to build custom tokenizers when necessary. Once text data is tokenized, a common next preprocessing step is to consider how to handle very common words that are not very informative, stop words. Chapter \ref{stopwords} examines this in detail.

\hypertarget{in-this-chapter-you-learned-1}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-1}}

\begin{itemize}
\item
  that tokens are meaningful units of text, such as words or n-grams
\item
  to implement different kinds of tokenization, the process of splitting text into tokens
\item
  how different kinds of tokenization affect the distribution of tokens
\item
  how to build your own tokenizer when the fast, consistent tokenizers that are available are not flexible enough
\end{itemize}

\hypertarget{stopwords}{%
\chapter{Stop words}\label{stopwords}}

Once we have split text into tokens, it often becomes clear that not all words carry the same amount of information, if any information at all, for a predictive modeling task. Common words that carry little (or perhaps no) meaningful information are called \emph{stop words}. It is common advice and practice to remove stop words for various NLP tasks, but the task of stop word removal is more nuanced than many resources may lead you to believe. In this chapter, we will investigate what a stop word list is, the differences between them, and the effects of using them in your preprocessing workflow.

The concept of stop words has a long history with Hans Peter Luhn credited with coining the term in 1960 \citep{Luhn1960}. Examples of these words in English are ``a'', ``the'', ``of'', and ``didn't''. These words are very common and typically don't add much to the meaning of a text but instead ensure the structure of a sentence is sound.

\begin{rmdnote}
Categorizing words as either informative or non-informative is limiting,
and we prefer to consider words as having a more fluid or continuous
amount of information associated with them. This informativeness is
context-specific as well. In fact, stop words themselves are often
important in genre or authorship identification.
\end{rmdnote}

Historically, one of the main reasons for removing stop words was to decrease the computational time for text mining; it can be regarded as a dimensionality reduction of text data and was commonly used in search engines to give better results \citep{Huston2010}.

Stop words can have different roles in a corpus. We generally categorize stop words into three groups: global, subject, and document stop words.

\index{stop words!global}Global stop words are words that are almost always low in meaning in a given language; these are words such as ``of'' and ``and'' in English which are needed to glue text together. These words are likely a safe bet for removal but they are small in number. You can find some global stop words in pre-made stop word lists (Section \ref{premadestopwords}).

\index{stop words!subject}Next up are subject-specific stop words. These words are uninformative for a given subject area. Subjects can be broad like finance and medicine or can be more specific like obituaries, health code violations, and job listings for librarians in Kansas.
Words like ``bath'', ``bedroom'', and ``entryway'' are generally not considered stop words in English, but they may not provide much information for differentiating suburban house listings and could be subject stop words for certain analysis. You will likely need to manually construct such a stop word list (Section \ref{homemadestopwords}). These kinds of stop words may improve your performance if you have the domain expertise to create a good list.

\index{stop words!document}Lastly, we have document level stop words. These words do not provide any or much information for a given document. These are difficult to classify and won't be worth the trouble to identify. Even if you can find document stop words, it is not obvious how to incorporate this kind of information in a regression or classification task.

\hypertarget{premadestopwords}{%
\section{Using premade stop word lists}\label{premadestopwords}}

A quick option for using stop words is to get a list that has already been created. This is appealing because it is not difficult, but be aware that not all lists are created equal. \citet{nothman-etal-2018-stop} found some alarming results in a study of 52 stop word lists available in open-source software packages. Among some of the more grave issues were misspellings\index{misspellings} (``fify'' instead of ``fifty''), the inclusion of clearly informative words such as ``computer'' and ``cry'', and internal inconsistencies such as including the word ``has'' but not the word ``does''. This is not to say that you should never use a stop word list that has been included in an open-source software project. However, you should always inspect and verify the list you are using, both to make sure it hasn't changed since you used it last, and also to check that it is appropriate for your use case.

There is a broad selection of stop word lists available today. For the purpose of this chapter, we will focus on three of the lists of English stop words provided by the \textbf{stopwords} package \citep{R-stopwords}. The first is from the SMART (System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System\index{stop word lists!SMART}, an information retrieval system developed at Cornell University in the 1960s \citep{Lewis2014}. The second is the \index{stop word lists!Snowball}English Snowball stop word list \citep{porter2001snowball}, and the last is the English list from the \href{https://github.com/stopwords-iso/stopwords-iso}{Stopwords ISO} collection\index{stop word lists!Stopwords ISO}. These stop word lists are all considered general purpose and not domain-specific.

\begin{rmdpackage}
The \textbf{stopwords} package contains a comprehensive collection of
stop word lists in one place for ease of use in analysis and other
packages.
\end{rmdpackage}

Before we start delving into the content inside the lists, let's take a look at how many words are included in each.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stopwords)}
\FunctionTok{length}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"smart"}\NormalTok{))}
\FunctionTok{length}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"snowball"}\NormalTok{))}
\FunctionTok{length}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"stopwords{-}iso"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 571
#> [1] 175
#> [1] 1298
\end{verbatim}

The lengths of these lists are quite different, with the longest list being over seven times longer than the shortest! Let's examine the overlap of the words that appear in the three lists in an UpSet plot in Figure \ref{fig:stopwordoverlap}. An UpSet plot \citep{Lex2014} visualizes intersections and aggregates of intersections of sets using a matrix layout, presenting the number of elements as well as summary statistics.

\index{stop word lists!SMART}
\index{stop word lists!Snowball}
\index{stop word lists!Stopwords ISO}

\begin{figure}

{\centering \includegraphics{03_stopwords_files/figure-latex/stopwordoverlap-1} 

}

\caption{Set intersections for three common stop word lists visualized as an UpSet plot}\label{fig:stopwordoverlap}
\end{figure}

The UpSet plot in Figure \ref{fig:stopwordoverlap} shows us that these three lists are almost true subsets of each other. The only exception is a set of ten words that appear in Snowball and ISO but not in the SMART list. What are those words?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setdiff}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"snowball"}\NormalTok{),}
        \FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"smart"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "she's"   "he'd"    "she'd"   "he'll"   "she'll"  "shan't"  "mustn't"
#>  [8] "when's"  "why's"   "how's"
\end{verbatim}

\index{stop word lists!SMART}
\index{stop word lists!Snowball}
All these words are contractions. This is \emph{not} because the SMART lexicon doesn't include contractions; if we look, there are almost fifty of them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_subset}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"smart"}\NormalTok{), }\StringTok{"\textquotesingle{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "a's"       "ain't"     "aren't"    "c'mon"     "c's"       "can't"    
#>  [7] "couldn't"  "didn't"    "doesn't"   "don't"     "hadn't"    "hasn't"   
#> [13] "haven't"   "he's"      "here's"    "i'd"       "i'll"      "i'm"      
#> [19] "i've"      "isn't"     "it'd"      "it'll"     "it's"      "let's"    
#> [25] "shouldn't" "t's"       "that's"    "there's"   "they'd"    "they'll"  
#> [31] "they're"   "they've"   "wasn't"    "we'd"      "we'll"     "we're"    
#> [37] "we've"     "weren't"   "what's"    "where's"   "who's"     "won't"    
#> [43] "wouldn't"  "you'd"     "you'll"    "you're"    "you've"
\end{verbatim}

We seem to have stumbled upon an inconsistency; why does \index{stop word lists!SMART}SMART include \texttt{"he\textquotesingle{}s"} but not \texttt{"she\textquotesingle{}s"}? It is hard to say, but this could be worth rectifying before applying these stop word lists to an analysis or model preprocessing.\index{preprocessing!challenges} This stop word list was likely generated by selecting the most frequent words across a large corpus of text that had more representation for text about men than women. This is once again a reminder that we should always look carefully at any pre-made word list or another artifact we use to make sure it works well with our needs\footnote{This advice applies to any kind of pre-made lexicon or word list, not just stop words. For instance, the same concerns apply to sentiment lexicons. The NRC sentiment lexicon of \citet{Mohammad13} associates the word ``white'' with trust and the word ``black'' with sadness, which could have unintended consequences when analyzing text about racial groups.}.

\begin{rmdwarning}
It is perfectly acceptable to start with a premade word list and remove
or append additional words according to your particular use case.
\end{rmdwarning}

\index{preprocessing}When you select a stop word list, it is important that you consider its size and breadth. Having a small and concise list of words can moderately reduce your token count while not having too great of an influence on your models, assuming that you picked appropriate words. As the size of your stop word list grows, each word added will have a diminishing positive effect with the increasing risk that a meaningful word has been placed on the list by mistake. In Section \ref{casestudystopwords}, we show the effects of different stop word lists on model training.

\hypertarget{stop-word-removal-in-r}{%
\subsection{Stop word removal in R}\label{stop-word-removal-in-r}}

Now that we have seen stop word lists, we can move forward with removing these words. The particular way we remove stop words depends on the shape of our data. If you have your text in a tidy format with one word per row, you can use \texttt{filter()} from \textbf{dplyr} with a negated \texttt{\%in\%} if you have the stop words as a vector, or you can use \texttt{anti\_join()} from \textbf{dplyr} if the stop words are in a \texttt{tibble()}. Like in our previous chapter, let's examine the text of ``The Fir-Tree'' by Hans Christian Andersen, and use \textbf{tidytext} to tokenize the text into words.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(hcandersenr)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidytext)}

\NormalTok{fir\_tree }\OtherTok{\textless{}{-}} \FunctionTok{hca\_fairytales}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(book }\SpecialCharTok{==} \StringTok{"The fir tree"}\NormalTok{,}
\NormalTok{         language }\SpecialCharTok{==} \StringTok{"English"}\NormalTok{)}

\NormalTok{tidy\_fir\_tree }\OtherTok{\textless{}{-}}\NormalTok{ fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text)}
\end{Highlighting}
\end{Shaded}

\index{stop word lists!Snowball}Let's use the Snowball stop word list as an example. Since the stop words return from this function as a vector, we will use \texttt{filter()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(word }\SpecialCharTok{\%in\%} \FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"snowball"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1,547 x 3
#>    book         language word   
#>    <chr>        <chr>    <chr>  
#>  1 The fir tree English  far    
#>  2 The fir tree English  forest 
#>  3 The fir tree English  warm   
#>  4 The fir tree English  sun    
#>  5 The fir tree English  fresh  
#>  6 The fir tree English  air    
#>  7 The fir tree English  made   
#>  8 The fir tree English  sweet  
#>  9 The fir tree English  resting
#> 10 The fir tree English  place  
#> # ... with 1,537 more rows
\end{verbatim}

If we use the \texttt{get\_stopwords()} function from \textbf{tidytext} instead, then we can use the \texttt{anti\_join()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"snowball"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1,547 x 3
#>    book         language word   
#>    <chr>        <chr>    <chr>  
#>  1 The fir tree English  far    
#>  2 The fir tree English  forest 
#>  3 The fir tree English  warm   
#>  4 The fir tree English  sun    
#>  5 The fir tree English  fresh  
#>  6 The fir tree English  air    
#>  7 The fir tree English  made   
#>  8 The fir tree English  sweet  
#>  9 The fir tree English  resting
#> 10 The fir tree English  place  
#> # ... with 1,537 more rows
\end{verbatim}

The result of these two stop word removals is the same since we used the same stop word list in both cases.

\hypertarget{homemadestopwords}{%
\section{Creating your own stop words list}\label{homemadestopwords}}

Another way to get a stop word list is to create one yourself. Let's explore a few different ways to find appropriate words to use. We will use the tokenized data from ``The Fir-Tree'' as a first example. Let's take the words and rank them by their count or frequency.

\begin{figure}

{\centering \includegraphics{03_stopwords_files/figure-latex/unnamed-chunk-10-1} 

}

\caption{Words from "The Fir Tree" ordered by count or frequency}\label{fig:unnamed-chunk-10}
\end{figure}

We recognize many of what we would consider stop words in the first column here, with three big exceptions. We see \texttt{"tree"} at 3, \texttt{"fir"} at 12 and \texttt{"little"} at 22. These words appear high on our list but they do provide valuable information as they all reference the main character. \index{preprocessing!challenges}What went wrong with this approach? Creating a stop word list using high-frequency words works best when it is created on a \textbf{corpus} of documents\index{corpus}, not an individual document. This is because the words found in a single document will be document specific and the overall pattern of words will not generalize that well.

\begin{rmdnote}
In NLP, a corpus is a set of texts or documents. The set of Hans Christian Andersen's fairy tales can be considered a corpus, with each fairy tale a document within that corpus. The set of United States Supreme Court opinions can be considered a different corpus, with each written opinion being a document within \emph{that} corpus. Both data sets are described in more detail in Appendix \ref{appendixdata}.
\end{rmdnote}
\index{corpus!definition}

The word \texttt{"tree"} does seem important as it is about the main character, but it could also be appearing so often that it stops providing any information. Let's try a different approach, extracting high-frequency words from the corpus of \emph{all} English fairy tales by H.C. Andersen.

\begin{figure}

{\centering \includegraphics{03_stopwords_files/figure-latex/unnamed-chunk-13-1} 

}

\caption{Words in all English fairy tales by Hans Christian Andersen ordered by count or frequency}\label{fig:unnamed-chunk-13}
\end{figure}

This list is more appropriate for our concept of stop words, and now it is time for us to make some choices. How many do we want to include in our stop word list? Which words should we add and/or remove based on prior information? Selecting the number of words to remove is best done by a case-by-case basis as it can be difficult to determine a priori how many different ``meaningless'' words appear in a corpus. Our suggestion is to start with a low number like twenty and increase by ten words until you get to words that are not appropriate as stop words for your analytical purpose.

It is worth keeping in mind that such a list is not perfect.\index{preprocessing!challenges} Depending on how your text was generated or processed, strange tokens can surface as possible stop words due to encoding or optical character recognition errors. Further, these results are based on the corpus of documents we have available, which is potentially biased. In our example here, all the fairy tales were written by the same European white man from the early 1800s.

\begin{rmdnote}
This bias can be minimized by removing words we would expect to be
over-represented or to add words we expect to be under-represented.
\end{rmdnote}

Easy examples are to include the complements to the words in the list if they are not already present. Include ``big'' if ``small'' is present, ``old'' if ``young'' is present. This example list has words associated with women often listed lower in rank than words associated with men. With \texttt{"man"} being at rank 79 but \texttt{"woman"} at rank 179, choosing a threshold of 100 would lead to only one of these words being included. Depending on how important you think such nouns are going to be in your texts, consider either adding \texttt{"woman"} or deleting \texttt{"man"}.\footnote{On the other hand, the more biased stop word list may be helpful when modeling a corpus with gender imbalance, depending on your goal; words like ``she'' and ``her'' can identify where women are mentioned.}

\index{bias}Figure \ref{fig:genderrank} shows how the words associated with men have a higher rank than the words associated with women. By using a single threshold to create a stop word list, you would likely only include one form of such words.

\begin{figure}

{\centering \includegraphics{03_stopwords_files/figure-latex/genderrank-1} 

}

\caption{Tokens ranked according to total occurrences, with rank 1 having the most occurrences}\label{fig:genderrank}
\end{figure}

Imagine now we would like to create a stop word list that spans multiple different genres, in such a way that the subject-specific stop words don't overlap. For this case, we would like words to be denoted as a stop word only if it is a stop word in all the genres. You could find the words individually in each genre and use the right intersections. However, that approach might take a substantial amount of time.

Below is a bad approach where we try to create a multi-language list of stop words. To accomplish this we calculate the \href{https://www.tidytextmining.com/tfidf.html}{\emph{inverse document frequency}} (IDF) \index{inverse document frequency}of each word. The inverse document frequency of a word is a quantity that is low for commonly used words in a collection of documents and high for words not used often in a collection of documents. It is typically defined as

\[idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}\]
If the word ``dog'' appears in 4 out of 100 documents then it would have an \texttt{idf("dog")\ =\ log(100/4)\ =\ 3.22} and if the word ``cat'' appears in 99 out of 100 documents then it would have an \texttt{idf("cat")\ =\ log(100/99)\ =\ 0.01}. Notice how the idf values goes to zero (as a matter of fact when a term appears in all the documents then the idf of that word is 0 \texttt{log(100/100)\ =\ log(1)\ =\ 0}), the more documents it is contained in.
What happens if we create a stop word list based on words with the lowest IDF? The following function takes a tokenized dataframe and returns a dataframe with a column for each word and a column for the IDF.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rlang)}
\NormalTok{calc\_idf }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, word, document) \{}
\NormalTok{  words }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(\{\{word\}\}) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{()}
\NormalTok{  n\_docs }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(}\FunctionTok{pull}\NormalTok{(df, \{\{document\}\})))}
\NormalTok{  n\_words }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nest}\NormalTok{(}\AttributeTok{data =} \FunctionTok{c}\NormalTok{(\{\{word\}\})) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pull}\NormalTok{(data) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{map\_dfc}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ words }\SpecialCharTok{\%in\%} \FunctionTok{unique}\NormalTok{(}\FunctionTok{pull}\NormalTok{(.x, \{\{word\}\}))) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{rowSums}\NormalTok{()}
  
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{word =}\NormalTok{ words,}
         \AttributeTok{idf =} \FunctionTok{log}\NormalTok{(n\_docs }\SpecialCharTok{/}\NormalTok{ n\_words))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Here is the result when we try to create a cross-language list of stop words, by taking each fairy tale as a document. It is not very good!

\begin{rmdnote}
The overlap between words that appear in each language is very small,
but these words are what we mostly see in this list.
\end{rmdnote}

\begin{figure}

{\centering \includegraphics{03_stopwords_files/figure-latex/unnamed-chunk-18-1} 

}

\caption{Words from all of H.C. Andersen's fairy tales in Danish, English, French, German, and Spanish, counted and ordered by IDF}\label{fig:unnamed-chunk-18}
\end{figure}

\begin{rmdwarning}
This didn't work very well because there is very little overlap between
common words. Instead, let us limit the calculation to only one language
and calculate the IDF of each word we can find compared to words that
appear in a lot of documents.
\end{rmdwarning}

\index{inverse document frequency}

\begin{figure}

{\centering \includegraphics{03_stopwords_files/figure-latex/unnamed-chunk-21-1} 

}

\caption{Words from all of H.C. Andersen's fairy tales in English, counted and ordered by IDF}\label{fig:unnamed-chunk-21}
\end{figure}

This time we get better results. The list starts with ``a'', ``the'', ``and'', and ``to'' and continues with many more reasonable choices of stop words. We need to look at these results manually to turn this into a list. We need to go as far down in rank as we are comfortable with. You as a data practitioner are in full control of how you want to create the list. If you don't want to include ``little'' you are still able to add ``are'' to your list even though it is lower on the list.

\hypertarget{all-stop-word-lists-are-context-specific}{%
\section{All stop word lists are context-specific}\label{all-stop-word-lists-are-context-specific}}

\index{preprocessing!challenges}Context is important in text modeling, so it is important to ensure that the stop word lexicon you use reflects the word space that you are planning on using it in. One common concern to consider is how pronouns bring information to your text. Pronouns are included in many different stop word lists (although inconsistently) but they will often \emph{not} be noise in text data. Similarly, \citet{Bender2021} discuss how a list of about 400 ``Dirty, Naughty, Obscene or Otherwise Bad Words''\index{language!obscene} were used to filter and remove text before training a trillion parameter large language model, to protect it from learning offensive language, but the authors point out that in some community contexts, such words are reclaimed or used to describe marginalized identities.\index{context!importance of}

On the other hand, sometimes you will have to add in words yourself, depending on the domain. If you are working with texts for dessert recipes, certain ingredients (sugar, eggs, water) and actions (whisking, baking, stirring) may be frequent enough to pass your stop word threshold, but you may want to keep them as they may be informative. Throwing away ``eggs'' as a common word would make it harder or downright impossible to determine if certain recipes are vegan or not while whisking and stirring may be fine to remove as distinguishing between recipes that do and don't require a whisk might not be that big of a deal.

\hypertarget{what-happens-when-you-remove-stop-words}{%
\section{What happens when you remove stop words}\label{what-happens-when-you-remove-stop-words}}

We have discussed different ways of finding and removing stop words; now let's see what happens once you do remove them. First, let's explore the impact of the number of words that are included in the list. Figure \ref{fig:stopwordresults} shows what percentage of words are removed as a function of the number of words in a text. The different colors represent the three different stop word lists we have considered in this chapter.

\begin{figure}

{\centering \includegraphics{03_stopwords_files/figure-latex/stopwordresults-1} 

}

\caption{Proportion of words removed for different stop word lists and different document lengths}\label{fig:stopwordresults}
\end{figure}

We notice, as we would predict, that larger stop word lists remove more words than shorter stop word lists. In this example with fairy tales, over half of the words have been removed, with the largest list removing over 80\% of the words. We observe that shorter texts have a lower percentage of stop words. Since we are looking at fairy tales, this could be explained by the fact that a story has to be told regardless of the length of the fairy tale, so shorter texts are going to be denser with more informative words.

Another problem you may face is dealing with misspellings.\index{misspellings}

\begin{rmdwarning}
Most premade stop word lists assume that all the words are spelled
correctly.
\end{rmdwarning}

Handling misspellings when using premade lists can be done by manually adding common misspellings.\index{misspellings} You could imagine creating all words that are a certain string distance away from the stop words, but we do not recommend this as you would quickly include informative words this way.

\index{preprocessing!challenges}One of the downsides of creating your own stop word lists using frequencies is that you are limited to using words that you have already observed. It could happen that ``she'd'' is included in your training corpus but the word ``he'd'' did not reach the threshold. This is a case where you need to look at your words and adjust accordingly. Here the large premade stop word lists can serve as inspiration for missing words.

In Section \ref{casestudystopwords} we investigate the influence of removing stop words in the context of modeling. Given the right list of words, we see no harm to the model performance, and may even find improvement in due to noise reduction \citep{Feldman2007}.

\hypertarget{stop-words-in-languages-other-than-english}{%
\section{Stop words in languages other than English}\label{stop-words-in-languages-other-than-english}}

So far in this chapter, we have focused on English stop words, but English is not representative of every language. The notion of ``short'' and ``long'' lists we have used so far are specific to English as a language. You should expect different languages\index{language!Non-English} to have a different number of ``uninformative'' words, and for this number to depend on the morphological\index{morphology} richness of a language; lists that contain all possible morphological variants of each stop word could become quite large.

Different languages have different numbers of words in each class of words. An example is how the grammatical case influences the articles used in German. Below are tables showing the use of \href{https://deutsch.lingolia.com/en/grammar/nouns-and-articles/articles-noun-markers}{definite and indefinite articles in German}. Notice how German nouns have three genders (masculine, feminine, and neuter), which are not uncommon in languages around the world. Articles are almost always considered to be stop words in English as they carry very little information. However, German articles give some indication of the case which can be used when selecting a list of stop words in German, or any other language where the grammatical case is reflected in the text.

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lllll}
\caption*{
\large German Definite Articles (the)\\ 
\small \\ 
} \\ 
\toprule
 & Masculine & Feminine & Neuter & Plural \\ 
\midrule
Nominative & der & die & das & die \\ 
Accusative & den & die & das & die \\ 
Dative & dem & der & dem & den \\ 
Genitive & des & der & des & der \\ 
\bottomrule
\end{longtable}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{llll}
\caption*{
\large German Indefinite Articles (a/an)\\ 
\small \\ 
} \\ 
\toprule
 & Masculine & Feminine & Neuter \\ 
\midrule
Nominative & ein & eine & ein \\ 
Accusative & einen & eine & ein \\ 
Dative & einem & einer & einem \\ 
Genitive & eines & einer & eines \\ 
\bottomrule
\end{longtable}

Building lists of stop words in Chinese has been done both manually and automatically \citep{Zou2006ACC} but so far none has been accepted as a standard \citep{Zou2006}. A full discussion of stop word identification in Chinese text would be out of scope for this book, so we will just highlight some of the challenges that differentiate it from English.

\begin{rmdwarning}
Chinese text is much more complex than portrayed here. With different
systems and billions of users, there is much we won't be able to touch
on here.
\end{rmdwarning}

\index{language!Non-English}The main difference from English is the use of logograms instead of letters to convey information. However, Chinese characters should not be confused with Chinese words. The majority of words in modern Chinese are composed of multiple characters. This means that inferring the presence of words is more complicated and the notion of stop words will affect how this segmentation of characters is done.

\hypertarget{stopwordssummary}{%
\section{Summary}\label{stopwordssummary}}

In many standard NLP workflows, the removal of stop words is presented as a default or the correct choice without comment. Although removing stop words can improve the accuracy of your machine learning using text data, choices around such a step are complex. The content of existing stop word lists varies tremendously, and the available strategies for building your own can have subtle to not-so-subtle effects on your model results.

\hypertarget{in-this-chapter-you-learned-2}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-2}}

\begin{itemize}
\item
  what a stop word is and how to remove stop words from text data
\item
  how different stop word lists can vary
\item
  that the impact of stop word removal is different for different kinds of texts
\item
  about the bias built in to stop word lists and strategies for building such lists
\end{itemize}

\hypertarget{stemming}{%
\chapter{Stemming}\label{stemming}}

When we deal with text, often documents contain different versions of one base word, often called a \emph{stem}. ``The Fir-Tree'', for example, contains more than one version (i.e., inflected form) of the word \texttt{"tree"}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(hcandersenr)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidytext)}

\NormalTok{fir\_tree }\OtherTok{\textless{}{-}} \FunctionTok{hca\_fairytales}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(book }\SpecialCharTok{==} \StringTok{"The fir tree"}\NormalTok{,}
\NormalTok{         language }\SpecialCharTok{==} \StringTok{"English"}\NormalTok{)}

\NormalTok{tidy\_fir\_tree }\OtherTok{\textless{}{-}}\NormalTok{ fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{())}

\NormalTok{tidy\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(word, }\StringTok{"\^{}tree"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 3 x 2
#>   word       n
#>   <chr>  <int>
#> 1 tree      76
#> 2 trees     12
#> 3 tree's     1
\end{verbatim}

Trees, we see once again, are important in this story; the singular form appears 76 times and the plural form appears twelve times. (We'll come back to how we might handle the apostrophe in \texttt{"tree\textquotesingle{}s"} later in this chapter.)

What if we aren't interested in the difference between \texttt{"trees"} and \texttt{"tree"} \index{singular versus plural}and we want to treat both together? That idea is at the heart of \emph{stemming}, the process of identifying the base word (or stem) for a data set of words. Stemming is concerned with the linguistics subfield of morphology\index{morphology}, how words are formed. In this example, \texttt{"trees"} would lose its letter \texttt{"s"} while \texttt{"tree"} stays the same. If we counted word frequencies again after stemming, we would find that there are 88 occurrences of the stem \texttt{"tree"} (89, if we also find the stem for \texttt{"tree\textquotesingle{}s"}).

\hypertarget{how-to-stem-text-in-r}{%
\section{How to stem text in R}\label{how-to-stem-text-in-r}}

There have been many algorithms built for stemming words over the past half century or so; we'll focus on two approaches. \index{stemming algorithm!Porter}The first is the stemming algorithm of \citet{Porter80}, probably the most widely used stemmer for English. Porter himself released the algorithm implemented in the framework \href{https://snowballstem.org/}{Snowball} with an open-source license; you can use it from R via the \textbf{SnowballC} package \citep{R-SnowballC}. (It has been extended to languages other than English as well.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(SnowballC)}

\NormalTok{tidy\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(stem, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 570 x 2
#>    stem        n
#>    <chr>   <int>
#>  1 tree       88
#>  2 fir        34
#>  3 littl      23
#>  4 said       22
#>  5 stori      16
#>  6 thought    16
#>  7 branch     15
#>  8 on         15
#>  9 came       14
#> 10 know       14
#> # ... with 560 more rows
\end{verbatim}

Take a look at those stems. Notice that we do now have 88 incidences of \texttt{"tree"}. Also notice that some words don't look like they are spelled as real words; this is normal and expected with this stemming algorithm. The Porter algorithm identifies the stem of both \texttt{"story"} and \texttt{"stories"} as \texttt{"stori"}, not a regular English word but instead a special stem object.

\begin{rmdnote}
If you want to tokenize \emph{and} stem your text data, you can try out the function \texttt{tokenize\_word\_stems()} from the tokenizers package, which implements Porter stemming just like what we demonstrated here. For more on tokenization, see Chapter \ref{tokenization}.
\end{rmdnote}

Does Porter stemming\index{stemming algorithm!Porter} only work for English? Far from it! We can use the \texttt{language} argument to implement Porter stemming in multiple languages.\index{language!Non-English} First we can tokenize the text and \texttt{nest()} into list-columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stopword\_df }\OtherTok{\textless{}{-}} \FunctionTok{tribble}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{language, }\SpecialCharTok{\textasciitilde{}}\NormalTok{two\_letter,}
                       \StringTok{"danish"}\NormalTok{,  }\StringTok{"da"}\NormalTok{,}
                       \StringTok{"english"}\NormalTok{, }\StringTok{"en"}\NormalTok{,}
                       \StringTok{"french"}\NormalTok{,  }\StringTok{"fr"}\NormalTok{,}
                       \StringTok{"german"}\NormalTok{,  }\StringTok{"de"}\NormalTok{,}
                       \StringTok{"spanish"}\NormalTok{, }\StringTok{"es"}\NormalTok{)}

\NormalTok{tidy\_by\_lang }\OtherTok{\textless{}{-}} \FunctionTok{hca\_fairytales}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(book }\SpecialCharTok{==} \StringTok{"The fir tree"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(text, language) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{language =} \FunctionTok{str\_to\_lower}\NormalTok{(language)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest}\NormalTok{(}\AttributeTok{data =}\NormalTok{ word)}
\end{Highlighting}
\end{Shaded}

Then we can remove stop words (using \texttt{get\_stopwords(language\ =\ "da")} and similar for each language) and stem with the language-specific Porter algorithm. What are the top 20 stems for ``The Fir-Tree'' in each of these five languages, after removing the Snowball stop words\index{stop word lists!Snowball} for that language?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_by\_lang }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(stopword\_df) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{map2}\NormalTok{(}
\NormalTok{    data, two\_letter, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{anti\_join}\NormalTok{(.x, }\FunctionTok{get\_stopwords}\NormalTok{(}\AttributeTok{language =}\NormalTok{ .y)))}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word, }\AttributeTok{language =}\NormalTok{ language)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(language) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(stem) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{20}\NormalTok{, n) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  ungroup }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n, }\FunctionTok{fct\_reorder}\NormalTok{(stem, n), }\AttributeTok{fill =}\NormalTok{ language)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{language, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Frequency"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{04_stemming_files/figure-latex/porterlanguages-1} 

}

\caption{Porter stemming results in five languages}\label{fig:porterlanguages}
\end{figure}

Figure \ref{fig:porterlanguages} demonstrates some of the challenges in working with languages other English\index{preprocessing!challenges}\index{language!Non-English}; the stop word lists may not be even from language to language, and tokenization strategies that work for a language like English may struggle for a language like French with more stop word contractions. Given that, we see here words about little fir trees at the top for all languages, in their stemmed forms.

\index{stemming algorithm!Porter}The Porter stemmer is an algorithm that starts with a word and ends up with a single stem, but that's not the only kind of stemmer out there. Another class of stemmer are dictionary-based stemmers. One such stemmer is the stemming algorithm of the \href{http://hunspell.github.io/}{Hunspell} library.\index{stemming algorithm!Hunspell} The ``Hun'' in Hunspell stands for Hungarian; this set of NLP algorithms was originally written to handle Hungarian but has since been extended to handle many languages with compound words and complicated morphology.\index{language!Non-English} The Hunspell library is used mostly as a spell checker, but as part of identifying correct spellings, this library identifies word stems as well. You can use the Hunspell library from R via the \textbf{hunspell} \citep{R-hunspell} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(hunspell)}

\NormalTok{tidy\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{hunspell\_stem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(stem) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(stem, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 595 x 2
#>    stem       n
#>    <chr>  <int>
#>  1 tree      89
#>  2 fir       34
#>  3 little    23
#>  4 said      22
#>  5 story     16
#>  6 branch    15
#>  7 one       15
#>  8 came      14
#>  9 know      14
#> 10 now       14
#> # ... with 585 more rows
\end{verbatim}

Notice that the code here is a little different (we had to use \texttt{unnest()}) and that the results are a little different. We have only real English words, and we have more total rows in the result. What happened?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hunspell\_stem}\NormalTok{(}\StringTok{"discontented"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "contented" "content"
\end{verbatim}

\index{stemming algorithm!Hunspell}We have \emph{two} stems! This stemmer works differently; it uses both morphological analysis of a word and existing dictionaries to find possible stems. It's possible to end up with more than one, and it's possible for a stem to be a word that is not related by meaning to the original word. For example, one of the stems of ``number'' is ``numb'' with this library. The Hunspell library was built to be a spell checker, so depending on your analytical purposes, it may not be an appropriate choice.

\hypertarget{should-you-use-stemming-at-all}{%
\section{Should you use stemming at all?}\label{should-you-use-stemming-at-all}}

You will often see stemming as part of NLP pipelines, sometimes without much comment about when it is helpful or not. We encourage you to think of stemming as a preprocessing\index{preprocessing} step in text modeling, one that must be thought through and chosen (or not) with good judgment.

Why does stemming often help, if you are training a machine learning model for text? Stemming \emph{reduces the feature space} of text data. Let's see this in action, with a data set of United States Supreme Court opinions available in the \textbf{scotus} package, discussed in more detail in Section \ref{scotus-opinions}. How many words are there, after removing a standard data set of stopwords?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(scotus)}

\NormalTok{tidy\_scotus }\OtherTok{\textless{}{-}}\NormalTok{ scotus\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{())}

\NormalTok{tidy\_scotus }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 167,879 x 2
#>    word        n
#>    <chr>   <int>
#>  1 court  286448
#>  2 v      204176
#>  3 state  148320
#>  4 states 128160
#>  5 case   121439
#>  6 act    111033
#>  7 s.ct   108168
#>  8 u.s    106413
#>  9 upon   105069
#> 10 united 103267
#> # ... with 167,869 more rows
\end{verbatim}

There are 167,879 distinct words in this data set we have created (after removing stopwords) but notice that even in the most common words we see a pair like \texttt{"state"} and \texttt{"states"}. A common data structure for modeling, and a helpful mental model for thinking about the sparsity of text data, is a matrix.\index{matrix!sparse} Let's \texttt{cast()} this tidy data to a sparse matrix, technically, a document-feature matrix object from the \textbf{quanteda} \citep{R-quanteda} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_scotus }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(case\_name, word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_dfm}\NormalTok{(case\_name, word, n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Document-feature matrix of: 9,642 documents, 167,879 features (99.5% sparse).
\end{verbatim}

Look at the sparsity of this matrix. It's high! Think of this sparsity as the sparsity of data that we will want to use to build a supervised machine learning model.

What if instead we use stemming as a preprocessing step here?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_scotus }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(case\_name, stem) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_dfm}\NormalTok{(case\_name, stem, n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Document-feature matrix of: 9,642 documents, 135,570 features (99.5% sparse).
\end{verbatim}

We reduced the number of word features by many thousands, although the sparsity did not change much. Why is it possibly helpful to reduce the number of features? Common sense says that reducing the number of word features in our data set so dramatically will improve the performance of any machine learning model we train with it, \emph{assuming that we haven't lost any important information by stemming}.

\index{preprocessing!challenges}There is a growing body of academic research demonstrating that stemming can be counterproductive for text modeling. For example, \citet{Schofield16} and related work explore how choices around stemming and other preprocessing steps don't help and can actually hurt performance when training topic models for text. From \citet{Schofield16} specifically,

\begin{quote}
Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability.
\end{quote}

Topic modeling is an example of unsupervised machine learning\index{machine learning!unsupervised} for text and is not the same as the predictive modeling approaches we'll be focusing on in this book, but the lesson remains that stemming may or may not be beneficial for any specific context. As we work through the rest of this chapter and learn more about stemming, consider what information we lose when we stem text in exchange for reducing the number of word features. Stemming can be helpful in some contexts, but typical stemming algorithms are somewhat aggressive and have been built to favor sensitivity (or recall, or the true positive rate) at the expense of specificity (or precision, or the true negative rate).

Most common stemming algorithms you are likely to encounter will successfully reduce words to stems (i.e., not leave extraneous word endings on the words) but at the expense of collapsing some words with dramatic differences in meaning, semantics, use, etc. to the same stems. Examples of the latter are numerous, but some include:

\begin{itemize}
\item
  meaning and mean
\item
  likely, like, liking
\item
  university and universe
\end{itemize}

In a supervised machine learning context, this affects a model's positive predictive value (precision), or ability to not incorrectly label true negatives as positive. In Chapter \ref{mlclassification}, we will train models to predict whether a complaint to the United States Consumer Financial Protection Bureau was about a mortgage or not. Stemming can increase a model's ability to find the positive examples, i.e., the complaints about mortgages. However, if the complaint text is over-stemmed, the resulting model loses its ability to label the negative examples, the complaints \emph{not} about mortgages, correctly.

\hypertarget{understand-a-stemming-algorithm}{%
\section{Understand a stemming algorithm}\label{understand-a-stemming-algorithm}}

If stemming is going to be in our NLP toolbox, it's worth sitting down with one approach in detail to understand how it works under the hood. \index{stemming algorithm!Porter}The Porter stemming algorithm is so approachable that we can walk through its outline in less than a page or so. It involves five steps, and the idea of a word \textbf{measure}.

Think of any word as made up alternating groups of vowels \(V\) and consonants \(C\). One or more vowels together are one instance of \(V\), and one or more consonants togther are one instance of \(C\). We can write any word as

\[[C](VC)^m[V]\]
where \(m\) is called the ``measure'' of the word. The first \(C\) and the last \(V\) in brackets are optional. In this framework, we could write out the word \texttt{"tree"} as

\[CV\]

with \(C\) being ``tr'' and \(V\) being ``ee''; it's an \texttt{m\ =\ 0} word. We would write out the word \texttt{"algorithms"} as

\[VCVCVC\]
and it is an \texttt{m\ =\ 3} word.

\begin{itemize}
\item
  The first step of the Porter stemmer is (perhaps this seems like cheating) actually made of three substeps working with plural and past participle word endings. In the first substep (1a), ``sses'' is replaced with ``ss'', ``ies'' is replaced with ``i'', and final single ``s'' letters are removed. The second substep (1b) depends on the measure of the word \texttt{m} but works with endings like ``eed'', ``ed'', ``ing'', adding ``e'' back on to make endings like ``ate'', ``ble'', and ``ize'' when appropriate. The third substep (1c) replaces ``y'' with ``i'' for words of a certain \texttt{m}.
\item
  The second step of the Porter stemmer takes the output of the first step and regularizes a set of 20 endings. In this step, ``ization'' goes to ``ize'', ``alism'' goes to ``al'', ``aliti'' goes to ``al'' (notice that the ending ``i'' there came from the first step), and so on for the other 17 endings.
\item
  The third step again processes the output, using a list of seven endings. Here, ``ical'' and ``iciti'' both go to ``ic'', ``ful'' and ``ness'' are both removed, and so forth for the three other endings in this step.
\item
  The fourth step involves a longer list of endings to deal with again (19), and they are all removed. Endings like ``ent'', ``ism'', ``ment'', and more are removed in this step.
\item
  The fifth and final step has two substeps, both which depend on the measure \texttt{m} of the word. In this step, depending on \texttt{m}, final ``e'' letters are sometimes removed and final double letters are sometimes removed.
\end{itemize}

\begin{rmdnote}
How would this work for a few example words? The word ``supervised''
loses its ``ed'' in step 1b and is not touched by the rest of the
algorithm, ending at ``supervis''. The word ``relational'' changes
``ational'' to ``ate'' in step 2 and loses its final ``e'' in step 5,
ending at ``relat''. Notice that neither of these results are regular
English words, but instead special stem objects. This is expected.
\end{rmdnote}

This algorithm was first published in \citet{Porter80} and is still broadly used; read \citet{Willett06} for background on how and why it has become a stemming standard. We can reach even \emph{further} back and examine what is considered the \index{stemming algorithm!Lovins}first ever published stemming algorithm in \citet{Lovins68}. The domain Lovins worked in was engineering, so her approach was particularly suited to technical terms. This algorithm uses much larger lists of word endings, conditions, and rules than the Porter algorithm and, although considered old-fashioned, is actually faster!

\begin{rmdwarning}
Check out the
\href{https://snowballstem.org/algorithms/german/stemmer.html}{steps of
a Snowball stemming algorithm for German}.
\end{rmdwarning}

\index{stemming algorithm!Snowball}

\hypertarget{handling-punctuation-when-stemming}{%
\section{Handling punctuation when stemming}\label{handling-punctuation-when-stemming}}

Punctuation contains information that can be used in text analysis. Punctuation \emph{is} typically less information-dense than the words themselves and thus it is often removed early in a text mining analysis project, but it's worth thinking through the impact of punctuation specifically on stemming. Think about words like \texttt{"they\textquotesingle{}re"} and \texttt{"child\textquotesingle{}s"}.

\index{preprocessing!challenges}We've already seen how punctuation and stemming can interact with our small example of ``The Fir-Tree''; none of the stemming strategies we've discussed so far have recognized \texttt{"tree\textquotesingle{}s"} as belonging to the same stem as \texttt{"trees"} and \texttt{"tree"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(word, }\StringTok{"\^{}tree"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 3 x 2
#>   word       n
#>   <chr>  <int>
#> 1 tree      76
#> 2 trees     12
#> 3 tree's     1
\end{verbatim}

It is possible to split tokens not only on white space but \textbf{also} on punctuation, using a regular expression (see Appendix \ref{regexp}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fir\_tree\_counts }\OtherTok{\textless{}{-}}\NormalTok{ fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text, }\AttributeTok{token =} \StringTok{"regex"}\NormalTok{, }\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s+|[[:punct:]]+"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(stem, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{fir\_tree\_counts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 572 x 2
#>    stem        n
#>    <chr>   <int>
#>  1 tree       89
#>  2 fir        34
#>  3 littl      23
#>  4 said       22
#>  5 stori      16
#>  6 thought    16
#>  7 branch     15
#>  8 on         15
#>  9 came       14
#> 10 know       14
#> # ... with 562 more rows
\end{verbatim}

Now we are able to put all these related words together, having identified them with the same stem.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fir\_tree\_counts }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(stem, }\StringTok{"\^{}tree"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 2
#>   stem      n
#>   <chr> <int>
#> 1 tree     89
\end{verbatim}

Handling punctuation in this way further reduces sparsity in word features. Whether this kind of tokenization and stemming strategy is a good choice in any particular data analysis situation depends on the particulars of the text characteristics.

\hypertarget{compare-some-stemming-options}{%
\section{Compare some stemming options}\label{compare-some-stemming-options}}

Let's compare a few simple stemming algorithms and see what results we end with. Let's look at ``The Fir-Tree'', specifically the tidied data set from which we have removed stop words. Let's compare three very straightforward stemming approaches.

\begin{itemize}
\item
  \textbf{Only remove final instances of the letter ``s''.} This probably strikes you as not a great idea after our discussion in this chapter, but it is something that people try in real life, so let's see what the impact is.\index{singular versus plural}
\item
  \textbf{Handle plural endings with slightly more complex rules in the ``S'' stemmer.} The S-removal stemmer or ``S'' stemmer of \citet{Harman91} is a simple algorithm with only three rules.\footnote{This simple, ``weak'' stemmer is handy to have in your toolkit for many applications. Notice how we implement it here using \texttt{dplyr::case\_when()}.}
\item
  \textbf{Implement actual Porter stemming.} We can now compare to the most commonly used stemming algorithm in English.\index{stemming algorithm!Porter}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stemming }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{book, }\SpecialCharTok{{-}}\NormalTok{language) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Remove S}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{str\_remove}\NormalTok{(word, }\StringTok{"s$"}\NormalTok{),}
         \StringTok{\textasciigrave{}}\AttributeTok{Plural endings}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{case\_when}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(word, }\StringTok{"[\^{}e|aies$]ies$"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}}
                                        \FunctionTok{str\_replace}\NormalTok{(word, }\StringTok{"ies$"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
                                      \FunctionTok{str\_detect}\NormalTok{(word, }\StringTok{"[\^{}e|a|oes$]es$"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}}
                                        \FunctionTok{str\_replace}\NormalTok{(word, }\StringTok{"es$"}\NormalTok{, }\StringTok{"e"}\NormalTok{),}
                                      \FunctionTok{str\_detect}\NormalTok{(word, }\StringTok{"[\^{}ss$|us$]s$"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}}
                                        \FunctionTok{str\_remove}\NormalTok{(word, }\StringTok{"s$"}\NormalTok{),}
                                      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ word),}
         \StringTok{\textasciigrave{}}\AttributeTok{Porter stemming}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{wordStem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Original word}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ word)}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:stemmingresults} shows the results of these stemming strategies. All successfully handled the transition from \texttt{"trees"} to \texttt{"tree"} in the same way, but we have different results for \texttt{"stories"} to \texttt{"story"} or \texttt{"stori"}, different handling of \texttt{"branches"}, and more. There are subtle differences in the output of even these straightforward stemming approaches that can effect the transformation of text features for modeling.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stemming }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gather}\NormalTok{(Type, Result, }\StringTok{\textasciigrave{}}\AttributeTok{Remove S}\StringTok{\textasciigrave{}}\SpecialCharTok{:}\StringTok{\textasciigrave{}}\AttributeTok{Porter stemming}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Type =} \FunctionTok{fct\_inorder}\NormalTok{(Type)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(Type, Result) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Type) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{20}\NormalTok{, n) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  ungroup }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\FunctionTok{fct\_reorder}\NormalTok{(Result, n),}
\NormalTok{             n, }\AttributeTok{fill =}\NormalTok{ Type)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Type, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \StringTok{"Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{04_stemming_files/figure-latex/stemmingresults-1} 

}

\caption{Results for three different stemming strategies}\label{fig:stemmingresults}
\end{figure}

\index{stemming algorithm!Porter}Porter stemming is the most different from the other two approaches. In the top twenty words here, we don't see a difference between removing only the letter ``s'' and taking the slightly more sophisticated ``S'' stemmer approach to plural endings. In what situations \emph{do} we see a difference?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stemming }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Remove S}\StringTok{\textasciigrave{}} \SpecialCharTok{!=} \StringTok{\textasciigrave{}}\AttributeTok{Plural endings}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Remove S}\StringTok{\textasciigrave{}}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Plural endings}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{.keep\_all =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 13 x 4
#>    `Original word` `Remove S`  `Plural endings` `Porter stemming`
#>    <chr>           <chr>       <chr>            <chr>            
#>  1 raspberries     raspberrie  raspberry        raspberri        
#>  2 strawberries    strawberrie strawberry       strawberri       
#>  3 less            les         less             less             
#>  4 us              u           us               u                
#>  5 brightness      brightnes   brightness       bright           
#>  6 conscious       consciou    conscious        consciou         
#>  7 faintness       faintnes    faintness        faint            
#>  8 happiness       happines    happiness        happi            
#>  9 ladies          ladie       lady             ladi             
#> 10 babies          babie       baby             babi             
#> 11 anxious         anxiou      anxious          anxiou           
#> 12 princess        princes     princess         princess         
#> 13 stories         storie      story            stori
\end{verbatim}

We also see situations where the same sets of original words are bucketed differently (not just with different stem labels) under different stemming strategies. In the following very small example, two of the strategies bucket these words into two stems while one strategy buckets them into one stem.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stemming }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gather}\NormalTok{(Type, Result, }\StringTok{\textasciigrave{}}\AttributeTok{Remove S}\StringTok{\textasciigrave{}}\SpecialCharTok{:}\StringTok{\textasciigrave{}}\AttributeTok{Porter stemming}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Result }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"come"}\NormalTok{, }\StringTok{"coming"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Original word}\StringTok{\textasciigrave{}}\NormalTok{, Type, Result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 9 x 3
#>   `Original word` Type            Result
#>   <chr>           <chr>           <chr> 
#> 1 come            Remove S        come  
#> 2 comes           Remove S        come  
#> 3 coming          Remove S        coming
#> 4 come            Plural endings  come  
#> 5 comes           Plural endings  come  
#> 6 coming          Plural endings  coming
#> 7 come            Porter stemming come  
#> 8 comes           Porter stemming come  
#> 9 coming          Porter stemming come
\end{verbatim}

\index{preprocessing!challenges}These different characteristics can either be positive or negative, depending on the nature of the text being modeled and the analytical question being pursued.

\begin{rmdwarning}
Language use is connected to culture and identity. How might the results
of stemming strategies be different for text created with the same
language (like English) but in different social or cultural contexts, or
by people with different identities? With what kind of text do you think
stemming algorithms behave most consistently, or most as expected? What
impact might that have on text modeling?
\end{rmdwarning}

\hypertarget{lemmatization}{%
\section{Lemmatization and stemming}\label{lemmatization}}

\index{lemmas}When people use the word ``stemming'' in natural language processing, they typically mean a system like the one we've been describing in this chapter, with rules, conditions, heuristics, and lists of word endings. Think of stemming as typically implemented in NLP as \textbf{rule-based}, operating on the word by itself. There is another option for normalizing words to a root that takes a different approach. Instead of using rules to cut words down to their stems, lemmatization\index{lemmatization|see {lemmas}} uses knowledge about a language's structure to reduce words down to their lemmas, the canonical or dictionary forms of words. Think of lemmatization as typically implemented in NLP as \textbf{linguistics-based}, operating on the word in its context.

Lemmatization requires more information than the rule-based stemmers we've discussed so far. We need to know what part of speech a word\index{part of speech} is to correctly identify its lemma \footnote{Part-of-speech information is also sometimes used directly in machine learning}, and we also need more information about what words mean in their contexts. Often lemmatizers use a rich lexical database like \href{https://wordnet.princeton.edu/}{WordNet} as a way to look up word meanings for a given part-of-speech use \citep{Miller95}. Notice that lemmatization involves more linguistic knowledge of a language than stemming.

\begin{rmdnote}
How does lemmatization work in languages other than English? Lookup
dictionaries connecting words, lemmas, and parts of speech for languages
other than English have been developed as well.
\end{rmdnote}

\index{language!Non-English}

A modern, efficient implementation for lemmatization is available in the excellent \href{https://spacy.io/}{spaCy} library \citep{spacy2}, which is written in Python.\index{Python}

\begin{rmdpackage}
NLP practitioners who work with R can use this library via the \textbf{spacyr} package \citep{Benoit19}, the \href{https://statsmaths.github.io/cleanNLP/}{\textbf{cleanNLP}} package \citep{Arnold17}, or as an ``engine'' in the \href{https://textrecipes.tidymodels.org/}{\textbf{textrecipes}} package \citep{textrecipes}.
\end{rmdpackage}

Section \ref{mlregressionlemmatization} demonstrates how to use textrecipes with spaCy as an engine and include lemmas as features for modeling. You might also consider using spaCy directly in R Markdown \href{https://rstudio.github.io/reticulate/articles/r_markdown.html}{via its Python engine}.\index{Python}

Let's briefly walk through how to use spacyr.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(spacyr)}
\FunctionTok{spacy\_initialize}\NormalTok{(}\AttributeTok{entity =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{fir\_tree }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"doc"}\NormalTok{, }\FunctionTok{row\_number}\NormalTok{())) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(doc\_id, }\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{spacy\_parse}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{(), }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"lemma"} \OtherTok{=} \StringTok{"word"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(lemma, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{20}\NormalTok{, n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n, }\FunctionTok{fct\_reorder}\NormalTok{(lemma, n))) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Frequency"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{04_stemming_files/figure-latex/lemmafirtree-1} 

}

\caption{Results for lemmatization, rather than stemming}\label{fig:lemmafirtree}
\end{figure}

Figure \ref{fig:lemmafirtree} demonstrates how different lemmatization\index{lemmas} is from stemming, especially is we compare to Figure \ref{fig:stemmingresults}. Punctuation characters are treated as tokens (these punctuation tokens can have predictive power for some modeling questions!) and all pronouns are lemmatized to \texttt{-PRON-}. We see our familiar friends ``tree'' and ``fir'', but notice that we see the normalized version ``say'' instead of ``said'', ``come'' instead of ``came'', and similar. This transformation to the canonical or dictionary form of words is the goal of lemmatization.

\begin{rmdnote}
Why did we need to initialize the spaCy library? You may not need to,
but spaCy is a full-featured NLP pipeline that not only tokenizes and
identifies lemmas but also performs entity recognition. We will not use
entity recognition in modeling or analysis in this book and it takes a
lot of computational power. Initializing with \texttt{entity\ =\ FALSE}
will allow lemmatization to run much faster.
\end{rmdnote}

Implementing lemmatization\index{lemmas} is slower and more complex than stemming. Just like with stemming, lemmatization often improves the true positive rate (or recall) but at the expense of the true negative rate (or precision)\index{precision} compared to not using lemmatization, but typically less so than stemming.

\hypertarget{stemming-and-stop-words}{%
\section{Stemming and stop words}\label{stemming-and-stop-words}}

Our deep dive into stemming came \emph{after} our chapters on tokenization (Chapter \ref{tokenization}) and stop words (Chapter \ref{stopwords}) because this is typically when you will want to implement stemming, if appropriate to your analytical question. Stop word lists are usually unstemmed, so you need to remove stop words before stemming text data. For example, the \index{stemming algorithm!Porter}Porter stemming algorithm transforms words like \texttt{"themselves"} to \texttt{"themselv"}, so stemming first would leave you without the ability to match up to the commonly used stop word lexicons.

A handy trick is to use the following function on your stop word list to return the words that don't have a stemmed version in the list. If the function returns a length 0 vector then you can stem and remove stop words in any order.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stopwords)}
\NormalTok{not\_stemmed\_in }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  x[}\SpecialCharTok{!}\NormalTok{SnowballC}\SpecialCharTok{::}\FunctionTok{wordStem}\NormalTok{(x) }\SpecialCharTok{\%in\%}\NormalTok{ x]}
\NormalTok{\}}

\FunctionTok{not\_stemmed\_in}\NormalTok{(}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =} \StringTok{"snowball"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "ourselves"  "yourselves" "his"        "they"       "themselves"
#>  [6] "this"       "are"        "was"        "has"        "does"      
#> [11] "you're"     "he's"       "she's"      "it's"       "we're"     
#> [16] "they're"    "i've"       "you've"     "we've"      "they've"   
#> [21] "let's"      "that's"     "who's"      "what's"     "here's"    
#> [26] "there's"    "when's"     "where's"    "why's"      "how's"     
#> [31] "because"    "during"     "before"     "above"      "once"      
#> [36] "any"        "only"       "very"
\end{verbatim}

Here we see that many of the words that are lost are the contractions.

\begin{rmdwarning}
In Section \ref{homemadestopwords}, we explored whether to include ``tree'' as a stop word for ``The Fir-Tree''. Now we can understand that this is more complicated than we first discussed, because there are different versions of the base word (``trees'', ``tree's'') in our data set. Interactions between preprocessing steps can have a major impact on your analysis.
\end{rmdwarning}

\hypertarget{stemmingsummary}{%
\section{Summary}\label{stemmingsummary}}

In this chapter, we explored stemming, the practice of identifying and extracting the base or stem for a word using rules and heuristics. Stemming reduces the sparsity of text data which can be helpful when training models, but at the cost of throwing information away. Lemmatization is another way to normalize words to a root, based on language structure and how words are used in their context.

\hypertarget{in-this-chapter-you-learned-3}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-3}}

\begin{itemize}
\item
  about the most broadly used stemming algorithms
\item
  how to implement stemming
\item
  that stemming changes the sparsity or feature space of text data
\item
  the differences between stemming and lemmatization
\end{itemize}

\hypertarget{embeddings}{%
\chapter{Word Embeddings}\label{embeddings}}

\begin{quote}
You shall know a word by the company it keeps.\\
\hfill --- \href{https://en.wikiquote.org/wiki/John_Rupert_Firth}{John Rupert Firth}
\end{quote}

So far in our discussion of natural language features, we have discussed \index{preprocessing}preprocessing steps such as tokenization, removing stop words, and stemming in detail. We implement these types of preprocessing steps to be able to represent our text data in some data structure that is a good fit for modeling.

\hypertarget{motivatingsparse}{%
\section{Motivating embeddings for sparse, high-dimensional data}\label{motivatingsparse}}

What kind of data structure might work well for typical text data? Perhaps, if we wanted to analyse or build a model for consumer complaints to the \href{https://www.consumerfinance.gov/data-research/consumer-complaints/}{United States Consumer Financial Protection Bureau (CFPB)}, described in Section \ref{cfpb-complaints}, we would start with straightforward word counts. Let's create a sparse matrix\index{matrix!sparse} where the matrix elements are the counts of words in each document.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidytext)}
\FunctionTok{library}\NormalTok{(SnowballC)}

\NormalTok{complaints }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/complaints.csv.gz"}\NormalTok{)}

\NormalTok{complaints }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{(), }\AttributeTok{by =} \StringTok{"word"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(complaint\_id, stem) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_dfm}\NormalTok{(complaint\_id, stem, n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Document-feature matrix of: 117,214 documents, 46,099 features (99.9% sparse).
\end{verbatim}

\index{matrix!sparse}

\begin{rmdwarning}
A \emph{sparse matrix} is a matrix where most of the elements are zero.
When working with text data, we say our data is ``sparse'' because most
documents do not contain most words, resulting in a representation of
mostly zeroes. There are special data structures and algorithms for
dealing with sparse data that can take advantage of their structure. For
example, an array can more efficiently store the locations and values of
only the non-zero elements instead of all elements.
\end{rmdwarning}

The data set of consumer complaints used in this book has been filtered to those submitted to the CFPB since 1 January 2019 that include a consumer complaint narrative (i.e., some submitted text).

Another way to represent our text data is to create a sparse matrix where the elements are weighted, rather than straightforward counts only. The \emph{term frequency}\index{term frequency} of a word is how frequently a word occurs in a document, and the \emph{inverse document frequency}\index{inverse document frequency} of a word decreases the weight for commonly used words and increases the weight for words that are not used often in a collection of documents. It is typically defined as:

\[idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}\]
\index{term frequency-inverse document frequency|see {tf-idf}}
These two quantities can be combined to calculate a term's \index{tf-idf}tf-idf (the two quantities multiplied together). This statistic measures the frequency of a term adjusted for how rarely it is used, and it is an example of a weighting scheme that can often work better than counts for predictive modeling with text features.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{(), }\AttributeTok{by =} \StringTok{"word"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(complaint\_id, stem) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_tf\_idf}\NormalTok{(stem, complaint\_id, n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_dfm}\NormalTok{(complaint\_id, stem, tf\_idf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Document-feature matrix of: 117,214 documents, 46,099 features (99.9% sparse).
\end{verbatim}

Notice that in either case, our final data structure is incredibly sparse and of high dimensionality with a huge number of features. Some modeling algorithms and the libraries which implement them can take advantage of the memory characteristics of sparse matrices for better performance; an example of this is regularized regression implemented in \textbf{glmnet} \citep{Friedman2010}. Some modeling algorithms, including tree-based algorithms, do not perform better with sparse input, and then some libraries are not built to take advantage of sparse data structures, even if it would improve performance for those algorithms. We have some computational tools to take advantage of sparsity, but they don't always solve all the problems that come along with big text data sets.

\index{matrix!sparse}
\index{corpus}
As the size of a corpus increases in terms of words or other tokens, both the sparsity and RAM required to hold the corpus in memory increase. Figure \ref{fig:sparsityram} shows how this works out; as the corpus grows, there are more words used just a few times included in the corpus. The sparsity increases and approaches 100\%, but even more notably, the memory required to store the corpus increases with the square of the number of tokens.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_dfm }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(frac) \{}
\NormalTok{  complaints }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{sample\_frac}\NormalTok{(frac) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unnest\_tokens}\NormalTok{(word, consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{get\_stopwords}\NormalTok{(), }\AttributeTok{by =} \StringTok{"word"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stem =} \FunctionTok{wordStem}\NormalTok{(word)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{count}\NormalTok{(complaint\_id, stem) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{cast\_dfm}\NormalTok{(complaint\_id, stem, n)}
\NormalTok{\}}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tibble}\NormalTok{(}\AttributeTok{frac =} \DecValTok{2} \SpecialCharTok{\^{}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{16}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{dfm =} \FunctionTok{map}\NormalTok{(frac, get\_dfm),}
         \AttributeTok{words =} \FunctionTok{map\_dbl}\NormalTok{(dfm, quanteda}\SpecialCharTok{::}\NormalTok{nfeat),}
         \AttributeTok{sparsity =} \FunctionTok{map\_dbl}\NormalTok{(dfm, quanteda}\SpecialCharTok{::}\NormalTok{sparsity),}
         \StringTok{\textasciigrave{}}\AttributeTok{RAM (in bytes)}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{(dfm, lobstr}\SpecialCharTok{::}\NormalTok{obj\_size)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(sparsity}\SpecialCharTok{:}\StringTok{\textasciigrave{}}\AttributeTok{RAM (in bytes)}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{names\_to =} \StringTok{"measure"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(words, value, }\AttributeTok{color =}\NormalTok{ measure)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{measure, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{label\_comma}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{label\_comma}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of unique words in corpus (log scale)"}\NormalTok{,}
       \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{05_word_embeddings_files/figure-latex/sparsityram-1} 

}

\caption{Memory requirements and sparsity increase with corpus size}\label{fig:sparsityram}
\end{figure}

Linguists have long worked on vector models for language that can reduce the number of dimensions representing text data based on how people use language; the quote that opened this chapter dates to 1957. These kinds of dense word vectors are often called \emph{word embeddings}.

\hypertarget{understand-word-embeddings-by-finding-them-yourself}{%
\section{Understand word embeddings by finding them yourself}\label{understand-word-embeddings-by-finding-them-yourself}}

Word embeddings are a way to represent text data as \index{vector}vectors of numbers based on a huge \index{corpus}corpus of text, capturing semantic meaning from words' context.

\begin{rmdnote}
Modern word embeddings are based on a statistical approach to modeling
language, rather than a linguistics or rules-based approach.
\end{rmdnote}

We can determine these vectors for a corpus of text using word counts and matrix factorization, as outlined by \citet{Moody2017}. This approach is valuable because it allows practitioners to find word vectors for their own collections of text (with no need to rely on pre-trained vectors) using familiar techniques that are not difficult to understand. Let's walk through how to do this using tidy data principles and sparse matrices\index{matrix!sparse}, on the data set of CFPB complaints. First, let's filter out words that are used only rarely in this data set and create a nested dataframe, with one row per complaint.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_complaints }\OtherTok{\textless{}{-}}\NormalTok{ complaints }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(complaint\_id, consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}=} \DecValTok{50}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n)}

\NormalTok{nested\_words }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_complaints }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest}\NormalTok{(}\AttributeTok{words =} \FunctionTok{c}\NormalTok{(word))}

\NormalTok{nested\_words}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 117,170 x 2
#>    complaint_id words                 
#>           <dbl> <list>                
#>  1      3384392 <tibble[,1] [18 x 1]> 
#>  2      3417821 <tibble[,1] [71 x 1]> 
#>  3      3433198 <tibble[,1] [77 x 1]> 
#>  4      3366475 <tibble[,1] [69 x 1]> 
#>  5      3385399 <tibble[,1] [213 x 1]>
#>  6      3444592 <tibble[,1] [19 x 1]> 
#>  7      3379924 <tibble[,1] [121 x 1]>
#>  8      3446975 <tibble[,1] [22 x 1]> 
#>  9      3214857 <tibble[,1] [64 x 1]> 
#> 10      3417374 <tibble[,1] [44 x 1]> 
#> # ... with 117,160 more rows
\end{verbatim}

Next, let's create a \texttt{slide\_windows()} function, using the \texttt{slide()} function from the \textbf{slider} package \citep{Vaughan2020} which implements fast sliding window computations written in C. Our new function identifies skipgram windows\index{skipgram windows} in order to calculate the skipgram probabilities, how often we find each word near each other word. We do this by defining a fixed-size moving window that centers around each word. Do we see \texttt{word1} and \texttt{word2} together within this window? We can calculate probabilities based on when we do or do not.

One of the arguments to this function is the \texttt{window\_size}, which determines the size of the sliding window that moves through the text, counting up words that we find within the window. The best choice for this window size depends on your analytical question because it determines what kind of semantic meaning the embeddings capture. A smaller window size, like three or four, focuses on how the word is used and learns what other words are functionally similar. A larger window size, like ten, captures more information about the domain or topic of each word, not constrained by how functionally similar the words are \citep{Levy2014}. A smaller window size is also faster to compute.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{slide\_windows }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(tbl, window\_size) \{}
\NormalTok{  skipgrams }\OtherTok{\textless{}{-}}\NormalTok{ slider}\SpecialCharTok{::}\FunctionTok{slide}\NormalTok{(}
\NormalTok{    tbl, }
    \SpecialCharTok{\textasciitilde{}}\NormalTok{.x, }
    \AttributeTok{.after =}\NormalTok{ window\_size }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }
    \AttributeTok{.step =} \DecValTok{1}\NormalTok{, }
    \AttributeTok{.complete =} \ConstantTok{TRUE}
\NormalTok{  )}
  
\NormalTok{  safe\_mutate }\OtherTok{\textless{}{-}} \FunctionTok{safely}\NormalTok{(mutate)}
  
\NormalTok{  out }\OtherTok{\textless{}{-}} \FunctionTok{map2}\NormalTok{(skipgrams,}
              \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(skipgrams),}
              \SpecialCharTok{\textasciitilde{}} \FunctionTok{safe\_mutate}\NormalTok{(.x, }\AttributeTok{window\_id =}\NormalTok{ .y))}

\NormalTok{  out }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{transpose}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pluck}\NormalTok{(}\StringTok{"result"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{compact}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now that we can find all the skipgram windows\index{skipgram windows}, we can calculate how often words occur on their own, and how often words occur together with other words. We do this using the point-wise mutual information (PMI)\index{PMI}\index{point-wise mutual information|see {PMI}}, a measure of association that measures exactly what we described in the previous sentence; it's the logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone. We use PMI to measure which words occur together more often than expected based on how often they occurred on their own.

For this example, let's use a window size of \emph{four}.

\begin{rmdpackage}
This next step is the computationally expensive part of finding word embeddings with this method, and can take a while to run. Fortunately, we can use the \textbf{furrr} package \citep{Vaughan2018} to take advantage of parallel processing because identifying skipgram windows in one document is independent from all the other documents.
\end{rmdpackage}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(widyr)}
\FunctionTok{library}\NormalTok{(furrr)}

\FunctionTok{plan}\NormalTok{(multisession)  }\DocumentationTok{\#\# for parallel processing}

\NormalTok{tidy\_pmi }\OtherTok{\textless{}{-}}\NormalTok{ nested\_words }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{words =} \FunctionTok{future\_map}\NormalTok{(words, slide\_windows, 4L)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{(window\_id, complaint\_id, window\_id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pairwise\_pmi}\NormalTok{(word, window\_id)}

\NormalTok{tidy\_pmi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4,818,402 x 3
#>    item1   item2           pmi
#>    <chr>   <chr>         <dbl>
#>  1 systems transworld  7.09   
#>  2 inc     transworld  5.96   
#>  3 is      transworld -0.135  
#>  4 trying  transworld -0.107  
#>  5 to      transworld -0.00206
#>  6 collect transworld  1.07   
#>  7 a       transworld -0.516  
#>  8 debt    transworld  0.919  
#>  9 that    transworld -0.542  
#> 10 not     transworld -1.17   
#> # ... with 4,818,392 more rows
\end{verbatim}

When PMI is high, the two words are associated with each other, likely to occur together. When PMI is low, the two words are not associated with each other, unlikely to occur together.

\begin{rmdwarning}
The step above used \texttt{unite()}, a function from \textbf{tidyr}
that pastes multiple columns into one, to make a new column for
\texttt{window\_id} from the old \texttt{window\_id} plus the
\texttt{complaint\_id}. This new column tells us which combination of
window and complaint each word belongs to.
\end{rmdwarning}

We can next determine the word vectors from the PMI values using singular value decomposition (SVD).
SVD\index{SVD}\index{singular value decomposition|see {SVD}} is a method for dimensionality reduction via \index{matrix factorization}matrix factorization \citep{Golub1970} which works by taking our data and decomposing it onto special orthogonal axes. The first axis is chosen to capture as much of the variance as possible. Keeping that first axis fixed, the remaining orthogonal axes are rotated to maximize the variance in the second. This is repeated for all the remaining axes.

In our application, we will use SVD to factor the PMI matrix into a set of smaller matrices containing the word embeddings with a size we get to choose. The embedding size is typically chosen to be in the low hundreds. Thus we get a matrix of dimension (\texttt{n\_vocabulary\ *\ n\_dim}) instead of dimension (\texttt{n\_vocabulary\ *\ n\_vocabulary}), which can be a vast reduction in size for large vocabularies.
Let's use the \texttt{widely\_svd()} function in \textbf{widyr} \citep{R-widyr}, creating 100-dimensional word embeddings. This matrix factorization is much faster than the previous step of identifying the skipgram windows\index{skipgram windows} and calculating PMI.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_word\_vectors }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_pmi }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{widely\_svd}\NormalTok{(}
\NormalTok{    item1, item2, pmi,}
    \AttributeTok{nv =} \DecValTok{100}\NormalTok{, }\AttributeTok{maxit =} \DecValTok{1000}
\NormalTok{  )}

\NormalTok{tidy\_word\_vectors}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 747,500 x 3
#>    item1   dimension   value
#>    <chr>       <int>   <dbl>
#>  1 systems         1 0.0165 
#>  2 inc             1 0.0191 
#>  3 is              1 0.0202 
#>  4 trying          1 0.0423 
#>  5 to              1 0.00904
#>  6 collect         1 0.0370 
#>  7 a               1 0.0126 
#>  8 debt            1 0.0430 
#>  9 that            1 0.0136 
#> 10 not             1 0.0213 
#> # ... with 747,490 more rows
\end{verbatim}

\begin{rmdnote}
\texttt{tidy\_word\_vectors} is not drastically smaller than
\texttt{tidy\_pmi} since the vocabulary is not enormous and
\texttt{tidy\_pmi} is represented in a sparse format.
\end{rmdnote}

We have now successfully found word embeddings, with clear and understandable code. This is a real benefit of this approach; this approach is based on counting, dividing, and matrix decomposition and is thus easier to understand and implement than options based on \index{deep learning}deep learning. Training word vectors or embeddings, even with this straightforward method, still requires a large data set (ideally, hundreds of thousands of documents or more) and a not insignificant investment of time and computational power.

\hypertarget{exploring-cfpb-word-embeddings}{%
\section{Exploring CFPB word embeddings}\label{exploring-cfpb-word-embeddings}}

Now that we have determined word embeddings for the data set of CFPB complaints, let's explore them and talk about they are used in modeling. We have projected the sparse, high-dimensional set of word features into a more dense, 100-dimensional set of features.

\begin{rmdwarn}
Each word can be represented as a numeric vector in this new feature
space. A single word is mapped to only one vector, so be aware that all
senses of a word are conflated in word embeddings. Because of this, word
embeddings are limited for understanding lexical semantics.
\end{rmdwarn}

Which words are close to each other in this new feature space of word embeddings? Let's create a simple function that will find the nearest words to any given example in using our newly created word embeddings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nearest\_neighbors }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, token) \{}
\NormalTok{  df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{widely}\NormalTok{(}
      \SpecialCharTok{\textasciitilde{}}\NormalTok{ \{}
\NormalTok{        y }\OtherTok{\textless{}{-}}\NormalTok{ .[}\FunctionTok{rep}\NormalTok{(token, }\FunctionTok{nrow}\NormalTok{(.)), ]}
\NormalTok{        res }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(. }\SpecialCharTok{*}\NormalTok{ y) }\SpecialCharTok{/} 
\NormalTok{          (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{rowSums}\NormalTok{(. }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(.[token, ] }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)))}

        \FunctionTok{matrix}\NormalTok{(res, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{, }\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}\AttributeTok{x =} \FunctionTok{names}\NormalTok{(res)))}
\NormalTok{        \},}
      \AttributeTok{sort =} \ConstantTok{TRUE}
\NormalTok{    )(item1, dimension, value) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{item2)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This function takes the tidy word embeddings as input, along with a word (or token, more strictly) as a string. It uses matrix multiplication and sums to calculate the cosine similarity between the word and all the words in the embedding to find which words are closer or farther to the input word, and returns a dataframe sorted by similarity.

What words are closest to \texttt{"error"} in the data set of CFPB complaints, as determined by our word embeddings?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_word\_vectors }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nearest\_neighbors}\NormalTok{(}\StringTok{"error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 7,475 x 2
#>    item1            value
#>    <chr>            <dbl>
#>  1 error            1    
#>  2 mistake          0.683
#>  3 clerical         0.627
#>  4 problem          0.582
#>  5 glitch           0.580
#>  6 errors           0.571
#>  7 miscommunication 0.512
#>  8 misunderstanding 0.486
#>  9 issue            0.478
#> 10 discrepancy      0.474
#> # ... with 7,465 more rows
\end{verbatim}

Mistakes, problems, glitches -- sounds bad!

What is closest to the word \texttt{"month"}?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_word\_vectors }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nearest\_neighbors}\NormalTok{(}\StringTok{"month"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 7,475 x 2
#>    item1        value
#>    <chr>        <dbl>
#>  1 month        1    
#>  2 year         0.607
#>  3 months       0.593
#>  4 monthly      0.454
#>  5 installments 0.446
#>  6 payment      0.429
#>  7 week         0.406
#>  8 weeks        0.400
#>  9 85.00        0.399
#> 10 bill         0.396
#> # ... with 7,465 more rows
\end{verbatim}

We see words about installments and payments, along with other time periods such as years and weeks. Notice that we did not stem this text data (see Chapter \ref{stemming}) but the word embeddings learned that ``month'', ``months'', and ``monthly'' belong together.

What words are closest in this embedding space to \texttt{"fee"}?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_word\_vectors }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nearest\_neighbors}\NormalTok{(}\StringTok{"fee"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 7,475 x 2
#>    item1     value
#>    <chr>     <dbl>
#>  1 fee       1    
#>  2 fees      0.746
#>  3 overdraft 0.678
#>  4 12.00     0.675
#>  5 14.00     0.645
#>  6 37.00     0.632
#>  7 charge    0.630
#>  8 11.00     0.630
#>  9 36.00     0.627
#> 10 28.00     0.624
#> # ... with 7,465 more rows
\end{verbatim}

We find a lot of dollar amounts, which makes sense. Let us filter out the numbers to see what non-dollar words are similar to ``fee''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_word\_vectors }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nearest\_neighbors}\NormalTok{(}\StringTok{"fee"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(item1, }\StringTok{"[0{-}9]*.[0{-}9]\{2\}"}\NormalTok{, }\AttributeTok{negate =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 7,047 x 2
#>    item1     value
#>    <chr>     <dbl>
#>  1 fee       1    
#>  2 fees      0.746
#>  3 overdraft 0.678
#>  4 charge    0.630
#>  5 nsf       0.609
#>  6 charged   0.594
#>  7 od        0.552
#>  8 waived    0.547
#>  9 assessed  0.538
#> 10 charges   0.530
#> # ... with 7,037 more rows
\end{verbatim}

We now find words about overdrafts and charges. The top two words are ``fee'' and ``fees''; word embeddings can learn that singular and plural\index{singular versus plural} forms of words are related and belong together. In fact, word embeddings can accomplish many of the same goals of tasks like stemming (Chapter \ref{stemming}) but more reliably and less arbitrarily.

Since we have found word embeddings via singular value decomposition, we can use these vectors to understand what principal components explain the most variation in the CFPB complaints. The orthogonal axes that SVD\index{SVD} used to represent our data were chosen so that the first axis accounts for the most variance, the second axis accounts for the next most variance, and so on. We can now explore which and how much each \emph{original} dimension (tokens in this case) contributed to each of the resulting principal components produced using SVD.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_word\_vectors }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(dimension }\SpecialCharTok{\textless{}=} \DecValTok{24}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(dimension) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{top\_n}\NormalTok{(}\DecValTok{12}\NormalTok{, }\FunctionTok{abs}\NormalTok{(value)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    ungroup }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{item1 =} \FunctionTok{reorder\_within}\NormalTok{(item1, value, dimension)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(item1, value, }\AttributeTok{fill =}\NormalTok{ dimension)) }\SpecialCharTok{+}
    \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{dimension, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_reordered}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}
      \AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
      \AttributeTok{y =} \StringTok{"Value"}\NormalTok{,}
      \AttributeTok{title =} \StringTok{"First 24 principal components for text of CFPB complaints"}\NormalTok{,}
      \AttributeTok{subtitle =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Top words contributing to the components that explain"}\NormalTok{,}
                       \StringTok{"the most variation"}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{05_word_embeddings_files/figure-latex/embeddingpca-1} 

}

\caption{Word embeddings for Consumer Finance Protection Bureau complaints}\label{fig:embeddingpca}
\end{figure}

It becomes very clear in Figure \ref{fig:embeddingpca} that stop words have not been removed, but notice that we can learn meaningful relationships in how very common words are used. Component 12 shows us how common prepositions are often used with words like \texttt{"regarding"}, \texttt{"contacted"}, and \texttt{"called"}, while component 9 highlights the use of \emph{different} common words when submitting a complaint about unethical, predatory, and/or deceptive practices. Stop words do carry information, and methods like determining word embeddings can make that information usable.

We created word embeddings\index{embeddings} and can explore them to understand our text data set, but how do we use this vector representation in modeling? The classic and simplest approach is to treat each document as a collection of words and summarize the word embeddings into \emph{document embeddings}, either using a mean or sum. This approach loses information about word order but is straightforward to implement. Let's \texttt{count()} to find the sum here in our example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_complaints }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(complaint\_id, word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_sparse}\NormalTok{(complaint\_id, word, n)}

\NormalTok{embedding\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_word\_vectors }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_sparse}\NormalTok{(item1, dimension, value)}

\NormalTok{doc\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ word\_matrix }\SpecialCharTok{\%*\%}\NormalTok{ embedding\_matrix}

\FunctionTok{dim}\NormalTok{(doc\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 117170    100
\end{verbatim}

We have a new matrix here that we can use as the input for modeling. Notice that we still have over 100,000 documents (we did lose a few complaints, compared to our example sparse matrices\index{matrix!sparse} at the beginning of the chapter, when we filtered out rarely used words) but instead of tens of thousands of features, we have exactly 100 features.

\begin{rmdnote}
These hundred features are the word embeddings we learned from the text
data itself.
\end{rmdnote}

If our word embeddings are of high quality, this translation of the high-dimensional space of words to the lower-dimensional space of the word embeddings allows our modeling based on such an input matrix to take advantage of the semantic meaning\index{semantics} captured in the embeddings.

This is a straightforward method for finding and using word embeddings, based on counting and linear algebra. It is valuable both for understanding what word embeddings are and how they work, but also in many real-world applications. This is not the method to reach for if you want to publish an academic NLP paper, but is excellent for many applied purposes. Other methods for determining word embeddings include GloVe \citep{Pennington2014}, implemented in R in the \textbf{text2vec} package \citep{Selivanov2018}, word2vec \citep{Mikolov2013}, and FastText \citep{Bojanowski2016}.
\index{embeddings!GloVe}
\index{embeddings!word2vec}
\index{embeddings!FastText}

\hypertarget{glove}{%
\section{Use pre-trained word embeddings}\label{glove}}

\index{embeddings!pre-trained}If your data set is too small, you typically cannot train reliable word embeddings.

\begin{rmdwarning}
How small is too small? It is hard to make definitive statements because
being able to determine useful word embeddings depends on the semantic
and pragmatic details of \emph{how} words are used in any given data
set. However, it may be unreasonable to expect good results with data
sets smaller than about a million words or tokens. (Here, we do not mean
about a million unique tokens, i.e.~the vocabulary size, but instead
about that many observations in the text data.)
\end{rmdwarning}

In such situations, we can still use word embeddings for feature creation in modeling, just not embeddings that we determine ourselves from our own data set. Instead, we can turn to \emph{pre-trained} word embeddings, such as the GloVe\index{embeddings!GloVe} word vectors trained on six billion tokens from Wikipedia and news sources. Several pre-trained GloVe vector representations are available in R via the \textbf{textdata} package \citep{Hvitfeldt2020}. Let's use \texttt{dimensions\ =\ 100}, since we trained 100-dimensional word embeddings in the previous section.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textdata)}

\NormalTok{glove6b }\OtherTok{\textless{}{-}} \FunctionTok{embedding\_glove6b}\NormalTok{(}\AttributeTok{dimensions =} \DecValTok{100}\NormalTok{)}
\NormalTok{glove6b}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 400,000 x 101
#>    token      d1      d2      d3      d4      d5      d6      d7      d8      d9
#>    <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>
#>  1 "the" -0.0382 -0.245   0.728  -0.400   0.0832  0.0440 -0.391   0.334  -0.575 
#>  2 ","   -0.108   0.111   0.598  -0.544   0.674   0.107   0.0389  0.355   0.0635
#>  3 "."   -0.340   0.209   0.463  -0.648  -0.384   0.0380  0.171   0.160   0.466 
#>  4 "of"  -0.153  -0.243   0.898   0.170   0.535   0.488  -0.588  -0.180  -1.36  
#>  5 "to"  -0.190   0.0500  0.191  -0.0492 -0.0897  0.210  -0.550   0.0984 -0.201 
#>  6 "and" -0.0720  0.231   0.0237 -0.506   0.339   0.196  -0.329   0.184  -0.181 
#>  7 "in"   0.0857 -0.222   0.166   0.134   0.382   0.354   0.0129  0.225  -0.438 
#>  8 "a"   -0.271   0.0440 -0.0203 -0.174   0.644   0.712   0.355   0.471  -0.296 
#>  9 "\""  -0.305  -0.236   0.176  -0.729  -0.283  -0.256   0.266   0.0253 -0.0748
#> 10 "'s"   0.589  -0.202   0.735  -0.683  -0.197  -0.180  -0.392   0.342  -0.606 
#> # ... with 399,990 more rows, and 91 more variables: d10 <dbl>, d11 <dbl>,
#> #   d12 <dbl>, d13 <dbl>, d14 <dbl>, d15 <dbl>, d16 <dbl>, d17 <dbl>,
#> #   d18 <dbl>, ...
\end{verbatim}

We can transform these word embeddings into a more tidy format, using \texttt{pivot\_longer()} from \textbf{tidyr}. Let's also give this tidied version the same column names as \texttt{tidy\_word\_vectors}, for convenience.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_glove }\OtherTok{\textless{}{-}}\NormalTok{ glove6b }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{contains}\NormalTok{(}\StringTok{"d"}\NormalTok{),}
               \AttributeTok{names\_to =} \StringTok{"dimension"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{item1 =}\NormalTok{ token)}

\NormalTok{tidy\_glove}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 40,000,000 x 3
#>    item1 dimension   value
#>    <chr> <chr>       <dbl>
#>  1 the   d1        -0.0382
#>  2 the   d2        -0.245 
#>  3 the   d3         0.728 
#>  4 the   d4        -0.400 
#>  5 the   d5         0.0832
#>  6 the   d6         0.0440
#>  7 the   d7        -0.391 
#>  8 the   d8         0.334 
#>  9 the   d9        -0.575 
#> 10 the   d10        0.0875
#> # ... with 39,999,990 more rows
\end{verbatim}

We've already explored some sets of ``synonyms'' in the embedding space we determined ourselves from the CPFB complaints. What about this embedding space learned via the GloVe algorithm\index{embeddings!GloVe} on a much larger data set? We just need to make one change to our \texttt{nearest\_neighbors()} function and add \texttt{maximum\_size\ =\ NULL}, because the matrices we are multiplying together are much larger this time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nearest\_neighbors }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, token) \{}
\NormalTok{  df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{widely}\NormalTok{(}
      \SpecialCharTok{\textasciitilde{}}\NormalTok{ \{}
\NormalTok{        y }\OtherTok{\textless{}{-}}\NormalTok{ .[}\FunctionTok{rep}\NormalTok{(token, }\FunctionTok{nrow}\NormalTok{(.)), ]}
\NormalTok{        res }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(. }\SpecialCharTok{*}\NormalTok{ y) }\SpecialCharTok{/} 
\NormalTok{          (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{rowSums}\NormalTok{(. }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(.[token, ] }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)))}

        \FunctionTok{matrix}\NormalTok{(res, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{, }\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}\AttributeTok{x =} \FunctionTok{names}\NormalTok{(res)))}
\NormalTok{        \},}
      \AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{,}
      \AttributeTok{maximum\_size =} \ConstantTok{NULL}
\NormalTok{    )(item1, dimension, value) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{item2)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\index{embeddings!pre-trained}Pre-trained word embeddings are trained on very large, general purpose English language data sets. Commonly used \href{https://code.google.com/archive/p/word2vec/}{word2vec embeddings} \index{embeddings!word2vec}are based on the Google News data set, and commonly used\index{embeddings!GloVe} \href{https://nlp.stanford.edu/projects/glove/}{GloVe embeddings} (what we are using here) and\index{embeddings!FastText} \href{https://fasttext.cc/docs/en/english-vectors.html}{FastText embeddings} are learned from the text of Wikipedia plus other sources. Keeping that in mind, what words are closest to \texttt{"error"} in the GloVe embeddings?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_glove }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nearest\_neighbors}\NormalTok{(}\StringTok{"error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 400,000 x 2
#>    item1       value
#>    <chr>       <dbl>
#>  1 error       1.   
#>  2 errors      0.792
#>  3 mistake     0.664
#>  4 correct     0.621
#>  5 incorrect   0.613
#>  6 fault       0.607
#>  7 difference  0.594
#>  8 mistakes    0.586
#>  9 calculation 0.584
#> 10 probability 0.583
#> # ... with 399,990 more rows
\end{verbatim}

Instead of problems and mistakes like in the CFPB embeddings, we now see words related to sports, especially baseball, where an error is a certain kind of act recorded in statistics. This could present a challenge for using the GloVe embeddings with the CFPB text data. Remember that different senses or uses of the same word are conflated in word embeddings; the high dimensional space of any set of word embeddings cannot distinguish between different uses of a word, such as the word ``error''.

What is closest to the word \texttt{"month"} in these pre-trained \index{embeddings!GloVe}GloVe embeddings?\index{embeddings!pre-trained}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_glove }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nearest\_neighbors}\NormalTok{(}\StringTok{"month"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 400,000 x 2
#>    item1    value
#>    <chr>    <dbl>
#>  1 month    1    
#>  2 week     0.939
#>  3 last     0.924
#>  4 months   0.898
#>  5 year     0.893
#>  6 weeks    0.865
#>  7 earlier  0.859
#>  8 tuesday  0.846
#>  9 ago      0.844
#> 10 thursday 0.841
#> # ... with 399,990 more rows
\end{verbatim}

Instead of words about payments, the GloVe results here focus on different time periods only.

What words are closest in the \index{embeddings!GloVe}GloVe embedding space to \texttt{"fee"}?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_glove }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{nearest\_neighbors}\NormalTok{(}\StringTok{"fee"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 400,000 x 2
#>    item1        value
#>    <chr>        <dbl>
#>  1 fee          1.   
#>  2 fees         0.832
#>  3 payment      0.741
#>  4 pay          0.711
#>  5 salary       0.700
#>  6 paid         0.668
#>  7 payments     0.653
#>  8 subscription 0.647
#>  9 paying       0.623
#> 10 expenses     0.619
#> # ... with 399,990 more rows
\end{verbatim}

The most similar words are, like with the CPFB embeddings, generally financial, but they are largely about salary and pay instead of about charges and overdrafts.

\index{semantics}\index{preprocessing!challenges}

\begin{rmdnote}
These examples highlight how pre-trained word embeddings can be useful
because of the incredibly rich semantic relationships they encode, but
also how these vector representations are often less than ideal for
specific tasks.
\end{rmdnote}

If we do choose to use pre-trained word embeddings, how do we go about integrating them into a modeling workflow? Again, we can create simple document embeddings by treating each document as a collection of words and summarizing the word embeddings. The GloVe embeddings\index{embeddings!GloVe} do not contain all the tokens in the CPFB complaints, and vice versa, so let's use \texttt{inner\_join()} to match up our data sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_complaints }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\AttributeTok{by =} \StringTok{"word"}\NormalTok{,}
\NormalTok{             tidy\_glove }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{distinct}\NormalTok{(item1) }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{rename}\NormalTok{(}\AttributeTok{word =}\NormalTok{ item1)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(complaint\_id, word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_sparse}\NormalTok{(complaint\_id, word, n)}

\NormalTok{glove\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_glove }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\AttributeTok{by =} \StringTok{"item1"}\NormalTok{,}
\NormalTok{             tidy\_complaints }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{distinct}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{rename}\NormalTok{(}\AttributeTok{item1 =}\NormalTok{ word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_sparse}\NormalTok{(item1, dimension, value)}

\NormalTok{doc\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ word\_matrix }\SpecialCharTok{\%*\%}\NormalTok{ glove\_matrix}

\FunctionTok{dim}\NormalTok{(doc\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 117163    100
\end{verbatim}

Since these GloVe embeddings\index{embeddings!GloVe} had the same number of dimensions as the word embeddings we found ourselves (100), we end up with the same number of columns as before but with slightly fewer documents in the data set. We have lost documents which contain only words not included in the GloVe embeddings.

\begin{rmdpackage}
The package \textbf{wordsalad} \citep{R-wordsalad} provides a unified interface for finding different kinds of word vectors from text using pre-trained embeddings. The options include fastText, GloVe, and word2vec.
\end{rmdpackage}

\index{embeddings!word2vec}
\index{embeddings!FastText}

\hypertarget{fairnessembeddings}{%
\section{Fairness and word embeddings}\label{fairnessembeddings}}

Perhaps more than any of the other preprocessing steps this book has covered so far, using word embeddings opens an analysis or model up to the possibility of being influenced by systemic unfairness and bias.\index{bias}

\index{corpus}

\begin{rmdwarning}
Embeddings are trained or learned from a large corpus of text data, and
whatever human prejudice or bias exists in the corpus becomes imprinted
into the vector data of the embeddings.
\end{rmdwarning}

This is true of all machine learning to some extent (models learn, reproduce, and often amplify whatever biases exist in training data) but this is literally, concretely true of word embeddings. \citet{Caliskan2016} show how the GloVe word embeddings (the same embeddings we used in Section \ref{glove}) replicate human-like semantic biases.

\begin{itemize}
\item
  Typically Black first names are associated with more unpleasant feelings than typically white first names.
\item
  Women's first names are more associated with family and men's first names are more associated with career.
\item
  Terms associated with women are more associated with the arts and terms associated with men are more associated with science.
\end{itemize}

\index{preprocessing!challenges}Results like these have been confirmed over and over again, such as when \citet{Bolukbasi2016} demonstrated gender stereotypes in how word embeddings encode professions or when Google Translate \href{https://twitter.com/seyyedreza/status/935291317252493312}{exhibited apparently sexist behavior when translating text from languages with no gendered pronouns}. Google has since \href{https://www.blog.google/products/translate/reducing-gender-bias-google-translate/}{worked to correct this problem} but in 2021 the problem \href{https://twitter.com/doravargha/status/1373211762108076034}{still exists for some languages}. \citet{Garg2018} even used the way bias and stereotypes can be found in word embeddings to quantify how social attitudes towards women and minorities have changed over time.

Remember that word embeddings are \emph{learned} or trained from some large data set of text; this training data is the source of the biases we observe when applying word embeddings to NLP tasks. \citet{Bender2021} outline how the very large data sets used in large language models do not mean that such models reflect representative or diverse viewpoints, or even can respond to changing social views. As one concrete example, a common data set used to train large embedding models is the text of Wikipedia, but Wikipedia \href{(https://en.wikipedia.org/wiki/Gender_bias_on_Wikipedia)}{itself has problems with, for example, gender bias}. Some of the gender discrepancies on Wikipedia can be attributed to social and historical factors, but some can be attributed to the site mechanics of Wikipedia itself \citep{Wagner2016}.

\begin{rmdnote}
It's safe to assume that any large corpus of language will contain
latent structure reflecting the biases of the people who generated that
language.
\end{rmdnote}

\index{corpus}
\index{bias}

When embeddings with these kinds of stereotypes are used as a preprocessing step in training a predictive model, the final model can exhibit racist, sexist, or otherwise biased characteristics. \citet{Speer2017} demonstrated how using pre-trained word embeddings\index{embeddings, pre-trained} to train a straightforward sentiment analysis model can result in text such as

\begin{quote}
``Let's go get Italian food''
\end{quote}

being scored much more positively than text such as\index{preprocessing!challenges}\index{bias}

\begin{quote}
``Let's go get Mexican food''
\end{quote}

because of characteristics of the text the word embeddings were trained on.

\hypertarget{using-word-embeddings-in-the-real-world}{%
\section{Using word embeddings in the real world}\label{using-word-embeddings-in-the-real-world}}

Given these profound and fundamental challenges with word embeddings, what options are out there? First, consider not using word embeddings when building a text model. Depending on the particular analytical question you are trying to answer, another numerical representation of text data (such as word frequencies or tf-idf\index{tf-idf} of single words or n-grams) may be more appropriate. Consider this option even more seriously if the model you want to train is already entangled with issues of bias, such as the sentiment analysis example in Section \ref{fairnessembeddings}.

Consider whether finding your own word embeddings, instead of relying on pre-trained embeddings\index{embeddings!pre-trained} created using an algorithm such as GloVe\index{embeddings!GloVe} or word2vec\index{embeddings!word2vec}, may help you. Building your own vectors is likely to be a good option when the text domain you are working in is \emph{specific} rather than general purpose; some examples of such domains could include customer feedback for a clothing e-commerce site, comments posted on a coding Q\&A site, or legal documents.

Learning good quality word embeddings is only realistic when you have a large corpus of text data (say, a million tokens) but if you have that much data, it is possible that embeddings learned from scratch based on your own data may not exhibit the same kind of semantic biases that exist in pre-trained word embeddings. Almost certainly there will be some kind of bias latent in any large text corpus, but when you use your own training data for learning word embeddings, you avoid the problem of \emph{adding} historic, systemic prejudice from general purpose language data sets.

\begin{rmdnote}
You can use the same approaches discussed in this chapter to check any
new embeddings for dangerous biases such as racism or sexism.
\end{rmdnote}

\index{bias}

NLP researchers have also proposed methods for debiasing embeddings. \citet{Bolukbasi2016} aim to remove stereotypes by postprocessing\index{postprocessing} pre-trained word vectors, choosing specific sets of words that are reprojected in the vector space so that some specific bias, such as gender bias, is mitigated. This is the most established method for reducing bias in embeddings to date, although other methods have been proposed as well, such as augmenting data with counterfactuals \citep{Lu2018}. Recent work \citep{Ethayarajh2019} has explored whether the association tests used to measure bias are even useful, and under what conditions debiasing can be effective.\index{preprocessing!challenges}

Other researchers, such as \citet{Caliskan2016}, suggest that corrections for fairness should happen at the point of \emph{decision} or action rather than earlier in the process of modeling, such as preprocessing steps like building word embeddings. The concern is that methods for debiasing word embeddings may allow the stereotypes to seep back in, and more recent work shows that this is exactly what can happen. \citet{Gonen2019} highlight how pervasive and consistent gender bias is across different word embedding models, \emph{even after} applying current debiasing methods.

\hypertarget{embeddingssummary}{%
\section{Summary}\label{embeddingssummary}}

Mapping words (or other tokens) to an embedding in a special vector space is a powerful approach in natural language processing. This chapter started from fundamentals to demonstrate how to determine word embeddings from a text data set, but a whole host of highly sophisticated techniques have been built on this foundation. For example, document embeddings can be learned from text directly \citep{Le2014} rather than summarized from word embeddings. More recently, embeddings have acted as one part of language models with transformers like ULMFiT \citep{Howard2018} and ELMo \citep{Peters2018}. It's important to keep in mind that even more advanced natural language algorithms, such as these language models with transformers, also exhibit such systemic biases \citep{Sheng2019}.

\hypertarget{in-this-chapter-you-learned-4}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-4}}

\begin{itemize}
\item
  what a word embedding is and why we use them
\item
  how to determine word embeddings from a text data set
\item
  how the vector space of word embeddings encodes word similarity
\item
  about a simple strategy to find document similarity
\item
  how to handle pre-trained word embeddings
\item
  why word embeddings carry historic and systemic bias
\item
  about approaches for debiasing word embeddings
\end{itemize}

\hypertarget{part-machine-learning-methods}{%
\part{Machine Learning Methods}\label{part-machine-learning-methods}}

\hypertarget{mlforeword}{%
\chapter*{Foreword}\label{mlforeword}}


\thispagestyle{myheadings}

It's time to use what we have discussed and learned in the first five chapters of this book in a supervised machine learning context, to make predictions from text data. In the next two chapters, we will focus putting into practice such machine learning algorithms as:

\begin{itemize}
\item
  naive Bayes,
\item
  support vector machines (SVM) \citep{Boser1992}, and
\item
  regularized linear models such as implemented in \href{https://glmnet.stanford.edu/}{glmnet} \citep{Friedman2010}.
\end{itemize}

We start in Chapter \ref{mlregression} with exploring regression models and continue in Chapter \ref{mlclassification} with classification models. These are different types of prediction problems, but in both, we can use the tools of supervised machine learning to connect our \emph{input}, which may exist entirely or partly as text data, with our \emph{outcome} of interest. Most supervised models for text data are built with one of three purposes in mind:

\begin{itemize}
\item
  The main goal of a \textbf{predictive model} is to generate the most accurate predictions possible.
\item
  An \textbf{inferential model} is created to test a hypothesis or draw conclusions about a population.
\item
  The main purpose of a \textbf{descriptive model} is to describe the properties of the observed data.
\end{itemize}

Many learning algorithms can be used for multiple of these purposes. Concerns about a model's predictive capacity may be as important for an inferential or descriptive model as for a model designed purely for prediction, and model interpretability and explainability may be important for a solely predictive or descriptive model as well as for an inferential model. We will use the \href{https://www.tidymodels.org/}{tidymodels} framework to address all of these issues, with its consistent approach to resampling, preprocessing, fitting, and evaluation.

\begin{rmdpackage}
The \textbf{tidymodels} framework \citep{R-tidymodels} is a collection of R packages for modeling and machine learning using tidyverse principles \citep{Wickham2019}. These packages facilitate resampling, preprocessing, modeling, and evaluation. There are core packages that you can load all together via \texttt{library(tidymodels)} and then extra packages for more specific tasks.
\end{rmdpackage}

As you read through these next chapters, notice the modeling \emph{process} moving through these stages; we'll discuss the structure of this process in more detail in the foreword for the deep learning chapters.

Before we starting fitting these models to real data sets, let's consider how to think about algorithmic bias\index{bias} for predictive modeling.
Rachel Thomas proposed a checklist at \href{https://opendatascience.com/odsc-west-2019-keynote-rachel-thomas-on-algorithmic-bias/}{ODSC West 2019} for algorithmic basic in machine learning.

\hypertarget{should-we-even-be-doing-this}{%
\section*{Should we even be doing this?}\label{should-we-even-be-doing-this}}


\thispagestyle{myheadings}

This is always the first step. Machine learning algorithms involve math and data, but that does not mean they are neutral. They can be used for purposes that are helpful, harmful, or even unethical.

\hypertarget{what-bias-is-already-in-the-data}{%
\section*{What bias is already in the data?}\label{what-bias-is-already-in-the-data}}


Chapter \ref{mlregression} uses a data set of United States Supreme Court opinions, with an uneven distribution of years. There are many more opinions from more recent decades than from earlier ones. Bias like this is extremely common in data sets and must be considered in modeling. In this case, we show how using regularized linear models results in better predictions across years than other approaches (Section \ref{comparerf}).

\hypertarget{can-the-code-and-data-be-audited}{%
\section*{Can the code and data be audited?}\label{can-the-code-and-data-be-audited}}


In the case of this book, the code and data are all publicly available. You as a reader can audit our methods and what kinds of bias exist\index{bias} in the data sets. When you take what you have learned in this book and apply it your real-world work, consider how accessible your code and data are to internal and external stakeholders.

\hypertarget{what-are-the-error-rates-for-sub-groups}{%
\section*{What are the error rates for sub-groups?}\label{what-are-the-error-rates-for-sub-groups}}


\thispagestyle{myheadings}

In Section \ref{mlmulticlass} we demonstrate how to measure model performance for a multiclass classifier, but you can also compute model metrics for sub-groups that are not explicitly in your model as class labels or predictors. Using tidy data principles and the \textbf{yardstick} package makes this task well within the reach of data practitioners.

\begin{rmdpackage}
In \textbf{tidymodels}, the \textbf{yardstick} package
{[}@R-yardstick{]} has functions for model evaluation.
\end{rmdpackage}

\hypertarget{what-is-the-accuracy-of-a-simple-rule-based-alternative}{%
\section*{What is the accuracy of a simple rule-based alternative?}\label{what-is-the-accuracy-of-a-simple-rule-based-alternative}}


Chapter \ref{mlclassification} shows how to train models to predict the category of a user complaint using sophisticated preprocessing steps and machine learning algorithms, but such a complaint could be categorized using simple \index{regex}regular expressions (Appendix \ref{regexp}), perhaps combined with other rules. Straightforward heuristics are easy to implement, maintain, and audit, compared to machine learning models; consider comparing the accuracy of models to simpler options.

\hypertarget{what-processes-are-in-place-to-handle-appeals-or-mistakes}{%
\section*{What processes are in place to handle appeals or mistakes?}\label{what-processes-are-in-place-to-handle-appeals-or-mistakes}}


If models such as those built in Chapter \ref{mlclassification} were put into production by an organization, what would happen if a complaint was classified incorrectly? We as data practitioners typically (hopefully) have a reasonable estimate of the true positive rate and true negative rate for models we train, so processes to handle misclassifications can be built with a good understanding of how often they will be used.

\hypertarget{how-diverse-is-the-team-that-built-it}{%
\section*{How diverse is the team that built it?}\label{how-diverse-is-the-team-that-built-it}}


\thispagestyle{myheadings}

The two-person team that wrote this book includes perspectives from a man and woman, and from someone who has always lived inside the United States and someone who is from a European country. However, we are both white with similar educational backgrounds. We must be aware of how the limited life experiences of individuals training and assessing machine learning models can cause unintentional harm.\index{bias}

\hypertarget{mlregression}{%
\chapter{Regression}\label{mlregression}}

In this chapter, we will use machine learning to predict \emph{continuous values} that are associated with text data. Like in all predictive modeling tasks, this chapter demonstrates how to use learning algorithms to find and model relationships between an outcome or target variable and other input features. What is unique about the focus of this book is that our features are created from text data following the techniques laid out in Chapters \ref{language} through \ref{embeddings}, and what is unique about the focus of this particular chapter is that our outcome is numeric and continuous. For example, let's consider a sample of opinions from the United States Supreme Court, available in the \textbf{scotus} \citep{R-scotus} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(scotus)}

\NormalTok{scotus\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10,000 x 5
#>    year  case_name                  docket_number     id text                   
#>    <chr> <chr>                      <chr>          <dbl> <chr>                  
#>  1 1903  Clara Perry, Plff. In Err~ 16             80304 "No. 16.\n State Repor~
#>  2 1987  West v. Conrail            85-1804        96216 "No. 85-1804.\n\n     ~
#>  3 1957  Roth v. United States      582            89930 "Nos. 582, 61.\nNo. 61~
#>  4 1913  McDermott v. Wisconsin     Nos. 112 and ~ 82218 "Nos. 112 and 113.\nMr~
#>  5 1826  Wetzell v. Bussard         <NA>           52899 "Feb. 7th.\nThis cause~
#>  6 1900  Forsyth v. Vehmeyer        180            79609 "No. 180.\nMr. Edward ~
#>  7 1871  Reed v. United States      <NA>           57846 "APPEAL and cross appe~
#>  8 1833  United States v. Mills     <NA>           53394 "CERTIFICATE of Divisi~
#>  9 1940  Puerto Rico v. Rubert Her~ 582            87714 "No. 582.\nMr. Wm. Cat~
#> 10 1910  Williams v. First Nat. Ba~ 130            81588 "No. 130.\nThe defenda~
#> # ... with 9,990 more rows
\end{verbatim}

This data set contains the entire text of each opinion in the \texttt{text} column, along with the \texttt{case\_name} and \texttt{docket\_number}. Notice that we also have the \texttt{year} that each case was decided by the Supreme Court; this is basically a continuous variable (rather than a group membership of discrete label).

\begin{rmdnote}
If we want to build a model to predict which court opinions were written
in which years, we would build a regression model.
\end{rmdnote}

\begin{itemize}
\item
  A \textbf{classification model} predicts a class label or group membership.
\item
  A \textbf{regression model} predicts a numeric or continuous value.
\end{itemize}

In text modeling, we use text data (such as the text of the court opinions), sometimes combined with other structured, non-text data, to predict the continuous value of interest (such as year of the court opinion). The goal of predictive modeling with text input features and a continuous outcome is to learn and model the relationship between the input features and the numeric target (outcome).

\hypertarget{firstmlregression}{%
\section{A first regression model}\label{firstmlregression}}

Let's build our first regression model using this sample of Supreme Court opinions. Before we start, let's check out how many opinions we have for each decade in Figure \ref{fig:scotushist}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \FunctionTok{as.numeric}\NormalTok{(year),}
         \AttributeTok{year =} \DecValTok{10} \SpecialCharTok{*}\NormalTok{ (year }\SpecialCharTok{\%/\%} \DecValTok{10}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(year) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(year, n)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Number of opinions per decade"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/scotushist-1} 

}

\caption{Supreme Court opinions per decade in sample}\label{fig:scotushist}
\end{figure}

This sample of opinions reflects the distribution over time of available opinions for analysis; there are many more opinions per year in this data set after about 1850 than before. This is an example of bias already in our data, as we discussed in the foreword to these chapters, and we will need to account for that in choosing a model and understanding our results.

\hypertarget{firstregression}{%
\subsection{Building our first regression model}\label{firstregression}}

Our first step in building a model is to split our data into training and testing sets. We use functions from \textbf{tidymodels} for this; we use \texttt{initial\_split()} to set up \emph{how} to split the data, and then we use the functions \texttt{training()} and \texttt{testing()} to create the data sets we need. Let's also convert the year to a numeric value since it was originally stored as a character, and remove the \texttt{\textquotesingle{}} character because of its effect on one of the models\footnote{The random forest implementation in the ranger package, demonstrated in Section \ref{comparerf}, does not handle special characters in columns names well.} we want to try out.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{scotus\_split }\OtherTok{\textless{}{-}}\NormalTok{ scotus\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \FunctionTok{as.numeric}\NormalTok{(year),}
         \AttributeTok{text =} \FunctionTok{str\_remove\_all}\NormalTok{(text, }\StringTok{"\textquotesingle{}"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{initial\_split}\NormalTok{()}

\NormalTok{scotus\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(scotus\_split)}
\NormalTok{scotus\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(scotus\_split)}
\end{Highlighting}
\end{Shaded}

Next, let's \index{preprocessing}preprocess our data to get it ready for modeling using a recipe. We'll use both general preprocessing functions from \textbf{tidymodels} and specialized functions just for text from \textbf{textrecipes} in this preprocessing.

\begin{rmdpackage}
The \textbf{recipes} package \citep{R-recipes} is part of \textbf{tidymodels} and provides functions for data preprocessing and feature engineering. The \textbf{textrecipes} package \citep{textrecipes} extends \textbf{recipes} by providing steps that create features for modeling from text, as we explored in the first five chapters of this book.
\end{rmdpackage}
\index{preprocessing}
\index{feature engineering}

What are the steps in creating this recipe?

\begin{itemize}
\item
  First, we must specify in our initial \texttt{recipe()} statement the form of our model (with the formula \texttt{year\ \textasciitilde{}\ text}, meaning we will predict the year of each opinion from the text of that opinion) and what our training data is.
\item
  Then, we tokenize (Chapter \ref{tokenization}) the text of the court opinions.
\item
  Next, we filter to only keep the top 1000 tokens by term frequency. We filter out those less frequent words because we expect them to be too rare to be reliable, at least for our first attempt. (We are \emph{not} removing stop words yet; we'll explore removing them in Section \ref{casestudystopwords}.)
\item
  The recipe step \texttt{step\_tfidf()}, used with defaults here, weights each token frequency by the inverse document frequency.\index{tf-idf}
\item
  As a last step, we normalize (center and scale) these tf-idf values. This centering and scaling is needed because we're going to use a support vector machine model.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textrecipes)}

\NormalTok{scotus\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(year }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ scotus\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}

\NormalTok{scotus\_rec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>    outcome          1
#>  predictor          1
#> 
#> Operations:
#> 
#> Tokenization for text
#> Text filtering for text
#> Term frequency-inverse document frequency with text
#> Centering and scaling for all_predictors()
\end{verbatim}

Now that we have a full specification of the preprocessing recipe, we can \texttt{prep()} this recipe to estimate all the necessary parameters for each step using the training data and \texttt{bake()} it to apply the steps to data, like the training data (with \texttt{new\_data\ =\ NULL}), testing data, or new data at prediction time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(scotus\_rec)}
\NormalTok{scotus\_bake }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(scotus\_prep, }\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{)}

\FunctionTok{dim}\NormalTok{(scotus\_bake)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 7500 1001
\end{verbatim}

For most modeling tasks, you will not need to \texttt{prep()} or \texttt{bake()} your recipe directly; instead you can build up a tidymodels \texttt{workflow()} to bundle together your modeling components.

\begin{rmdpackage}
In \textbf{tidymodels}, the \textbf{workflows} package \citep{R-workflows} offers infrastructure for bundling model components. A \emph{model workflow} is a convenient way to combine different modeling components (a preprocessor plus a model specification); when these are bundled explicitly, it can be easier to keep track of your modeling plan, as well as fit your model and predict on new data.
\end{rmdpackage}

Let's create a \texttt{workflow()} to bundle together our recipe with any model specifications we may want to create later. First, let's create an empty \texttt{workflow()} and then only add the data preprocessor \texttt{scotus\_rec} to it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(scotus\_rec)}

\NormalTok{scotus\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: None
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 4 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> * step_normalize()
\end{verbatim}

Notice that there is no model yet: \texttt{Model:\ None}. It's time to specify the model we will use! Let's build a support vector machine (SVM) model. While they don't see widespread use in cutting-edge machine learning research today, they are frequently used in practice and have properties that make them well-suited for text classification \citep{Joachims1998} and can give good performance \citep{Vantu2016}.

\begin{rmdnote}
An SVM model can be used for either regression or classification, and linear SVMs often work well with text data. Even better, linear SVMs typically do not need to be tuned (see Section \ref{tunelasso} for tuning model hyperparameters).
\end{rmdnote}

Before fitting, we set up a model specification. There are three components to specifying a model using tidymodels: the model algorithm (a linear SVM here), the mode (typically either classification or regression), and the computational engine we are choosing to use. For our linear SVM, let's use the \textbf{LiblineaR} engine \citep{R-LiblineaR}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_spec }\OtherTok{\textless{}{-}} \FunctionTok{svm\_linear}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"LiblineaR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Everything is now ready for us to fit our model. Let's add our model to the workflow with \texttt{add\_model()} and fit to our training data \texttt{scotus\_train}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_fit }\OtherTok{\textless{}{-}}\NormalTok{ scotus\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(svm\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ scotus\_train)}
\end{Highlighting}
\end{Shaded}

We have successfully fit an SVM model to this data set of Supreme Court opinions. What does the result look like? We can access the fit using \texttt{pull\_workflow\_fit()}, and even \texttt{tidy()} the model coefficient results into a convenient dataframe format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull\_workflow\_fit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{estimate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1,001 x 2
#>    term                  estimate
#>    <chr>                    <dbl>
#>  1 Bias                   1920.  
#>  2 tfidf_text_appeals        1.48
#>  3 tfidf_text_see            1.45
#>  4 tfidf_text_later          1.36
#>  5 tfidf_text_even           1.33
#>  6 tfidf_text_example        1.30
#>  7 tfidf_text_noted          1.25
#>  8 tfidf_text_petitioner     1.25
#>  9 tfidf_text_based          1.25
#> 10 tfidf_text_relevant       1.20
#> # ... with 991 more rows
\end{verbatim}

The term \texttt{Bias} here means the same thing as an intercept. We see here what terms contribute to a Supreme Court opinion being written more recently, like ``appeals'' and ``petitioner''.

What terms contribute to a Supreme Court opinion being written further in the past, for this first attempt at a model?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull\_workflow\_fit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(estimate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1,001 x 2
#>    term             estimate
#>    <chr>               <dbl>
#>  1 tfidf_text_1st      -1.79
#>  2 tfidf_text_but      -1.73
#>  3 tfidf_text_the      -1.62
#>  4 tfidf_text_same     -1.55
#>  5 tfidf_text_it       -1.44
#>  6 tfidf_text_this     -1.43
#>  7 tfidf_text_cause    -1.41
#>  8 tfidf_text_bound    -1.37
#>  9 tfidf_text_be       -1.36
#> 10 tfidf_text_been     -1.35
#> # ... with 991 more rows
\end{verbatim}

Here we see words like ``ought'' and ``therefore''.

\hypertarget{firstregressionevaluation}{%
\subsection{Evaluation}\label{firstregressionevaluation}}

One option for evaluating our model is to predict one time on the testing set to measure performance.

\begin{rmdwarning}
The testing set is extremely valuable data, however, and in real world
situations, we advise that you only use this precious resource one time
(or at most, twice).
\end{rmdwarning}

The purpose of the testing data is to estimate how your final model will perform on new data; we set aside a proportion of the data available and pretend that it is not available to us for training the model so we can use it to estimate model performance on strictly out-of-sample data. Often during the process of modeling, we want to compare models or different model parameters. If we use the test set for these kinds of tasks, we risk fooling ourselves that we are doing better than we really are.

Another option for evaluating models is to predict one time on the training set to measure performance. This is the \emph{same data} that was used to train the model, however, and evaluating on the training data often results in performance estimates that are too optimistic. This is especially true for powerful machine learning algorithms that can learn subtle patterns from data; we risk overfitting to the training set.\index{models!comparing}

Yet another option for evaluating or comparing models is to use a separate validation set. In this situation, we split our data \emph{not} into two sets (training and testing) but into three sets (testing, training, and validation). The validation set is used for computing performance metrics to compare models or model parameters. This can be a great option if you have enough data for it, but often we as machine learning practitioners are not so lucky.

What are we to do, then, if we want to train multiple models and find the best one? Or compute a reliable estimate for how our model has performed without wasting the valuable testing set? We can use \textbf{resampling}. When we resample, we create new simulated data sets from the training set for the purpose of, for example, measuring model performance.

Let's estimate the performance of the linear SVM regression model we just fit. We can do this using resampled data sets built from the training set.

\begin{rmdpackage}
In \textbf{tidymodels}, the package for data splitting and resampling is
\textbf{rsample} {[}@R-rsample{]}.
\end{rmdpackage}

Let's create 10-fold cross-validation sets, and use these resampled sets for performance estimates.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{scotus\_folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(scotus\_train)}

\NormalTok{scotus\_folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> #  10-fold cross-validation 
#> # A tibble: 10 x 2
#>    splits             id    
#>    <list>             <chr> 
#>  1 <split [6750/750]> Fold01
#>  2 <split [6750/750]> Fold02
#>  3 <split [6750/750]> Fold03
#>  4 <split [6750/750]> Fold04
#>  5 <split [6750/750]> Fold05
#>  6 <split [6750/750]> Fold06
#>  7 <split [6750/750]> Fold07
#>  8 <split [6750/750]> Fold08
#>  9 <split [6750/750]> Fold09
#> 10 <split [6750/750]> Fold10
\end{verbatim}

Each of these ``splits'' contains information about how to create cross-validation folds from the original training data. In this example, 90\% of the training data is included in each fold for analysis and the other 10\% is held out for assessment. Since we used cross-validation, each Supreme Court opinion appears in only one of these held-out assessment sets.

In Section \ref{firstregression}, we fit one time to the training data as a whole. Now, to estimate how well that model performs, let's fit many times, once to each of these resampled folds, and then evaluate on the heldout part of each resampled fold.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{svm\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  scotus\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_model}\NormalTok{(svm\_spec),}
\NormalTok{  scotus\_folds,}
  \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}

\NormalTok{svm\_rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Resampling results
#> # 10-fold cross-validation 
#> # A tibble: 10 x 5
#>    splits          id     .metrics          .notes           .predictions       
#>    <list>          <chr>  <list>            <list>           <list>             
#>  1 <split [6750/7~ Fold01 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  2 <split [6750/7~ Fold02 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  3 <split [6750/7~ Fold03 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  4 <split [6750/7~ Fold04 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  5 <split [6750/7~ Fold05 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  6 <split [6750/7~ Fold06 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  7 <split [6750/7~ Fold07 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  8 <split [6750/7~ Fold08 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#>  9 <split [6750/7~ Fold09 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
#> 10 <split [6750/7~ Fold10 <tibble[,4] [2 x~ <tibble[,1] [0 ~ <tibble[,4] [750 x~
\end{verbatim}

These results look a lot like the resamples, but they have some additional columns, like the \texttt{.metrics} that we can use to measure how well this model performed and the \texttt{.predictions} we can use to explore that performance more deeply. What results do we see, in terms of performance metrics?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_metrics}\NormalTok{(svm\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric .estimator   mean     n std_err .config             
#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
#> 1 rmse    standard   15.9      10 0.189   Preprocessor1_Model1
#> 2 rsq     standard    0.892    10 0.00150 Preprocessor1_Model1
\end{verbatim}

The default performance metrics to be computed for regression models are RMSE (root mean squared error) and \(R^2\) (coefficient of determination). RMSE is a metric that is in the same units as the original data, so in units of \emph{years}, in our case; the RMSE of this first regression model is 15.9 years.

\index{root mean squared error|see {RMSE}}
\index{RMSE}
\index{coefficient of determination}

\begin{rmdnote}
RSME and \(R^2\) are performance metrics used for regression models.

RSME is a measure of the difference between the predicted and observed
values; if the model fits the data well, RMSE is lower. To compute RMSE,
you take the mean values of the squared difference between the predicted
and observed values, then take the square root.

\(R^2\) is the squared correlation between the predicted and observed
values. When the model fits the data well, the predicted and observed
values are closer together with a higher correlation between them. The
correlation between two variables is bounded between -1 and 1, so the
closer \(R^2\) is to one, the better.
\end{rmdnote}

These values are quantitative estimates for how well our model performed, and can be compared across different kinds of models. Figure \ref{fig:firstregpredict} shows the predicted years for these Supreme Court opinions plotted against the true years when they were published, for all the resampled data sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(year, .pred, }\AttributeTok{color =}\NormalTok{ id)) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{color =} \StringTok{"gray80"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Truth"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Predicted year"}\NormalTok{,}
    \AttributeTok{color =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Predicted and true years for Supreme Court opinions"}\NormalTok{,}
    \AttributeTok{subtitle =} \StringTok{"Each cross{-}validation fold is shown in a different color"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/firstregpredict-1} 

}

\caption{Most Supreme Court opinions are near the dashed line, indicating good agreement between our SVM regression predictions and the real years}\label{fig:firstregpredict}
\end{figure}

The average spread of points in this plot above and below the dashed line corresponds to RMSE, which is 15.9 years for this model. When RMSE is better (lower), the points will be closer to the dashed line. This first model we have tried did not do a great job for Supreme Court opinions from before 1850, but for opinions after 1850, this looks pretty good!

\begin{rmdwarning}
Hopefully you are convinced that using resampled data sets for measuring
performance is the right choice, but it can be computationally
expensive. Instead of fitting once, we must fit the model one time for
\emph{each} resample. The resamples are independent of each other, so
this is a great fit for parallel processing. The tidymodels framework is
designed to work fluently with parallel processing in R, using multiple
cores or multiple machines. The implementation details of parallel
processing are operating system specific, so
\href{https://tune.tidymodels.org/articles/extras/optimizations.html}{look
at tidymodels' documentation for how to get started}.
\end{rmdwarning}

\hypertarget{regnull}{%
\section{Compare to the null model}\label{regnull}}

One way to assess a model like this one is to compare its performance to a ``null model.''

\begin{rmdnote}
A null model is a simple, non-informative model that always predicts the largest class (for classification) or the mean (such as the mean year of Supreme Court opinions, in our specific regression case)\footnote{This is sometimes called a ``baseline model.''}.
\end{rmdnote}

We can use the same function \texttt{fit\_resamples()} and the same preprocessing recipe as before, switching out our SVM model specification for the \texttt{null\_model()} specification.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_regression }\OtherTok{\textless{}{-}} \FunctionTok{null\_model}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"parsnip"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{null\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  scotus\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_model}\NormalTok{(null\_regression),}
\NormalTok{  scotus\_folds,}
  \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(rmse)}
\NormalTok{)}

\NormalTok{null\_rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Resampling results
#> # 10-fold cross-validation 
#> # A tibble: 10 x 4
#>    splits             id     .metrics             .notes              
#>    <list>             <chr>  <list>               <list>              
#>  1 <split [6750/750]> Fold01 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  2 <split [6750/750]> Fold02 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  3 <split [6750/750]> Fold03 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  4 <split [6750/750]> Fold04 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  5 <split [6750/750]> Fold05 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  6 <split [6750/750]> Fold06 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  7 <split [6750/750]> Fold07 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  8 <split [6750/750]> Fold08 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#>  9 <split [6750/750]> Fold09 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
#> 10 <split [6750/750]> Fold10 <tibble[,4] [1 x 4]> <tibble[,1] [0 x 1]>
\end{verbatim}

What results do we obtain from the null model, in terms of performance metrics?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_metrics}\NormalTok{(null\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 6
#>   .metric .estimator  mean     n std_err .config             
#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               
#> 1 rmse    standard    48.0    10   0.512 Preprocessor1_Model1
\end{verbatim}

The RMSE indicates that this null model is dramatically worse than our first model. Even our first very attempt at a regression model (using only unigrams and very little specialized preprocessing)\index{preprocessing} did much better than the null model; the text of the Supreme Court opinions has enough information in it related to the year the opinions were published that we can build successful models.

\hypertarget{comparerf}{%
\section{Compare to a random forest model}\label{comparerf}}

Random forest models are broadly used in predictive modeling contexts because they are low-maintenance and perform well. For example, see \citet{Caruana2008} and \citet{Olson2017} for comparisons of the performance of common models such as random forest, decision tree, support vector machines, etc. trained on benchmark data sets; random forest models were one of the best overall. Let's see how a random forest model performs with our data set of Supreme Court opinions.

First, let's build a random forest model specification, using the ranger implementation. Random forest models are known for performing well without hyperparameter tuning, so we will just make sure we have enough \texttt{trees}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_spec }\OtherTok{\textless{}{-}} \FunctionTok{rand\_forest}\NormalTok{(}\AttributeTok{trees =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"ranger"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{rf\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Random Forest Model Specification (regression)
#> 
#> Main Arguments:
#>   trees = 1000
#> 
#> Computational engine: ranger
\end{verbatim}

Now we can fit this random forest model. Let's use \texttt{fit\_resamples()} again, so we can evaluate the model performance. We will use three arguments to this function:

\begin{itemize}
\item
  Our modeling \texttt{workflow()}, with the same preprocessing recipe we have been using so far in this chapter plus our new random forest model specification
\item
  Our cross-validation resamples of the Supreme Court opinions
\item
  A \texttt{control} argument to specify that we want to keep the predictions, to explore after fitting
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  scotus\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_model}\NormalTok{(rf\_spec),}
\NormalTok{  scotus\_folds,}
  \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can use \texttt{collect\_metrics()} to obtain and format the performance metrics for this random forest model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_metrics}\NormalTok{(rf\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric .estimator   mean     n std_err .config             
#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
#> 1 rmse    standard   15.0      10 0.487   Preprocessor1_Model1
#> 2 rsq     standard    0.919    10 0.00434 Preprocessor1_Model1
\end{verbatim}

This looks pretty promising, so let's explore the predictions for this random forest model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_predictions}\NormalTok{(rf\_rs) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(year, .pred, }\AttributeTok{color =}\NormalTok{ id)) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{color =} \StringTok{"gray80"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Truth"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Predicted year"}\NormalTok{,}
    \AttributeTok{color =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{title =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Predicted and true years for Supreme Court opinions using"}\NormalTok{,}
                  \StringTok{"a random forest model"}\NormalTok{, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{),}
    \AttributeTok{subtitle =} \StringTok{"Each cross{-}validation fold is shown in a different color"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/rfpredict-1} 

}

\caption{The random forest model did not perform very sensibly across years, compared to our first attempt using a linear SVM model}\label{fig:rfpredict}
\end{figure}

Figure \ref{fig:rfpredict} shows some of the strange behavior from our fitted model. The overall performance metrics look pretty good, but predictions are too high and too low around certain threshold years. \index{models!challenges}

It is very common to run into problems when using tree-based models like random forests with text data. One of the defining characteristics of text data is that it is \emph{sparse}, with many features but most features not occurring in most observations. Tree-based models such as random forests are often not well-suited to sparse data because of how decision trees model outcomes \citep{Tang2018}.

\begin{rmdnote}
Models that work best with text tend to be models designed for or
otherwise appropriate for sparse data.
\end{rmdnote}

Algorithms that work well with sparse data are less important when text has been transformed to a non-sparse representation such as with word embeddings (Chapter \ref{embeddings}).

\hypertarget{casestudystopwords}{%
\section{Case study: removing stop words}\label{casestudystopwords}}

We did not remove stop words (Chapter \ref{stopwords}) in any of our models so far in this chapter. What impact will removing stop words have, and how do we know which stop word list is the best to use? The best way to answer these questions is with experimentation.

Removing stop words is part of \index{preprocessing}data preprocessing, so we define this step as part of our preprocessing recipe. Let's use the best model we've found so far (the linear SVM model from Section \ref{firstregressionevaluation}) and switch in a different recipe in our modeling workflow.

Let's build a small recipe wrapper helper function so we can pass a value \texttt{stopword\_name} to \texttt{step\_stopwords()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stopword\_rec }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(stopword\_name) \{}
  \FunctionTok{recipe}\NormalTok{(year }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ scotus\_train) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_stopwords}\NormalTok{(text, }\AttributeTok{stopword\_source =}\NormalTok{ stopword\_name) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_tfidf}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

For example, now we can create a recipe that removes the Snowball stop words list\index{stop word lists!Snowball} by calling this function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stopword\_rec}\NormalTok{(}\StringTok{"snowball"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>    outcome          1
#>  predictor          1
#> 
#> Operations:
#> 
#> Tokenization for text
#> Stop word removal for text
#> Text filtering for text
#> Term frequency-inverse document frequency with text
#> Centering and scaling for all_predictors()
\end{verbatim}

Next, let's set up a new workflow that has a model only, using \texttt{add\_model()}. We start with the empty \texttt{workflow()} and then add our linear SVM regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(svm\_spec)}

\NormalTok{svm\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: None
#> Model: svm_linear()
#> 
#> -- Model -----------------------------------------------------------------------
#> Linear Support Vector Machine Specification (regression)
#> 
#> Computational engine: LiblineaR
\end{verbatim}

Notice that for this workflow, there is no preprocessor yet: \texttt{Preprocessor:\ None}. This workflow uses the same linear SVM specification that we used in Section \ref{firstmlregression} but we are going to combine several different preprocessing recipes with it, one for each stop word lexicon we want to try.

Now we can put this all together and fit these models which include stop word removal. We could create a little helper function for fitting like we did for the recipe, but we have printed out all three calls to \texttt{fit\_resamples()} for extra clarity. Notice for each one that there are two arguments:

\begin{itemize}
\item
  A workflow, which consists of the linear SVM model specification and a data preprocessing recipe with stop word removal
\item
  The same cross-validation folds we created earlier
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{snowball\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  svm\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_recipe}\NormalTok{(}\FunctionTok{stopword\_rec}\NormalTok{(}\StringTok{"snowball"}\NormalTok{)),}
\NormalTok{  scotus\_folds}
\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{234}\NormalTok{)}
\NormalTok{smart\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  svm\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_recipe}\NormalTok{(}\FunctionTok{stopword\_rec}\NormalTok{(}\StringTok{"smart"}\NormalTok{)),}
\NormalTok{  scotus\_folds}
\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{345}\NormalTok{)}
\NormalTok{stopwords\_iso\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  svm\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_recipe}\NormalTok{(}\FunctionTok{stopword\_rec}\NormalTok{(}\StringTok{"stopwords{-}iso"}\NormalTok{)),}
\NormalTok{  scotus\_folds}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After fitting models to each of the cross-validation folds, these sets of results contain metrics computed for removing that set of stop words.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_metrics}\NormalTok{(smart\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric .estimator   mean     n std_err .config             
#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
#> 1 rmse    standard   16.8      10 0.194   Preprocessor1_Model1
#> 2 rsq     standard    0.880    10 0.00305 Preprocessor1_Model1
\end{verbatim}

We can explore whether one of these sets of stop words performed better than the others by comparing the performance, for example in terms of RMSE as shown Figure \ref{fig:snowballrmse}. This plot shows the five best models for each set of stop words, using \texttt{show\_best()} applied to each via \texttt{purrr::map\_dfr()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{word\_counts }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"snowball"}\NormalTok{, }\StringTok{"smart"}\NormalTok{, }\StringTok{"stopwords{-}iso"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{words =} \FunctionTok{map\_int}\NormalTok{(name, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{length}\NormalTok{(stopwords}\SpecialCharTok{::}\FunctionTok{stopwords}\NormalTok{(}\AttributeTok{source =}\NormalTok{ .))))}

\FunctionTok{list}\NormalTok{(}\AttributeTok{snowball =}\NormalTok{ snowball\_rs,}
     \AttributeTok{smart =}\NormalTok{ smart\_rs,}
     \StringTok{\textasciigrave{}}\AttributeTok{stopwords{-}iso}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ stopwords\_iso\_rs) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map\_dfr}\NormalTok{(show\_best, }\StringTok{"rmse"}\NormalTok{, }\AttributeTok{.id =} \StringTok{"name"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(word\_counts, }\AttributeTok{by =} \StringTok{"name"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{name =} \FunctionTok{paste0}\NormalTok{(name, }\StringTok{" ("}\NormalTok{, words, }\StringTok{" words)"}\NormalTok{),}
         \AttributeTok{name =} \FunctionTok{fct\_reorder}\NormalTok{(name, words)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(name, mean, }\AttributeTok{color =}\NormalTok{ name)) }\SpecialCharTok{+}
  \FunctionTok{geom\_crossbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ mean }\SpecialCharTok{{-}}\NormalTok{ std\_err, }\AttributeTok{ymax =}\NormalTok{ mean }\SpecialCharTok{+}\NormalTok{ std\_err), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \StringTok{"RMSE"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Model performance for three stop word lexicons"}\NormalTok{,}
       \AttributeTok{subtitle =} \StringTok{"For this data set, the Snowball lexicon performed best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/snowballrmse-1} 

}

\caption{Comparing model performance for predicting the year of Supreme Court opinions with three different stop word lexicons}\label{fig:snowballrmse}
\end{figure}

The \index{stop word lists!Snowball}Snowball lexicon contains the smallest number of words (see Figure \ref{fig:stopwordoverlap}) and, in this case, results in the best performance. Removing fewer stop words results in the best performance.

\begin{rmdwarning}
This result is not generalizable to all data sets and contexts, but the
approach outlined in this section \textbf{is} generalizable.
\end{rmdwarning}

This approach can be used to compare different lexicons and find the best one for a specific data set and model. Notice how the results all stop word lexicons are worse than removing no stopwords at all (remember that the RMSE was 15.9 years in Section \ref{firstregressionevaluation}). This indicates that, for this particular data set, removing even a small stop word list is not a great choice.

When removing stop words does appear to help a model, it's good to know that removing stop words isn't computationally slow or difficult so the cost for this improvement is low.\index{computational speed}

\hypertarget{casestudyngrams}{%
\section{Case study: varying n-grams}\label{casestudyngrams}}

Each model trained so far in this chapter has involved single words or \emph{unigrams}, but using \index{tokenization!n-gram}n-grams (Section \ref{tokenizingngrams}) can integrate different kinds of information into a model. Bigrams and trigrams (or even higher order n-grams) capture concepts that span single words, as well as effects from word order, that can be predictive.

This is another part of data preprocessing\index{preprocessing}, so we again define this step as part of our preprocessing recipe. Let's build another small recipe wrapper helper function so we can pass a list of options \texttt{ngram\_options} to \texttt{step\_tokenize()}. We'll use it with the same model as the previous section.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ngram\_rec }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(ngram\_options) \{}
  \FunctionTok{recipe}\NormalTok{(year }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ scotus\_train) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_tokenize}\NormalTok{(text, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{options =}\NormalTok{ ngram\_options) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_tfidf}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

There are two options we can specify, \texttt{n} and \texttt{n\_min}, when we are using \texttt{engine\ =\ "tokenizers"}. We can set up a recipe with only \texttt{n\ =\ 1} to tokenize and only extract the unigrams.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ngram\_rec}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We can use \texttt{n\ =\ 3,\ n\_min\ =\ 1} to identify the set of all trigrams, bigrams, \emph{and} unigrams.\index{tokenization!n-gram}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ngram\_rec}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{, }\AttributeTok{n\_min =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
Including n-grams of different orders in a model (such as trigrams,
bigrams, plus unigrams) allows the model to learn at different levels of
linguistic organization and context.
\end{rmdnote}

We can reuse the same workflow \texttt{svm\_wf} from our earlier case study; these types of modular components are a benefit to adopting this approach to supervised machine learning. This workflow provides the linear SVM specification. Let's put it all together and create a helper function to use \texttt{fit\_resamples()} with this model plus our helper recipe function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_ngram }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(ngram\_options) \{}
  \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{    svm\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_recipe}\NormalTok{(}\FunctionTok{ngram\_rec}\NormalTok{(ngram\_options)),}
\NormalTok{    scotus\_folds}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
We could have created this type of small function for trying out different stop word lexicons in Section \ref{casestudystopwords}, but there we showed each call to \texttt{fit\_resamples()} for extra clarity.
\end{rmdwarning}

With this helper function, let's try out predicting the year of Supreme Court opinions using:

\begin{itemize}
\item
  only unigrams
\item
  bigrams and unigrams
\item
  trigrams, bigrams, and unigrams
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{unigram\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_ngram}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{))}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{234}\NormalTok{)}
\NormalTok{bigram\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_ngram}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{n =} \DecValTok{2}\NormalTok{, }\AttributeTok{n\_min =} \DecValTok{1}\NormalTok{))}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{345}\NormalTok{)}
\NormalTok{trigram\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_ngram}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{, }\AttributeTok{n\_min =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

These sets of results contain metrics computed for the model with that tokenization strategy.\index{tokenization!n-gram}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_metrics}\NormalTok{(bigram\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric .estimator   mean     n std_err .config             
#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
#> 1 rmse    standard   16.0      10 0.191   Preprocessor1_Model1
#> 2 rsq     standard    0.890    10 0.00211 Preprocessor1_Model1
\end{verbatim}

We can compare the performance of these models in terms of RMSE as shown Figure \ref{fig:ngramrmse}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{list}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{1}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ unigram\_rs,}
     \StringTok{\textasciigrave{}}\AttributeTok{1 and 2}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ bigram\_rs,}
     \StringTok{\textasciigrave{}}\AttributeTok{1, 2, and 3}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ trigram\_rs) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map\_dfr}\NormalTok{(collect\_metrics, }\AttributeTok{.id =} \StringTok{"name"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"rmse"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(name, mean, }\AttributeTok{color =}\NormalTok{ name)) }\SpecialCharTok{+}
  \FunctionTok{geom\_crossbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ mean }\SpecialCharTok{{-}}\NormalTok{ std\_err, }\AttributeTok{ymax =}\NormalTok{ mean }\SpecialCharTok{+}\NormalTok{ std\_err), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Degree of n{-}grams"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"RMSE"}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Model performance for different degrees of n{-}gram tokenization"}\NormalTok{,}
    \AttributeTok{subtitle =} \FunctionTok{paste}\NormalTok{(}\StringTok{"For the same number of tokens,"}\NormalTok{,}
                     \StringTok{"bigrams plus unigrams performed best"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/ngramrmse-1} 

}

\caption{Comparing model performance for predicting the year of Supreme Court opinions with three different degrees of n-grams}\label{fig:ngramrmse}
\end{figure}

Each of these models was trained with \texttt{max\_tokens\ =\ 1e3}, i.e., including only the top 1000 tokens for each tokenization strategy. Holding the number of tokens constant, using unigrams alone performs best for this corpus of Supreme Court opinions. To be able to incorporate the more complex information in bigrams or trigrams, we would need to increase the number of tokens in the model considerably.

Keep in mind that adding n-grams\index{tokenization!n-gram} is computationally expensive\index{computational speed} to start with, especially compared to the typical improvement in model performance gained. We can benchmark the whole model workflow, including preprocessing\index{preprocessing} and modeling. Using bigrams plus unigrams takes more than twice as long to train than only unigrams (number of tokens held constant), and adding in trigrams as well takes almost five times as long as training on unigrams alone.

\hypertarget{mlregressionlemmatization}{%
\section{Case study: lemmatization}\label{mlregressionlemmatization}}

As we discussed in Section \ref{lemmatization}, we can normalize words to their roots or \index{lemma}\textbf{lemmas} based on each word's context and the structure of a language. Table \ref{tab:lemmatb} shows both the original words and the lemmas for one sentence from a Supreme Court opinion, using lemmatization implemented via the \href{https://spacy.io/}{spaCy} library as made available through the \textbf{spacyr} R package \citep{Benoit19}.

\begin{table}

\caption{\label{tab:lemmatb}Lemmatization of one sentence from a Supreme Court opinion}
\centering
\begin{tabular}[t]{ll}
\toprule
original word & lemma\\
\midrule
However & however\\
, & ,\\
the & the\\
Court & Court\\
of & of\\
\addlinespace
Appeals & Appeals\\
disagreed & disagree\\
with & with\\
the & the\\
District & District\\
\addlinespace
Court & Court\\
's & 's\\
construction & construction\\
of & of\\
the & the\\
\addlinespace
state & state\\
statute & statute\\
, & ,\\
concluding & conclude\\
that & that\\
\addlinespace
it & -PRON-\\
did & do\\
authorize & authorize\\
issuance & issuance\\
of & of\\
\addlinespace
the & the\\
orders & order\\
to & to\\
withhold & withhold\\
to & to\\
\addlinespace
the & the\\
Postal & Postal\\
Service & Service\\
. & .\\
\bottomrule
\end{tabular}
\end{table}

Notice several things about lemmatization\index{lemma} that are different from the kind of default tokenization (Chapter \ref{tokenization}) you may be more familiar with.

\begin{itemize}
\item
  Words are converted to lower case except for proper nouns.
\item
  The lemma for pronouns is \texttt{-PRON-}.
\item
  Irregular verbs are converted to their canonical form (``did'' to ``do'').
\end{itemize}

Using lemmatization\index{lemma} instead of a more straightforward tokenization strategy is slower because of the increased complexity involved, but it can be worth it. Let's explore how to train a model using \emph{lemmas} instead of \emph{words}.

Lemmatization is, like choices around n-grams and stop words, part of data preprocessing\index{preprocessing} so we define how to set up lemmatization as part of our preprocessing recipe. We use \texttt{engine\ =\ "spacyr"} for tokenization (instead of the default) and add \texttt{step\_lemma()} to our preprocessing. This step extracts the lemmas from the parsing done by the tokenization engine.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spacyr}\SpecialCharTok{::}\FunctionTok{spacy\_initialize}\NormalTok{(}\AttributeTok{entity =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> NULL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lemma\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(year }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ scotus\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text, }\AttributeTok{engine =} \StringTok{"spacyr"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_lemma}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}

\NormalTok{lemma\_rec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>    outcome          1
#>  predictor          1
#> 
#> Operations:
#> 
#> Tokenization for text
#> Lemmatization for text
#> Text filtering for text
#> Term frequency-inverse document frequency with text
#> Centering and scaling for all_predictors()
\end{verbatim}

Let's combine this lemmatized\index{lemma} text with our linear SVM workflow. We can then fit our workflow to our resampled data sets and estimate performance using lemmatization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lemma\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  svm\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_recipe}\NormalTok{(lemma\_rec),}
\NormalTok{  scotus\_folds}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

How did this model perform?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_metrics}\NormalTok{(lemma\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric .estimator   mean     n std_err .config             
#>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
#> 1 rmse    standard   14.3      10 0.206   Preprocessor1_Model1
#> 2 rsq     standard    0.912    10 0.00224 Preprocessor1_Model1
\end{verbatim}

The best value for RMSE at 14.3 shows us that using lemmatization\index{lemma} can have a significant benefit for model performance, compared to 15.9 from fitting a non-lemmatized linear SVM model in Section \ref{firstregressionevaluation}. The best model using lemmatization is better than the best model without. However, this comes at a cost of much slower training because of the procedure involved in identifying lemmas; adding \texttt{step\_lemma()} to our preprocessing increases the overall time to train the workflow by over tenfold.\index{computational speed}

\begin{rmdnote}
We can use \texttt{engine\ =\ "spacyr"} to assign part-of-speech tags to
the tokens during tokenization, and this information can be used in
various useful ways in text modeling. One approach is to filter tokens
to only retain a certain part-of-speech, like nouns. An example of how
to do this is illustrated in this
\href{https://www.hvitfeldt.me/blog/tidytuesday-pos-textrecipes-the-office/}{\textbf{textrecipes}
blogpost} and can be performed with \texttt{step\_pos\_filter()}.
\end{rmdnote}

\index{part of speech}

\hypertarget{case-study-feature-hashing}{%
\section{Case study: feature hashing}\label{case-study-feature-hashing}}

The models we have created so far have used tokenization (Chapter \ref{tokenization}) to split apart text data into tokens that are meaningful to us as human beings (words, bigrams) and then weighted these tokens by simple counts with word frequencies or weighted counts with tf-idf.\index{tf-idf}
A problem with these methods is that the output space can be vast and dynamic.
We have limited ourselves to 1000 tokens so far in this chapter, but we could easily have more than 10,000 features in our training set.
We may run into computational problems with memory or long processing times; deciding how many tokens to include can become a trade-off between computational time and information.
This style of approach also doesn't let us take advantage of new tokens we didn't see in our training data.

One method that has gained popularity in the machine learning field is the \textbf{hashing trick}.
This method addresses many of the challenges outlined above and is very fast with a low memory footprint.

Let's start with the basics of feature hashing.\index{hashing function}
First proposed by \citet{Weinberger2009}, feature hashing was introduced as a dimensionality reduction method with a simple premise.
We begin with a hashing function which we then apply to our tokens.

\begin{rmdwarning}
A hashing function takes input of variable size and maps it to output of
a fixed size. Hashing functions are commonly used in cryptography.
\end{rmdwarning}

We will use the \texttt{hash()} function from the \textbf{rlang} package package to illustrate the behavior of hashing functions.
The \texttt{rlang::hash()} function uses the XXH128 hash algorithm of the xxHash library, which generates a 128-bit hash. This is a more complex hashing function than what is normally used for the hashing trick. The 32-bit version of MurmurHash3 \citep{appleby2008} is often used for its speed and good properties.

\begin{rmdnote}
Hashing functions are typically very fast and have certain properties.
For example, the output of a hash function is expected to be uniform,
with the whole output space filled evenly. The ``avalanche effect''
describes how similar strings are hashed in such a way that their hashes
are not similar in the output space.
\end{rmdnote}

Suppose we have many country names in a character vector.
We can apply the hashing function\index{hashing function} to each of the country names to project them into an integer space defined by the hashing function.

Since \texttt{hash()} creates hashes that are very long, let's create \texttt{small\_hash()} for demonstration purposes here that generates slightly smaller hashes. (The specific details of what hashes are generated are not important here.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rlang)}
\NormalTok{countries }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Palau"}\NormalTok{, }\StringTok{"Luxembourg"}\NormalTok{, }\StringTok{"Vietnam"}\NormalTok{, }\StringTok{"Guam"}\NormalTok{, }\StringTok{"Argentina"}\NormalTok{,}
               \StringTok{"Mayotte"}\NormalTok{, }\StringTok{"Bouvet Island"}\NormalTok{, }\StringTok{"South Korea"}\NormalTok{, }\StringTok{"San Marino"}\NormalTok{,}
               \StringTok{"American Samoa"}\NormalTok{)}

\NormalTok{small\_hash }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{strtoi}\NormalTok{(}\FunctionTok{substr}\NormalTok{(}\FunctionTok{hash}\NormalTok{(x), }\DecValTok{26}\NormalTok{, }\DecValTok{32}\NormalTok{), }\DecValTok{16}\NormalTok{)}
\NormalTok{\}}

\FunctionTok{map\_int}\NormalTok{(countries, small\_hash)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1]   4292706   2881716 242176357 240902473 204438359  88787026 230339508
#>  [8]  15112074  96146649 192775182
\end{verbatim}

Our \texttt{small\_hash()} function uses \texttt{7\ *\ 4\ =\ 28} bits, so the number of possible values is \texttt{2\^{}28\ =\ 268435456}. This is admittedly not much of an improvement over ten country names.
Let's take the modulo of these big integer values to project them down to a more manageable space.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map\_int}\NormalTok{(countries, small\_hash) }\SpecialCharTok{\%\%} \DecValTok{24}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] 18 12 13  1 23 10 12 18  9  6
\end{verbatim}

Now we can use these values as indices when creating a matrix.\index{matrix!sparse}

\begin{verbatim}
#> 10 x 24 sparse Matrix of class "ngCMatrix"
#>                                                               
#> Palau          . . . . . . . . . . . . . . . . . | . . . . . .
#> Luxembourg     . . . . . . . . . . . | . . . . . . . . . . . .
#> Vietnam        . . . . . . . . . . . . | . . . . . . . . . . .
#> Guam           | . . . . . . . . . . . . . . . . . . . . . . .
#> Argentina      . . . . . . . . . . . . . . . . . . . . . . | .
#> Mayotte        . . . . . . . . . | . . . . . . . . . . . . . .
#> Bouvet Island  . . . . . . . . . . . | . . . . . . . . . . . .
#> South Korea    . . . . . . . . . . . . . . . . . | . . . . . .
#> San Marino     . . . . . . . . | . . . . . . . . . . . . . . .
#> American Samoa . . . . . | . . . . . . . . . . . . . . . . . .
\end{verbatim}

This method is very fast\index{computational speed}; both the hashing and modulo can be performed independently for each input since neither need information about the full corpus.
Since we are reducing the space, there is a chance that multiple words are hashed to the same value.
This is called a collision and at first glance, it seems like it would be a big problem for a model.
However, research finds that using feature hashing has roughly the same accuracy as a simple bag-of-words model and the effect of collisions is quite minor \citep{Forman2008}.

\begin{rmdnote}
Another step that is taken to avoid the negative effects of hash
collisions is to use a \emph{second} hashing function that returns 1 and
-1. This determines if we are adding or subtracting the index we get
from the first hashing function. Suppose both the words ``outdoor'' and
``pleasant'' hash to the integer value 583. Without the second hashing
they would collide to 2. Using signed hashing, we have a 50\% chance
that they will cancel each other out, which tries to stop one feature
from growing too much.
\end{rmdnote}

There are downsides to using feature hashing.\index{hashing function}\index{hashing function!challenges} Feature hashing:

\begin{itemize}
\item
  still has one tuning parameter, and
\item
  cannot be reversed.
\end{itemize}

The number of buckets you have correlates with computation speed and collision rate which in turn affects performance.
It is your job to find the output that best suits your needs.
Increasing the number of buckets will decrease the collision rate but will, in turn, return a larger output data set which increases model fitting time.
The number of buckets is tunable in tidymodels using the \textbf{tune} package.

Perhaps the more important downside to using feature hashing is that the operation can't be reversed.
We are not able to detect if a collision occurs and it is difficult to understand the effect of any word in the model.
Remember that we are left with \texttt{n} columns of \emph{hashes} (not tokens), so if we find that the 274th column is a highly predictive feature, we cannot know in general which tokens contribute to that column.
We cannot directly connect model values to words or tokens at all.
We could go back to our training set and create a paired list of the tokens and what hashes they map to. Sometimes we might find only one token in that list, but it may have two (or three or four or more!) different tokens contributing.
This feature hashing method is used because of its speed and scalability, not because it is interpretable.

Feature hashing on tokens\index{hashing function} is available in tidymodels using the \texttt{step\_texthash()} step from \textbf{textrecipes}. Let's \texttt{prep()} and \texttt{bake()} this recipe for demonstration purposes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_hash }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(year }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ scotus\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_texthash}\NormalTok{(text, }\AttributeTok{signed =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{num\_terms =} \DecValTok{512}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{)}

\FunctionTok{dim}\NormalTok{(scotus\_hash)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 7500  513
\end{verbatim}

There are many columns in the results. Let's take a \texttt{glimpse()} at the first ten columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_hash }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\FunctionTok{num\_range}\NormalTok{(}\StringTok{"text\_hash00"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 7,500
#> Columns: 9
#> $ text_hash001 <dbl> -47, -2, -9, -10, -15, -7, -5, -12, -4, -9, -2, -1, 2, -2~
#> $ text_hash002 <dbl> 0, -4, -2, 0, 2, -8, 6, 1, 0, 6, -1, 0, 1, 0, 0, 0, -4, -~
#> $ text_hash003 <dbl> 1, -1, 7, -3, 3, 1, 0, 1, 0, 4, 1, 0, 2, 0, 1, -2, -1, -4~
#> $ text_hash004 <dbl> -7, 0, 0, -10, -5, 4, 7, -1, 0, -4, 0, 0, 0, 0, -1, 0, -4~
#> $ text_hash005 <dbl> -1, -4, 1, 0, 2, -2, -1, -17, 0, 0, 0, 0, -1, -1, 0, 0, -~
#> $ text_hash006 <dbl> 42, 3, 11, 0, 42, 9, 26, 6, 0, 18, 8, -1, 2, 6, 0, 0, 26,~
#> $ text_hash007 <dbl> -17, -1, -1, 1, -7, 0, 1, -3, 0, -1, 0, 0, 0, 0, 0, 0, -6~
#> $ text_hash008 <dbl> 15, 1, -2, -1, 3, 5, -1, -2, -1, -1, 5, -2, 1, 1, -1, 4, ~
#> $ text_hash009 <dbl> 6, 0, -4, 0, -30, 0, 0, 0, 0, -3, 0, -1, 0, 0, 0, 0, 0, -~
\end{verbatim}

By using \texttt{step\_texthash()} we can quickly generate machine-ready data with a consistent number of variables.
This typically results in a slight loss of performance compared to using a traditional bag-of-words representation. An example of this loss is illustrated in this \href{https://www.hvitfeldt.me/blog/textrecipes-series-featurehashing/}{\textbf{textrecipes} blogpost}.

\hypertarget{text-normalization}{%
\subsection{Text normalization}\label{text-normalization}}

\index{preprocessing!challenges}When working with text, you will inevitably run into problems with encodings and related irregularities.
These kinds of problems have a significant influence on feature hashing\index{hashing function}, as well as other preprocessing steps.
Consider the German word ``schön.''\index{language!Non-English}
The o with an umlaut (two dots over it) is a fairly simple character but it can be represented in a couple of different ways.
We can either use a single character \href{https://www.fileformat.info/info/unicode/char/00f6/index.htm}{\textbackslash U00f6} to represent the letter with an umlaut.
Alternatively, we can use two characters, one for the o and one character to denote the presence of two dots over the previous character \href{https://www.fileformat.info/info/unicode/char/0308/index.htm}{\textbackslash U0308}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s1 }\OtherTok{\textless{}{-}} \StringTok{"sch\textbackslash{}U00f6n"}
\NormalTok{s2 }\OtherTok{\textless{}{-}} \StringTok{"scho\textbackslash{}U0308n"}
\end{Highlighting}
\end{Shaded}

These two strings will print the same for us as human readers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "schön"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "schön"
\end{verbatim}

However, they are not equal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s1 }\SpecialCharTok{==}\NormalTok{ s2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE
\end{verbatim}

This poses a problem for the avalanche effect, which is needed for feature hashing to perform correctly. The avalanche effect will results in these two words (which should be identical) hashing to completely different values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{small\_hash}\NormalTok{(s1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 180735918
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{small\_hash}\NormalTok{(s2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 3013209
\end{verbatim}

We can deal with this problem by performing \textbf{text normalization} on our text before feeding it into our preprocessing\index{preprocessing} engine.
One library to perform text normalization is the \textbf{stringi} package, which includes many different text normalization methods.
How these methods work is beyond the scope of this book, but know that the text normalization functions make text like our two versions of ``schön''\index{language!Non-English} equivalent. We will use \texttt{stri\_trans\_nfc()} for this example, which performs Canonical Decomposition, followed by Canonical Composition, but we could also use \texttt{textrecipes::step\_text\_normalize()} within a tidymodels recipe for the same task.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stringi)}

\FunctionTok{stri\_trans\_nfc}\NormalTok{(s1) }\SpecialCharTok{==} \FunctionTok{stri\_trans\_nfc}\NormalTok{(s2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{small\_hash}\NormalTok{(}\FunctionTok{stri\_trans\_nfc}\NormalTok{(s1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 180735918
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{small\_hash}\NormalTok{(}\FunctionTok{stri\_trans\_nfc}\NormalTok{(s2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 180735918
\end{verbatim}

Now we see that the strings are equal after normalization.

\begin{rmdwarning}
This issue of text normalization can be important even if you don't use
feature hashing in your machine learning.
\end{rmdwarning}

\index{hashing function}

Since these words are encoded in different ways, they will be counted separately when we are counting token frequencies.
Representing what should be a single token in multiple ways will split the counts. This will introduce noise in the best case, and in worse cases, some tokens will fall below the cutoff when we select tokens, leading to a loss of potentially informative words.

Luckily this is easily addressed by using \texttt{stri\_trans\_nfc()} on our text columns \emph{before} starting preprocessing, or perhaps more conveniently, by using \texttt{textrecipes::step\_text\_normalize()} \emph{within} a preprocessing recipe.

\hypertarget{what-evaluation-metrics-are-appropriate}{%
\section{What evaluation metrics are appropriate?}\label{what-evaluation-metrics-are-appropriate}}

We have focused on using RMSE and \(R^2\) as metrics for our models in this chapter, the defaults in the tidymodels framework. Other metrics can also be appropriate for regression models. Another common set of regression metric options are the various flavors of mean absolute error.

If you know before you fit your model that you want to compute one or more of these metrics, you can specify them in a call to \texttt{metric\_set()}. Let's set up a tuning grid for mean absolute error (\texttt{mae}) and mean absolute percent error (\texttt{mape}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lemma\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  svm\_wf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_recipe}\NormalTok{(lemma\_rec),}
\NormalTok{  scotus\_folds,}
  \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(mae, mape)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you have already fit your model, you can still compute and explore non-default metrics as long as you saved the predictions for your resampled data sets using \texttt{control\_resamples(save\_pred\ =\ TRUE)}.

Let's go back to the first linear SVM model we tuned in Section \ref{firstregressionevaluation}, with results in \texttt{svm\_rs}. We can compute the overall mean absolute percent error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mape}\NormalTok{(year, .pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 mape    standard       0.623
\end{verbatim}

We can also compute the mean absolute percent error for each resample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mape}\NormalTok{(year, .pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 4
#>    id     .metric .estimator .estimate
#>    <chr>  <chr>   <chr>          <dbl>
#>  1 Fold01 mape    standard       0.620
#>  2 Fold02 mape    standard       0.613
#>  3 Fold03 mape    standard       0.646
#>  4 Fold04 mape    standard       0.607
#>  5 Fold05 mape    standard       0.609
#>  6 Fold06 mape    standard       0.631
#>  7 Fold07 mape    standard       0.619
#>  8 Fold08 mape    standard       0.616
#>  9 Fold09 mape    standard       0.632
#> 10 Fold10 mape    standard       0.640
\end{verbatim}

Similarly, we can do the same for the mean absolute error, which gives a result in units of the original data (years, in this case) instead of relative units.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mae}\NormalTok{(year, .pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 4
#>    id     .metric .estimator .estimate
#>    <chr>  <chr>   <chr>          <dbl>
#>  1 Fold01 mae     standard        11.9
#>  2 Fold02 mae     standard        11.7
#>  3 Fold03 mae     standard        12.4
#>  4 Fold04 mae     standard        11.7
#>  5 Fold05 mae     standard        11.6
#>  6 Fold06 mae     standard        12.1
#>  7 Fold07 mae     standard        11.9
#>  8 Fold08 mae     standard        11.8
#>  9 Fold09 mae     standard        12.1
#> 10 Fold10 mae     standard        12.3
\end{verbatim}

\begin{rmdnote}
For the full set of regression metric options, see the
\href{https://yardstick.tidymodels.org/reference/}{yardstick
documentation}.
\end{rmdnote}

\hypertarget{mlregressionfull}{%
\section{The full game: regression}\label{mlregressionfull}}

In this chapter, we started from the beginning and then explored both different types of models and different data preprocessing steps. Let's take a step back and build one final model, using everything we've learned. For our final model, let's again use a linear SVM regression model, since it performed better than the other options we looked at. We will:

\begin{itemize}
\item
  train on the same set of cross-validation resamples used throughout this chapter,
\item
  \emph{tune} the number of tokens used in the model to find a value that fits our needs,
\item
  include both unigrams and bigrams\index{tokenization!n-gram},
\item
  choose not to use lemmatization\index{lemmas}, to demonstrate what is possible for situations when training time makes lemmatization an impractical choice, and
\item
  finally evaluate on the testing set, which we have not touched at all yet.
\end{itemize}

We will include a much larger number of tokens than before, which should give us the latitude to include both unigrams and bigrams, despite the result we saw in Section \ref{casestudyngrams}.

\hypertarget{preprocess-the-data}{%
\subsection{Preprocess the data}\label{preprocess-the-data}}

First, let's create the data preprocessing recipe. By setting the tokenization options to \texttt{list(n\ =\ 2,\ n\_min\ =\ 1)}, we will include both unigrams and bigrams in our model.

When we set \texttt{max\_tokens\ =\ tune()}, we can train multiple models with different numbers of maximum tokens and then compare these models' performance to choose the best value. Before we set \texttt{max\_tokens\ =\ 1e3} to choose a specific value for the number of tokens included in our model, but here we are going to try multiple different values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(year }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ scotus\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{n =} \DecValTok{2}\NormalTok{, }\AttributeTok{n\_min =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \FunctionTok{tune}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}

\NormalTok{final\_rec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>    outcome          1
#>  predictor          1
#> 
#> Operations:
#> 
#> Tokenization for text
#> Text filtering for text
#> Term frequency-inverse document frequency with text
#> Centering and scaling for all_predictors()
\end{verbatim}

\hypertarget{specify-the-model}{%
\subsection{Specify the model}\label{specify-the-model}}

Let's use the same linear SVM regression model specification we have used multiple times in this chapter, and set it up here again to remind ourselves.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svm\_spec }\OtherTok{\textless{}{-}} \FunctionTok{svm\_linear}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"LiblineaR"}\NormalTok{)}

\NormalTok{svm\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Linear Support Vector Machine Specification (regression)
#> 
#> Computational engine: LiblineaR
\end{verbatim}

We can combine the preprocessing recipe and the model specification in a tunable workflow. We can't fit this workflow right away to training data, because the value for \texttt{max\_tokens} hasn't been chosen yet.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(final\_rec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(svm\_spec)}

\NormalTok{tune\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: svm_linear()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 4 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> * step_normalize()
#> 
#> -- Model -----------------------------------------------------------------------
#> Linear Support Vector Machine Specification (regression)
#> 
#> Computational engine: LiblineaR
\end{verbatim}

\hypertarget{tune-the-model}{%
\subsection{Tune the model}\label{tune-the-model}}

\index{models!tuning}Before we tune the model, we need to set up a set of possible parameter values to try.

\begin{rmdwarning}
There is \emph{one} tunable parameter in this model, the maximum number
of tokens included in the model.
\end{rmdwarning}

Let's include different possible values for this parameter starting from the value we've already tried, for a combination of six models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}
  \FunctionTok{max\_tokens}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\FloatTok{1e3}\NormalTok{, }\FloatTok{6e3}\NormalTok{)),}
  \AttributeTok{levels =} \DecValTok{6}
\NormalTok{)}
\NormalTok{final\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 6 x 1
#>   max_tokens
#>        <int>
#> 1       1000
#> 2       2000
#> 3       3000
#> 4       4000
#> 5       5000
#> 6       6000
\end{verbatim}

Now it's time for tuning. Instead of using \texttt{fit\_resamples()} as we have throughout this chapter, we are going to use \texttt{tune\_grid()}, a function that has a very similar set of arguments. We pass this function our workflow (which holds our preprocessing recipe and SVM model), our resampling folds, and also the grid of possible parameter values to try. Let's save the predictions so we can explore them in more detail, and let's also set custom metrics instead of using the defaults. Let's compute RMSE, mean absolute error, and mean absolute percent error during tuning.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_rs }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{  tune\_wf,}
\NormalTok{  scotus\_folds,}
  \AttributeTok{grid =}\NormalTok{ final\_grid,}
  \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(rmse, mae, mape),}
  \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}

\NormalTok{final\_rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Tuning results
#> # 10-fold cross-validation 
#> # A tibble: 10 x 5
#>    splits          id     .metrics          .notes          .predictions        
#>    <list>          <chr>  <list>            <list>          <list>              
#>  1 <split [6750/7~ Fold01 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  2 <split [6750/7~ Fold02 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  3 <split [6750/7~ Fold03 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  4 <split [6750/7~ Fold04 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  5 <split [6750/7~ Fold05 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  6 <split [6750/7~ Fold06 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  7 <split [6750/7~ Fold07 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  8 <split [6750/7~ Fold08 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#>  9 <split [6750/7~ Fold09 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
#> 10 <split [6750/7~ Fold10 <tibble[,5] [18 ~ <tibble[,1] [0~ <tibble[,5] [4,500 ~
\end{verbatim}

We trained all these models!

\hypertarget{regression-final-evaluation}{%
\subsection{Evaluate the modeling}\label{regression-final-evaluation}}

Now that all of the models with possible parameter values have been trained, we can compare their performance. Figure \ref{fig:scotusfinaltunevis} shows us the relationship between performance (as measured by the metrics we chose) and the number of tokens.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(max\_tokens, mean, }\AttributeTok{color =}\NormalTok{ .metric)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{.metric, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Number of tokens"}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Linear SVM performance across number of tokens"}\NormalTok{,}
    \AttributeTok{subtitle =} \StringTok{"Performance improves as we include more tokens"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/scotusfinaltunevis-1} 

}

\caption{Performance improves significantly at about 4000 tokens}\label{fig:scotusfinaltunevis}
\end{figure}

Since this is our final version of this model, we want to choose final parameters and update our model object so we can use it with new data. We have several options for choosing our final parameters, such as selecting the numerically best model (which would be one of the ones with the most tokens in our situation here) or the simplest model within some limit around the numerically best result. In this situation, we likely want to choose a simpler model with fewer tokens that gives close-to-best performance.

Let's choose by percent loss compared to the best model, with the default 2\% loss.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chosen\_mae }\OtherTok{\textless{}{-}}\NormalTok{ final\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_by\_pct\_loss}\NormalTok{(}\AttributeTok{metric =} \StringTok{"mae"}\NormalTok{, max\_tokens)}

\NormalTok{chosen\_mae}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 9
#>   max_tokens .metric .estimator  mean     n std_err .config          .best .loss
#>        <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>            <dbl> <dbl>
#> 1       4000 mae     standard    10.1    10   0.118 Preprocessor4_M~  10.0 0.438
\end{verbatim}

After we have those parameters, \texttt{penalty} and \texttt{max\_tokens}, we can finalize our earlier tunable workflow, by updating it with this value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_wf }\OtherTok{\textless{}{-}} \FunctionTok{finalize\_workflow}\NormalTok{(tune\_wf, chosen\_mae)}

\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: svm_linear()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 4 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> * step_normalize()
#> 
#> -- Model -----------------------------------------------------------------------
#> Linear Support Vector Machine Specification (regression)
#> 
#> Computational engine: LiblineaR
\end{verbatim}

The \texttt{final\_wf} workflow now has a finalized value for \texttt{max\_tokens}.

We can now fit this finalized workflow on training data and \emph{finally} return to our testing data.

\begin{rmdwarning}
Notice that this is the first time we have used our testing data during
this entire chapter; we compared and now tuned models using resampled
data sets instead of touching the testing set.
\end{rmdwarning}

We can use the function \texttt{last\_fit()} to \textbf{fit} our model one last time on our training data and \textbf{evaluate} it on our testing data. We only have to pass this function our finalized model/workflow and our data split.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fitted }\OtherTok{\textless{}{-}} \FunctionTok{last\_fit}\NormalTok{(final\_wf, scotus\_split)}

\FunctionTok{collect\_metrics}\NormalTok{(final\_fitted)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 4
#>   .metric .estimator .estimate .config             
#>   <chr>   <chr>          <dbl> <chr>               
#> 1 rmse    standard      13.2   Preprocessor1_Model1
#> 2 rsq     standard       0.926 Preprocessor1_Model1
\end{verbatim}

The metrics for the test set look about the same as the resampled training data and indicate we did not overfit during tuning. The RMSE of our final model has improved compared to our earlier models, both because we are combining multiple preprocessing steps and because we have tuned the number of tokens.

The output of \texttt{last\_fit()} also contains a fitted model (a \texttt{workflow}, to be more specific), that has been trained on the \emph{training} data. We can \texttt{tidy()} this final result to understand what the most important variables are in the predictions, shown in Figure \ref{fig:scotusvip}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_fit }\OtherTok{\textless{}{-}} \FunctionTok{pull\_workflow\_fit}\NormalTok{(final\_fitted}\SpecialCharTok{$}\NormalTok{.workflow[[}\DecValTok{1}\NormalTok{]])}

\NormalTok{scotus\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(term }\SpecialCharTok{!=} \StringTok{"Bias"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{sign =} \FunctionTok{case\_when}\NormalTok{(estimate }\SpecialCharTok{\textgreater{}} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{"Later (after mean year)"}\NormalTok{,}
                     \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Earlier (before mean year)"}\NormalTok{),}
    \AttributeTok{estimate =} \FunctionTok{abs}\NormalTok{(estimate),}
    \AttributeTok{term =} \FunctionTok{str\_remove\_all}\NormalTok{(term, }\StringTok{"tfidf\_text\_"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(sign) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{20}\NormalTok{, estimate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ estimate,}
             \AttributeTok{y =} \FunctionTok{fct\_reorder}\NormalTok{(term, estimate),}
             \AttributeTok{fill =}\NormalTok{ sign)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sign, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{title =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Variable importance for predicting year of"}\NormalTok{,}
                  \StringTok{"Supreme Court opinions"}\NormalTok{),}
    \AttributeTok{subtitle =} \FunctionTok{paste}\NormalTok{(}\StringTok{"These features are the most importance"}\NormalTok{,}
                     \StringTok{"in predicting the year of an opinion"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/scotusvip-1} 

}

\caption{Some words or bigrams increase a Supreme Court opinion's probability of being written later (more recently) while some increase its probability of being written earlier}\label{fig:scotusvip}
\end{figure}

The tokens (unigrams or bigrams) that contribute in the positive direction, like ``court said'' and ``testified'', are associated with higher, later years and those that contribute in the negative direction, like ``ought'' and ``consequently'', are associated with lower, earlier years for these Supreme Court opinions.

\begin{rmdnote}
Some of these features are unigrams and some are bigrams, and stop words
are included because we did not remove them from the model.
\end{rmdnote}

We can also examine how the true and predicted years compare for the testing set. Figure \ref{fig:scotusfinalpredvis} shows us that, like for our earlier models on the resampled training data, we can predict the year of Supreme Court opinions for the testing data starting from about 1850. Predictions are less reliable before that year. This is an example of finding different error rates across sub-groups of observations, like we discussed in the foreword to these chapters; these differences can lead to unfairness and algorithmic bias\index{bias} when models are applied in the real world.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fitted }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(year, .pred)) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{color =} \StringTok{"gray80"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Truth"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Predicted year"}\NormalTok{,}
    \AttributeTok{title =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Predicted and true years for the testing set of"}\NormalTok{,}
                  \StringTok{"Supreme Court opinions"}\NormalTok{),}
    \AttributeTok{subtitle =} \StringTok{"For the testing set, predictions are more reliable after 1850"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{06_ml_regression_files/figure-latex/scotusfinalpredvis-1} 

}

\caption{Predicted and true years from a linear SVM regression model with bigrams and unigrams}\label{fig:scotusfinalpredvis}
\end{figure}

Finally, we can gain more insight into our model and how it is behaving by looking at observations from the test set that have been \emph{mispredicted}. Let's bind together the predictions on the test set with the original Supreme Court opinion test data and filter to observations with a prediction that is more than 25 years wrong.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_bind }\OtherTok{\textless{}{-}} \FunctionTok{collect\_predictions}\NormalTok{(final\_fitted) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(scotus\_test }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{year, }\SpecialCharTok{{-}}\NormalTok{id)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{abs}\NormalTok{(year }\SpecialCharTok{{-}}\NormalTok{ .pred) }\SpecialCharTok{\textgreater{}} \DecValTok{25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There isn't too much training data to start with for the earliest years, so we are unlikely to quickly gain insight from looking at the oldest opinions. However, what do the more recent opinions that were predicted inaccurately look like?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_bind }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{year) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(year, .pred, case\_name, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 137 x 4
#>     year .pred case_name                       text                             
#>    <dbl> <dbl> <chr>                           <chr>                            
#>  1  2008 1982. Sprint Communications Co. v. A~ "Supreme Court of United States.~
#>  2  2004 1978. BedRoc Limited, LLC v. United ~ "No. 02-1593.\nThe Pittman Under~
#>  3  2004 1978. Sosa v. Alvarez-Machain         "No. 03-339.\nThe Drug Enforceme~
#>  4  2002 1954. JPMorgan Chase Bank v. Traffic~ "No. 01-651.\nCERTIORARI TO THE ~
#>  5  1996 2021. Bank One Chicago, NA v. Midwes~ "No. 94-1175.\n\n        Syllabu~
#>  6  1995 1959. Johnny Paul Penry v. Texas. No~ "Justice SCALIA, Circuit Justice~
#>  7  1993 1964. Tennessee v. Middlebrooks       "No. 92-989.\nCERTIORARI TO THE ~
#>  8  1992 2021. Martin v. District of Columbia~ "Nos. 92-5584, 92-5618.\nReheari~
#>  9  1992 1966. INS v. Elias-Zacarias           "No. 90-1342.\n\n        Syllabu~
#> 10  1992 1966. Republic Nat. Bank of Miami v.~ "No. 91-767.\nSyllabus*\nThe Gov~
#> # ... with 127 more rows
\end{verbatim}

There are some interesting examples here where we can understand why the model would mispredict:

\begin{itemize}
\item
  \emph{BedRoc Limited, LLC v. United States} was a case decided in 2004 regarding the 1919 Pittman Act.
\item
  The case written by Antonin Scalia functioning as a circuit justice during his time on the Supreme Court is confusing, given that he served as a federal judge on the D.C. Circuit Court of Appeals earlier and appears in the training set as such.
\end{itemize}

\begin{rmdwarning}
Looking at examples that your model does not perform well for is well
worth your time, for similar reasons that exploratory data analysis is
valuable before you begin training your model.
\end{rmdwarning}

\hypertarget{mlregressionsummary}{%
\section{Summary}\label{mlregressionsummary}}

You can use regression modeling to predict a continuous variable from a data set, including a text data set. Linear support vector machine models, along with regularized linear models (which we will cover in the next chapter), often work well for text data sets, while tree-based models such as random forest often behave poorly in practice. There are many possible \index{preprocessing}preprocessing steps for text data, from removing stop words to \index{tokenization!n-gram}n-gram tokenization strategies to \index{lemmas}lemmatization, that may improve your model. Resampling data sets and careful use of metrics allow you to make good choices among these possible options, given your own concerns and priorities.

\hypertarget{in-this-chapter-you-learned-5}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-5}}

\begin{itemize}
\item
  what kind of quantities can be modeled using regression
\item
  to evaluate a model using resampled data
\item
  how to compare different model types
\item
  about measuring the impact of n-gram tokenization on models
\item
  how to implement lemmatization and stop word removal with text models
\item
  how feature hashing can be used as a fast alternative to bag-of-words
\item
  about performance metrics for regression models
\end{itemize}

\hypertarget{mlclassification}{%
\chapter{Classification}\label{mlclassification}}

In Chapter \ref{mlregression}, we focused on modeling to predict \emph{continuous values} for documents, such as what year a Supreme Court opinion was published. This is an example of a regression model. We can also use machine learning to predict \emph{labels} on documents using a classification model. For both types of prediction questions, we develop a learner or model to describe the relationship between a target or outcome variable and our input features; what is different about a classification model is the nature of that outcome.

\begin{itemize}
\item
  A \textbf{regression model} predicts a numeric or continuous value.
\item
  A \textbf{classification model} predicts a class label or group membership.
\end{itemize}

For our classification example in this chapter, let's consider the data set of consumer complaints submitted to the US Consumer Finance Protection Bureau. Let's read in the complaint data (Section \ref{cfpb-complaints}) with \texttt{read\_csv()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{complaints }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/complaints.csv.gz"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can start by taking a quick \texttt{glimpse()} at the data to see what we have to work with. This data set contains a text field with the complaint, along with information regarding what it was for,
how and when it was filed, and the response from the bureau.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(complaints)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 117,214
#> Columns: 18
#> $ date_received                <date> 2019-09-24, 2019-10-25, 2019-11-08, 2019~
#> $ product                      <chr> "Debt collection", "Credit reporting, cre~
#> $ sub_product                  <chr> "I do not know", "Credit reporting", "I d~
#> $ issue                        <chr> "Attempts to collect debt not owed", "Inc~
#> $ sub_issue                    <chr> "Debt is not yours", "Information belongs~
#> $ consumer_complaint_narrative <chr> "transworld systems inc. \nis trying to c~
#> $ company_public_response      <chr> NA, "Company has responded to the consume~
#> $ company                      <chr> "TRANSWORLD SYSTEMS INC", "TRANSUNION INT~
#> $ state                        <chr> "FL", "CA", "NC", "RI", "FL", "TX", "SC",~
#> $ zip_code                     <chr> "335XX", "937XX", "275XX", "029XX", "333X~
#> $ tags                         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
#> $ consumer_consent_provided    <chr> "Consent provided", "Consent provided", "~
#> $ submitted_via                <chr> "Web", "Web", "Web", "Web", "Web", "Web",~
#> $ date_sent_to_company         <date> 2019-09-24, 2019-10-25, 2019-11-08, 2019~
#> $ company_response_to_consumer <chr> "Closed with explanation", "Closed with e~
#> $ timely_response              <chr> "Yes", "Yes", "Yes", "Yes", "Yes", "Yes",~
#> $ consumer_disputed            <chr> "N/A", "N/A", "N/A", "N/A", "N/A", "N/A",~
#> $ complaint_id                 <dbl> 3384392, 3417821, 3433198, 3366475, 33853~
\end{verbatim}

In this chapter, we will build classification models to predict what type of financial \texttt{product} the complaints are referring to, i.e., a label or categorical variable. The goal of predictive modeling with text input features and a categorical outcome is to learn and model the relationship between those input features, typically created through steps as outlined in Chapters \ref{language} through \ref{embeddings}, and the class label or categorical outcome. Most classification models do predict the probability of a class (a numeric output), but the particular characteristics of this output make classification models different enough from regression models that we handle them differently.

\hypertarget{classfirstattemptlookatdata}{%
\section{A first classification model}\label{classfirstattemptlookatdata}}

For our first model, let's build a binary classification model to predict whether a submitted complaint is about ``Credit reporting, credit repair services, or other personal consumer reports'' or not.

\begin{rmdnote}
This kind of ``yes or no'' binary classification model is both common
and useful in real-world text machine learning problems.
\end{rmdnote}

The outcome variable \texttt{product} contains more categories than this, so we need to transform this variable to only contains the values ``Credit reporting, credit repair services, or other personal consumer reports'' and ``Other''.

It is always a good idea to look at your data! Here are the first six complaints:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(complaints}\SpecialCharTok{$}\NormalTok{consumer\_complaint\_narrative)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "transworld systems inc. \nis trying to collect a debt that is not mine,
not owed and is inaccurate."
#> [2] "I would like to request the suppression of the following items from my
credit report, which are the result of my falling victim to identity theft.
This information does not relate to [ transactions that I have made/accounts
that I have opened ], as the attached supporting documentation can attest. As
such, it should be blocked from appearing on my credit report pursuant to
section 605B of the Fair Credit Reporting Act."
#> [3] "Over the past 2 weeks, I have been receiving excessive amounts of
telephone calls from the company listed in this complaint. The calls occur
between XXXX XXXX and XXXX XXXX to my cell and at my job. The company does not
have the right to harass me at work and I want this to stop. It is extremely
distracting to be told 5 times a day that I have a call from this collection
agency while at work."
#> [4] "I was sold access to an event digitally, of which I have all the
screenshots to detail the transactions, transferred the money and was provided
with only a fake of a ticket. I have reported this to paypal and it was for the
amount of {$21.00} including a {$1.00} fee from paypal. \n\nThis occured on
XX/XX/2019, by paypal user who gave two accounts : 1 ) XXXX 2 ) XXXX XXXX"
#> [5] "While checking my credit report I noticed three collections by a
company called ARS that i was unfamiliar with. I disputed these collections
with XXXX, and XXXX and they both replied that they contacted the creditor and
the creditor verified the debt so I asked for proof which both bureaus replied
that they are not required to prove anything. I then mailed a certified letter
to ARS requesting proof of the debts n the form of an original aggrement, or a
proof of a right to the debt, or even so much as the process as to how the bill
was calculated, to which I was simply replied a letter for each collection
claim that listed my name an account number and an amount with no other
information to verify the debts after I sent a clear notice to provide me
evidence. Afterwards I recontacted both XXXX, and XXXX, to redispute on the
premise that it is not my debt if evidence can not be drawn up, I feel as if I
am being personally victimized by ARS on my credit report for debts that are
not owed to them or any party for that matter, and I feel discouraged that the
credit bureaus who control many aspects of my personal finances are so
negligent about my information."
#> [6] "I would like the credit bureau to correct my XXXX XXXX XXXX XXXX
balance. My correct balance is XXXX"
\end{verbatim}

The complaint narratives contain many series of capital \texttt{"X"}'s. These strings (like ``XX/XX'' or ``XXXX XXXX XXXX XXXX'') are used to to protect personally identifiable information (PII)\index{PII}\index{personally identifiable information|see {PII}} in this publicly available data set. This is not a universal censoring mechanism; censoring and PII protection will vary from source to source. Hopefully you will be able to find information on PII\index{PII} censoring\index{censoring} in a data dictionary, but you should always look at the data yourself to verify.

We also see that monetary amounts are surrounded by curly brackets (like \texttt{"\{\$21.00\}"}); this is another text preprocessing\index{preprocessing} step that has been taken care of for us. We could craft a regular expression\index{regex} to extract all the dollar amounts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints}\SpecialCharTok{$}\NormalTok{consumer\_complaint\_narrative }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{str\_extract\_all}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$[0{-}9}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.]*}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compact}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "{$21.00}" "{$1.00}" 
#> 
#> [[2]]
#> [1] "{$2300.00}"
#> 
#> [[3]]
#> [1] "{$200.00}"  "{$5000.00}" "{$5000.00}" "{$770.00}"  "{$800.00}" 
#> [6] "{$5000.00}"
#> 
#> [[4]]
#> [1] "{$15000.00}" "{$11000.00}" "{$420.00}"   "{$15000.00}"
#> 
#> [[5]]
#> [1] "{$0.00}" "{$0.00}" "{$0.00}" "{$0.00}"
#> 
#> [[6]]
#> [1] "{$650.00}"
\end{verbatim}

In Section \ref{customfeatures}, we will use an approach like this for custom feature engineering from the text.

\hypertarget{classfirstmodel}{%
\subsection{Building our first classification model}\label{classfirstmodel}}

This data set includes more possible predictors than the text alone, but for this first model we will only use the text variable \texttt{consumer\_complaint\_narrative}.
Let's create a factor outcome variable \texttt{product} with two levels, ``Credit'' and ``Other''.
Then, we split the data into training and testing data sets.
We can use the \texttt{initial\_split()} function from \textbf{rsample} to create this binary split of the data.
The \texttt{strata} argument ensures that the distribution of \texttt{product} is similar in the training set and testing set.
Since the split uses random sampling, we set a seed so we can reproduce our results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{complaints2class }\OtherTok{\textless{}{-}}\NormalTok{ complaints }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{product =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{if\_else}\NormalTok{(}
\NormalTok{    product }\SpecialCharTok{==} \FunctionTok{paste}\NormalTok{(}\StringTok{"Credit reporting, credit repair services,"}\NormalTok{,}
                     \StringTok{"or other personal consumer reports"}\NormalTok{),}
    \StringTok{"Credit"}\NormalTok{, }\StringTok{"Other"}
\NormalTok{  )))}

\NormalTok{complaints\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(complaints2class, }\AttributeTok{strata =}\NormalTok{ product)}

\NormalTok{complaints\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(complaints\_split)}
\NormalTok{complaints\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(complaints\_split)}
\end{Highlighting}
\end{Shaded}

The dimensions of the two splits show that this first step worked as we planned.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(complaints\_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 87911    18
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(complaints\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 29303    18
\end{verbatim}

Next we need to preprocess\index{preprocessing} this data to prepare it for modeling; we have text data, and we need to build numeric features for machine learning from that text.

The \textbf{recipes} package, part of tidymodels, allows us to create a specification of preprocessing steps we want to perform. These transformations are estimated (or ``trained'') on the training set so that they can be applied in the same way on the testing set or new data at prediction time, without data leakage.
We initialize our set of preprocessing transformations with the \texttt{recipe()} function, using a formula expression to specify the variables, our outcome plus our predictor, along with the data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(product }\SpecialCharTok{\textasciitilde{}}\NormalTok{ consumer\_complaint\_narrative, }\AttributeTok{data =}\NormalTok{ complaints\_train)}
\end{Highlighting}
\end{Shaded}

Now we add steps to process the text of the complaints; we use \textbf{textrecipes} to handle the \texttt{consumer\_complaint\_narrative} variable. First we tokenize the text to words with \texttt{step\_tokenize()}. By default this uses \texttt{tokenizers::tokenize\_words()}.
Before we calculate tf-idf\index{tf-idf} we use \texttt{step\_tokenfilter()} to only keep the 1000 most frequent tokens, to avoid creating too many variables in our first model. To finish, we use \texttt{step\_tfidf()} to compute tf-idf.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textrecipes)}

\NormalTok{complaints\_rec }\OtherTok{\textless{}{-}}\NormalTok{ complaints\_rec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(consumer\_complaint\_narrative, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(consumer\_complaint\_narrative)}
\end{Highlighting}
\end{Shaded}

Now that we have a full specification of the preprocessing recipe, we can build up a tidymodels \texttt{workflow()} to bundle together our modeling components.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaint\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(complaints\_rec)}
\end{Highlighting}
\end{Shaded}

Let's start with a naive Bayes model \citep{kim2006, Kibriya2005, Eibe2006}, which is available in the tidymodels package \textbf{discrim}.
One of the main advantages of a naive Bayes model is its ability to handle a large number of features, such as those we deal with when using word count methods.
Here we have only kept the 1000 most frequent tokens, but we could have kept more tokens and a naive Bayes model would still be able to handle such predictors well. For now, we will limit the model to a moderate number of tokens.

\begin{rmdpackage}
In \textbf{tidymodels}, the package for creating model specifications is
\textbf{parsnip} {[}@R-parsnip{]}. The \textbf{parsnip} package provides
the functions for creating all the models we have used so far, but other
extra packages provide more. The \textbf{discrim} package is an
extension package for \textbf{parsnip} that contains model definitions
for various discriminant analysis models, including naive Bayes.
\end{rmdpackage}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(discrim)}
\NormalTok{nb\_spec }\OtherTok{\textless{}{-}} \FunctionTok{naive\_Bayes}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"naivebayes"}\NormalTok{)}

\NormalTok{nb\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Naive Bayes Model Specification (classification)
#> 
#> Computational engine: naivebayes
\end{verbatim}

Now we have everything we need to fit our first classification model. We can add the naive Bayes model to our workflow, and then we can fit this workflow to our training data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_fit }\OtherTok{\textless{}{-}}\NormalTok{ complaint\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(nb\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ complaints\_train)}
\end{Highlighting}
\end{Shaded}

We have trained our first classification model!

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

Like we discussed in Section \ref{firstregressionevaluation}, we should not use the test set to compare models or different model parameters. The test set is a precious resource that should only be used at the end of the model training process to estimate performance on new data. Instead, we will use \emph{resampling} methods to evaluate our model.

Let's use resampling to estimate the performance of the naive Bayes classification model we just fit. We can do this using resampled data sets built from the training set. Let's create cross 10-fold cross-validation sets, and use these resampled sets for performance estimates.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{234}\NormalTok{)}
\NormalTok{complaints\_folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(complaints\_train)}

\NormalTok{complaints\_folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> #  10-fold cross-validation 
#> # A tibble: 10 x 2
#>    splits               id    
#>    <list>               <chr> 
#>  1 <split [79119/8792]> Fold01
#>  2 <split [79120/8791]> Fold02
#>  3 <split [79120/8791]> Fold03
#>  4 <split [79120/8791]> Fold04
#>  5 <split [79120/8791]> Fold05
#>  6 <split [79120/8791]> Fold06
#>  7 <split [79120/8791]> Fold07
#>  8 <split [79120/8791]> Fold08
#>  9 <split [79120/8791]> Fold09
#> 10 <split [79120/8791]> Fold10
\end{verbatim}

Each of these splits contains information about how to create cross-validation folds from the original training data. In this example, 90\% of the training data is included in each fold and the other 10\% is held out for evaluation.

For convenience, let's again use a \texttt{workflow()} for our resampling estimates of performance.

\begin{rmdwarning}
Using a \texttt{workflow()} isn't required (you can fit or tune a model
plus a preprocessor) but it can make your code easier to read and
organize.
\end{rmdwarning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(complaints\_rec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(nb\_spec)}

\NormalTok{nb\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: naive_Bayes()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 3 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> 
#> -- Model -----------------------------------------------------------------------
#> Naive Bayes Model Specification (classification)
#> 
#> Computational engine: naivebayes
\end{verbatim}

In the last section, we fit one time to the training data as a whole. Now, to estimate how well that model performs, let's fit the model many times, once to each of these resampled folds, and then evaluate on the heldout part of each resampled fold.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  nb\_wf,}
\NormalTok{  complaints\_folds,}
  \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can extract the relevant information using \texttt{collect\_metrics()} and \texttt{collect\_predictions()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_rs\_metrics }\OtherTok{\textless{}{-}} \FunctionTok{collect\_metrics}\NormalTok{(nb\_rs)}
\NormalTok{nb\_rs\_predictions }\OtherTok{\textless{}{-}} \FunctionTok{collect\_predictions}\NormalTok{(nb\_rs)}
\end{Highlighting}
\end{Shaded}

What results do we see, in terms of performance metrics?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_rs\_metrics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric  .estimator  mean     n  std_err .config             
#>   <chr>    <chr>      <dbl> <int>    <dbl> <chr>               
#> 1 accuracy binary     0.806    10 0.00184  Preprocessor1_Model1
#> 2 roc_auc  binary     0.878    10 0.000715 Preprocessor1_Model1
\end{verbatim}

The default performance parameters for binary classification are accuracy and ROC AUC (area under the receiver operator characteristic curve). For these resamples, the average accuracy is 80.6\%.

\index{accuracy}
\index{ROC AUC}
\index{area under the receiver operator characteristic curve|see {ROC AUC}}

\begin{rmdnote}
Accuracy and ROC AUC are performance metrics used for classification
models. For both, values closer to 1 are better.

Accuracy is the proportion of the data that are predicted correctly. Be
aware that accuracy can be misleading in some situations, such as for
imbalanced data sets.

ROC AUC measures how well a classifier performs at different thresholds.
The ROC curve plots the true positive rate against the false positive
rate, and AUC closer to 1 indicates a better-performing model while AUC
closer to 0.5 indicates a model that does no better than random
guessing.
\end{rmdnote}

Figure \ref{fig:firstroccurve} shows the ROC curve, a visualization of how well a classification model can distinguish between classes, for our first classification model on each of the resampled data sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_rs\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ product, .pred\_Credit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{color =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"ROC curve for US Consumer Finance Complaints"}\NormalTok{,}
    \AttributeTok{subtitle =} \StringTok{"Each resample fold is shown in a different color"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/firstroccurve-1} 

}

\caption{ROC curve for naive Bayes classifier with resamples of US Consumer Finance Bureau complaints}\label{fig:firstroccurve}
\end{figure}

The area under each of these curves is the \texttt{roc\_auc} metric we have computed. If the curve was close to the diagonal line, then the model's predictions would be no better than random guessing.

Another way to evaluate our model is to evaluate the \index{matrix!confusion}confusion matrix. A confusion matrix tabulates a model's false positives and false negatives for each class.
The function \texttt{conf\_mat\_resampled()} computes a separate confusion matrix for each resample and takes the average of the cell counts. This allows us to visualize an overall confusion matrix rather than needing to examine each resample individually.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf\_mat\_resampled}\NormalTok{(nb\_rs, }\AttributeTok{tidy =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/firstheatmap-1} 

}

\caption{Confusion matrix for naive Bayes classifier, showing some bias towards predicting 'Credit'}\label{fig:firstheatmap}
\end{figure}

In \index{matrix!confusion}Figure \ref{fig:firstheatmap}, the squares for ``Credit''/``Credit'' and ``Other''/``Other'' have a darker shade than the off diagonal squares. This is a good sign, meaning that our model is right more often than not! However, this first model is struggling somewhat since many observations from the ``Credit'' class are being mispredicted as ``Other''.

\begin{rmdwarning}
One metric alone cannot give you a complete picture of how well your
classification model is performing. The confusion matrix is a good
starting point to get an overview of your model performance, as it
includes rich information.
\end{rmdwarning}

\index{matrix!confusion}

This is real data from a government agency, and these kinds of performance metrics must be interpreted in the context of how such a model would be used. What happens if the model we trained gets a classification wrong for a consumer complaint? What impact will it have if more ``Other'' complaints are correctly identified than ``Credit'' complaints, either for consumers or for policymakers?

\hypertarget{classnull}{%
\section{Compare to the null model}\label{classnull}}

Like we did in Section \ref{regnull}, we can assess a model like this one by comparing its performance to a ``null model'' or baseline model, a simple, non-informative model that always predicts the largest class for classification. Such a model is perhaps the simplest heuristic or rule-based alternative that we can consider as we assess our modeling efforts.

We can build a classification \texttt{null\_model()} specification and add it to a \texttt{workflow()} with the same preprocessing recipe we used in the previous section, to estimate performance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_classification }\OtherTok{\textless{}{-}} \FunctionTok{null\_model}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"parsnip"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}

\NormalTok{null\_rs }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(complaints\_rec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(null\_classification) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{    complaints\_folds}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

What results do we obtain from the null model, in terms of performance metrics?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric  .estimator  mean     n std_err .config             
#>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               
#> 1 accuracy binary     0.526    10 0.00149 Preprocessor1_Model1
#> 2 roc_auc  binary     0.5      10 0       Preprocessor1_Model1
\end{verbatim}

The accuracy and ROC AUC indicate that this null model is, like in the regression case, dramatically worse than even our first model. The text of the CFPB complaints is predictive relative to the category we are building models for.

\hypertarget{comparetolasso}{%
\section{Compare to a lasso classification model}\label{comparetolasso}}

Regularized linear models are a class of statistical model that can be used in regression and classification tasks. Linear models are not considered cutting edge in NLP research, but are a \index{models!in production}workhorse in real-world practice. Here we will use a lasso regularized model \citep{Tibshirani1996}, where the regularization method also performs variable selection. In text analysis, we typically have many tokens, which are the features in our machine learning problem.

\begin{rmdnote}
Using regularization helps us choose a simpler model that we expect to
generalize better to new observations, and variable selection helps us
identify which features to include in our model.
\end{rmdnote}

Lasso regression or classification learns how much of a \emph{penalty} to put on some features (sometimes penalizing all the way down to zero) so that we can select only some features out of the high-dimensional space of original possible variables (tokens) for the final model.

Let's create a specification of lasso regularized model. Remember that in tidymodels, specifying a model has three components: the algorithm, the mode, and the computational engine.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso\_spec }\OtherTok{\textless{}{-}} \FunctionTok{logistic\_reg}\NormalTok{(}\AttributeTok{penalty =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{mixture =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glmnet"}\NormalTok{)}

\NormalTok{lasso\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = 0.01
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

Then we can create another \texttt{workflow()} object with the lasso specification. Notice that we can reuse our text preprocessing recipe.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(complaints\_rec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(lasso\_spec)}

\NormalTok{lasso\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = 0.01
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

Now we estimate the performance of this first lasso classification model with \texttt{fit\_resamples()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2020}\NormalTok{)}
\NormalTok{lasso\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  lasso\_wf,}
\NormalTok{  complaints\_folds,}
  \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's again extract the relevant information using \texttt{collect\_metrics()} and \texttt{collect\_predictions()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso\_rs\_metrics }\OtherTok{\textless{}{-}} \FunctionTok{collect\_metrics}\NormalTok{(lasso\_rs)}
\NormalTok{lasso\_rs\_predictions }\OtherTok{\textless{}{-}} \FunctionTok{collect\_predictions}\NormalTok{(lasso\_rs)}
\end{Highlighting}
\end{Shaded}

Now we can see that \texttt{lasso\_rs\_metrics} contains the same default performance metrics we have been using so far in this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso\_rs\_metrics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   .metric  .estimator  mean     n  std_err .config             
#>   <chr>    <chr>      <dbl> <int>    <dbl> <chr>               
#> 1 accuracy binary     0.868    10 0.000977 Preprocessor1_Model1
#> 2 roc_auc  binary     0.939    10 0.000849 Preprocessor1_Model1
\end{verbatim}

This looks pretty promising, considering we haven't yet done any tuning of the lasso hyperparameters.
Figure \ref{fig:lassoroccurve} shows the ROC curves for this regularized model on each of the resampled data sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso\_rs\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ product, .pred\_Credit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{color =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"ROC curve for US Consumer Finance Complaints"}\NormalTok{,}
    \AttributeTok{subtitle =} \StringTok{"Each resample fold is shown in a different color"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/lassoroccurve-1} 

}

\caption{ROC curve for lasso regularized classifier with resamples of US Consumer Finance Bureau complaints}\label{fig:lassoroccurve}
\end{figure}

Let's finish this section by generating a confusion matrix\index{matrix!confusion}, shown in Figure \ref{fig:lassoheatmap}.
Our lasso model is better at separating the classes than the naive Bayes model in Section \ref{classfirstmodel}, and our results are more symmetrical than those for the naive Bayes model in Figure \ref{fig:firstheatmap}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf\_mat\_resampled}\NormalTok{(lasso\_rs, }\AttributeTok{tidy =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/lassoheatmap-1} 

}

\caption{Confusion matrix for a lasso regularized classifier, with more symmetric results}\label{fig:lassoheatmap}
\end{figure}

\hypertarget{tunelasso}{%
\section{Tuning lasso hyperparameters}\label{tunelasso}}

\index{models!tuning}The value \texttt{penalty\ =\ 0.01} for regularization in Section \ref{comparetolasso} was picked somewhat arbitrarily. How do we know the \emph{right} or \emph{best} regularization parameter penalty? This is a model hyperparameter and we cannot learn its best value during model training, but we can estimate the best value by training many models on resampled data sets and exploring how well all these models perform. Let's build a new model specification for \textbf{model tuning}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune\_spec }\OtherTok{\textless{}{-}} \FunctionTok{logistic\_reg}\NormalTok{(}\AttributeTok{penalty =} \FunctionTok{tune}\NormalTok{(), }\AttributeTok{mixture =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glmnet"}\NormalTok{)}

\NormalTok{tune\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

After the tuning process, we can select a single best numeric value.

\begin{rmdnote}
Think of \texttt{tune()} here as a placeholder for the regularization
penalty.
\end{rmdnote}

We can create a regular grid of values to try, using a convenience function for \texttt{penalty()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{penalty}\NormalTok{(), }\AttributeTok{levels =} \DecValTok{30}\NormalTok{)}
\NormalTok{lambda\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 30 x 1
#>     penalty
#>       <dbl>
#>  1 1.00e-10
#>  2 2.21e-10
#>  3 4.89e-10
#>  4 1.08e- 9
#>  5 2.40e- 9
#>  6 5.30e- 9
#>  7 1.17e- 8
#>  8 2.59e- 8
#>  9 5.74e- 8
#> 10 1.27e- 7
#> # ... with 20 more rows
\end{verbatim}

The function \texttt{grid\_regular()} is from the \textbf{dials} package. It chooses sensible values to try for a parameter like the regularization penalty; here, we asked for 30 different possible values.

Now it is time to tune! Let's use \texttt{tune\_grid()} to fit a model at each of the values for the regularization penalty in our regular grid.

\begin{rmdpackage}
In \textbf{tidymodels}, the package for tuning is called \textbf{tune}.
Tuning a model uses a similar syntax compared to fitting a model to a
set of resampled data sets for the purposes of evaluation
(\texttt{fit\_resamples()}) because the two tasks are so similar. The
difference is that when you tune, each model that you fit has
\emph{different} parameters and you want to find the best one.
\end{rmdpackage}

We add our tunable model specification \texttt{tune\_spec} to a workflow with the same preprocessing recipe we've been using so far, and then fit it to every possible parameter in \texttt{lambda\_grid} and every resample in \texttt{complaints\_folds} with \texttt{tune\_grid()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(complaints\_rec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(tune\_spec)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2020}\NormalTok{)}
\NormalTok{tune\_rs }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{  tune\_wf,}
\NormalTok{  complaints\_folds,}
  \AttributeTok{grid =}\NormalTok{ lambda\_grid,}
  \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}

\NormalTok{tune\_rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Tuning results
#> # 10-fold cross-validation 
#> # A tibble: 10 x 5
#>    splits          id     .metrics         .notes          .predictions         
#>    <list>          <chr>  <list>           <list>          <list>               
#>  1 <split [79119/~ Fold01 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,760~
#>  2 <split [79120/~ Fold02 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#>  3 <split [79120/~ Fold03 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#>  4 <split [79120/~ Fold04 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#>  5 <split [79120/~ Fold05 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#>  6 <split [79120/~ Fold06 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#>  7 <split [79120/~ Fold07 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#>  8 <split [79120/~ Fold08 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#>  9 <split [79120/~ Fold09 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
#> 10 <split [79120/~ Fold10 <tibble[,5] [60~ <tibble[,1] [0~ <tibble[,7] [263,730~
\end{verbatim}

\begin{rmdwarning}
Like when we used \texttt{fit\_resamples()}, tuning in tidymodels can
use multiple cores or multiple machines via parallel processing, because
the resampled data sets and possible parameters are independent of each
other. A discussion of parallel processing for all possible operating
systems is beyond the scope of this book, but it is well worth your time
to learn how to parallelize your machine learning tasks on \emph{your}
system.
\end{rmdwarning}

\index{computational speed}

Now, instead of one set of metrics, we have a set of metrics for each value of the regularization penalty.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_metrics}\NormalTok{(tune\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 60 x 7
#>     penalty .metric  .estimator  mean     n  std_err .config              
#>       <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>                
#>  1 1.00e-10 accuracy binary     0.890    10 0.00102  Preprocessor1_Model01
#>  2 1.00e-10 roc_auc  binary     0.952    10 0.000823 Preprocessor1_Model01
#>  3 2.21e-10 accuracy binary     0.890    10 0.00102  Preprocessor1_Model02
#>  4 2.21e-10 roc_auc  binary     0.952    10 0.000823 Preprocessor1_Model02
#>  5 4.89e-10 accuracy binary     0.890    10 0.00102  Preprocessor1_Model03
#>  6 4.89e-10 roc_auc  binary     0.952    10 0.000823 Preprocessor1_Model03
#>  7 1.08e- 9 accuracy binary     0.890    10 0.00102  Preprocessor1_Model04
#>  8 1.08e- 9 roc_auc  binary     0.952    10 0.000823 Preprocessor1_Model04
#>  9 2.40e- 9 accuracy binary     0.890    10 0.00102  Preprocessor1_Model05
#> 10 2.40e- 9 roc_auc  binary     0.952    10 0.000823 Preprocessor1_Model05
#> # ... with 50 more rows
\end{verbatim}

Let's visualize these metrics, accuracy and ROC AUC, in Figure \ref{fig:complaintstunevis} to see what the best model is.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(tune\_rs) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Lasso model performance across regularization penalties"}\NormalTok{,}
    \AttributeTok{subtitle =} \StringTok{"Performance metrics can be used to identity the best penalty"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/complaintstunevis-1} 

}

\caption{We can identify the best regularization penalty from model performance metrics, for example, at the highest ROC AUC. Note the logarithmic scale for the regularization penalty.}\label{fig:complaintstunevis}
\end{figure}

We can view the best results with \texttt{show\_best()} and a choice for the metric, such as ROC AUC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tune\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 5 x 7
#>        penalty .metric .estimator  mean     n  std_err .config              
#>          <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>                
#> 1 0.000356     roc_auc binary     0.953    10 0.000824 Preprocessor1_Model20
#> 2 0.000788     roc_auc binary     0.953    10 0.000827 Preprocessor1_Model21
#> 3 0.000161     roc_auc binary     0.953    10 0.000822 Preprocessor1_Model19
#> 4 0.0000728    roc_auc binary     0.953    10 0.000821 Preprocessor1_Model18
#> 5 0.0000000001 roc_auc binary     0.952    10 0.000823 Preprocessor1_Model01
\end{verbatim}

The best value for ROC AUC from this tuning run is 0.953. We can extract the best regularization parameter for this value of ROC AUC from our tuning results with \texttt{select\_best()}, or a simpler model with higher regularization with \texttt{select\_by\_pct\_loss()} or \texttt{select\_by\_one\_std\_err()} Let's choose the model with the best ROC AUC within one standard error of the numerically best model \citep{Breiman1984}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chosen\_auc }\OtherTok{\textless{}{-}}\NormalTok{ tune\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_by\_one\_std\_err}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{, }\SpecialCharTok{{-}}\NormalTok{penalty)}

\NormalTok{chosen\_auc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 9
#>    penalty .metric .estimator  mean     n  std_err .config          .best .bound
#>      <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>            <dbl>  <dbl>
#> 1 0.000788 roc_auc binary     0.953    10 0.000827 Preprocessor1_M~ 0.953  0.952
\end{verbatim}

Next, let's finalize our tunable workflow with this particular regularization penalty. This is the regularization penalty that our tuning results indicate give us the best model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{finalize\_workflow}\NormalTok{(tune\_wf, chosen\_auc)}

\NormalTok{final\_lasso}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: logistic_reg()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 3 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> 
#> -- Model -----------------------------------------------------------------------
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = 0.000788046281566992
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

Instead of \texttt{penalty\ =\ tune()} like before, now our workflow has finalized values for all arguments. The preprocessing recipe has been evaluated on the training data, and we tuned the regularization penalty so that we have a penalty value of 0.00079. This workflow is ready to go! It can now be fit to our training data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitted\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{fit}\NormalTok{(final\_lasso, complaints\_train)}
\end{Highlighting}
\end{Shaded}

What does the result look like? We can access the fit using \texttt{pull\_workflow\_fit()}, and even \texttt{tidy()} the model coefficient results into a convenient dataframe format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitted\_lasso }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull\_workflow\_fit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{estimate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1,001 x 3
#>    term                                         estimate  penalty
#>    <chr>                                           <dbl>    <dbl>
#>  1 tfidf_consumer_complaint_narrative_funds         26.5 0.000788
#>  2 tfidf_consumer_complaint_narrative_appraisal     22.1 0.000788
#>  3 tfidf_consumer_complaint_narrative_bonus         21.4 0.000788
#>  4 tfidf_consumer_complaint_narrative_debt          19.9 0.000788
#>  5 tfidf_consumer_complaint_narrative_escrow        17.8 0.000788
#>  6 tfidf_consumer_complaint_narrative_customers     17.2 0.000788
#>  7 tfidf_consumer_complaint_narrative_money         16.5 0.000788
#>  8 tfidf_consumer_complaint_narrative_emailed       15.9 0.000788
#>  9 tfidf_consumer_complaint_narrative_fees          15.1 0.000788
#> 10 tfidf_consumer_complaint_narrative_interest      14.5 0.000788
#> # ... with 991 more rows
\end{verbatim}

We see here, for the penalty we chose, what terms contribute the most to a complaint \emph{not} being about credit. The words are largely about mortgages and other financial products.

What terms contribute to a complaint being about credit reporting, for this tuned model? Here we see the names of the credit reporting agencies and words about credit inquiries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitted\_lasso }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull\_workflow\_fit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(estimate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1,001 x 3
#>    term                                          estimate  penalty
#>    <chr>                                            <dbl>    <dbl>
#>  1 tfidf_consumer_complaint_narrative_reseller      -86.4 0.000788
#>  2 tfidf_consumer_complaint_narrative_experian      -59.2 0.000788
#>  3 tfidf_consumer_complaint_narrative_transunion    -51.9 0.000788
#>  4 tfidf_consumer_complaint_narrative_equifax       -48.0 0.000788
#>  5 tfidf_consumer_complaint_narrative_compliant     -21.8 0.000788
#>  6 tfidf_consumer_complaint_narrative_reporting     -21.5 0.000788
#>  7 tfidf_consumer_complaint_narrative_report        -17.1 0.000788
#>  8 tfidf_consumer_complaint_narrative_freeze        -17.1 0.000788
#>  9 tfidf_consumer_complaint_narrative_inquiries     -16.9 0.000788
#> 10 tfidf_consumer_complaint_narrative_method        -16.0 0.000788
#> # ... with 991 more rows
\end{verbatim}

\begin{rmdnote}
Since we are using a linear model, the model coefficients are directly
interpretable and transparently give us variable importance. Many models
useful for machine learning with text do \emph{not} have such
transparent variable importance; in those situations, you can use other
model-independent or model-agnostic approaches like
\href{https://juliasilge.com/blog/last-airbender/}{permutation variable
importance}.
\end{rmdnote}

\hypertarget{casestudysparseencoding}{%
\section{Case study: sparse encoding}\label{casestudysparseencoding}}

We can change how our text data is represented to take advantage of its sparsity, especially for models like lasso regularized models. The regularized regression model we have been training in previous sections used \texttt{set\_engine("glmnet")}; this computational engine can be more efficient when text data is transformed to a \index{matrix!sparse}sparse matrix (Section \ref{motivatingsparse}), rather than a dense data frame or tibble representation.

To keep our text data sparse throughout modeling and use the sparse capabilities of \texttt{set\_engine("glmnet")}, we need to explicitly set a non-default preprocessing blueprint, using the package \textbf{hardhat} \citep{R-hardhat}.

\begin{rmdpackage}
The \textbf{hardhat} package is used by other tidymodels packages like
recipes and parsnip under the hood. As a tidymodels user, you typically
don't use hardhat functions directly. The exception is when you need to
customize something about your model or preprocessing, like in this
sparse data example.
\end{rmdpackage}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(hardhat)}
\NormalTok{sparse\_bp }\OtherTok{\textless{}{-}} \FunctionTok{default\_recipe\_blueprint}\NormalTok{(}\AttributeTok{composition =} \StringTok{"dgCMatrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This ``blueprint'' lets us specify during modeling how we want our data passed around from the \index{preprocessing}preprocessing into the model. The composition \texttt{"dgCMatrix"} is the most common sparse matrix type, from the Matrix package \citep{R-Matrix}, used in R for modeling. We can use this \texttt{blueprint} argument when we add our recipe to our modeling workflow, to define how the data should be passed into the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sparse\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(complaints\_rec, }\AttributeTok{blueprint =}\NormalTok{ sparse\_bp) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(tune\_spec)}

\NormalTok{sparse\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: logistic_reg()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 3 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> 
#> -- Model -----------------------------------------------------------------------
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

The last time we tuned a lasso model, we used the defaults for the penalty parameter and 30 levels. Let's restrict the values this time using the \texttt{range} argument, so we don't test out as small values for regularization, and only try 20 levels.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smaller\_lambda }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{penalty}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{)), }\AttributeTok{levels =} \DecValTok{20}\NormalTok{)}
\NormalTok{smaller\_lambda}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 20 x 1
#>      penalty
#>        <dbl>
#>  1 0.00001  
#>  2 0.0000183
#>  3 0.0000336
#>  4 0.0000616
#>  5 0.000113 
#>  6 0.000207 
#>  7 0.000379 
#>  8 0.000695 
#>  9 0.00127  
#> 10 0.00234  
#> 11 0.00428  
#> 12 0.00785  
#> 13 0.0144   
#> 14 0.0264   
#> 15 0.0483   
#> 16 0.0886   
#> 17 0.162    
#> 18 0.298    
#> 19 0.546    
#> 20 1
\end{verbatim}

We can tune this lasso regression model, in the same way that we did in Section \ref{tunelasso}. We will fit and assess each possible regularization parameter on each resampling fold, to find the best amount of regularization.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2020}\NormalTok{)}
\NormalTok{sparse\_rs }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{  sparse\_wf,}
\NormalTok{  complaints\_folds,}
  \AttributeTok{grid =}\NormalTok{ smaller\_lambda}
\NormalTok{)}

\NormalTok{sparse\_rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Tuning results
#> # 10-fold cross-validation 
#> # A tibble: 10 x 4
#>    splits               id     .metrics              .notes              
#>    <list>               <chr>  <list>                <list>              
#>  1 <split [79119/8792]> Fold01 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  2 <split [79120/8791]> Fold02 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  3 <split [79120/8791]> Fold03 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  4 <split [79120/8791]> Fold04 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  5 <split [79120/8791]> Fold05 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  6 <split [79120/8791]> Fold06 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  7 <split [79120/8791]> Fold07 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  8 <split [79120/8791]> Fold08 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  9 <split [79120/8791]> Fold09 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#> 10 <split [79120/8791]> Fold10 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
\end{verbatim}

How did this model turn out, especially compared to the tuned model that did not use the sparse capabilities of \texttt{set\_engine("glmnet")}?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sparse\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 5 x 7
#>     penalty .metric .estimator  mean     n  std_err .config              
#>       <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>                
#> 1 0.000695  roc_auc binary     0.953    10 0.000825 Preprocessor1_Model08
#> 2 0.000379  roc_auc binary     0.953    10 0.000824 Preprocessor1_Model07
#> 3 0.000207  roc_auc binary     0.953    10 0.000821 Preprocessor1_Model06
#> 4 0.000113  roc_auc binary     0.953    10 0.000820 Preprocessor1_Model05
#> 5 0.0000616 roc_auc binary     0.952    10 0.000822 Preprocessor1_Model04
\end{verbatim}

The best ROC AUC is nearly identical; the best ROC AUC for the non-sparse tuned lasso model in Section \ref{tunelasso} was 0.953. The best regularization parameter (\texttt{penalty}) is a little different (the best value in Section \ref{tunelasso} was 0.00036) but we used a different grid so didn't try out exactly the same values. We ended up with nearly the same performance and best tuned model.

Importantly, this tuning also took a bit less time to complete.\index{computational speed}

\begin{itemize}
\item
  The \emph{preprocessing}\index{preprocessing} was not much faster, because tokenization and computing tf-idf take a long time.\index{tf-idf}
\item
  The \emph{model fitting} was much faster, because for highly sparse data, this implementation of regularized regression is much faster for sparse matrix\index{matrix!sparse} input than any dense input.
\end{itemize}

Overall, the whole tuning workflow is about 10\% faster using the sparse preprocessing blueprint. Depending on how computationally expensive your preprocessing is relative to your model and how sparse your data is, you may expect to see larger (or smaller) gains from moving to a sparse data representation.

\begin{rmdnote}
Since our model performance is about the same and we see gains in
training time, let's use this sparse representation for the rest of this
chapter.
\end{rmdnote}

\hypertarget{mlmulticlass}{%
\section{Two class or multiclass?}\label{mlmulticlass}}

Most of this chapter focuses on binary classification, where we have two classes in our outcome variable (such as ``Credit'' and ``Other'') and each observation can either be one or the other. This is a simple scenario with straightforward evaluation strategies because the results only have a two-by-two contingency matrix.
However, it is not always possible to limit a modeling question to two classes. Let's explore how to deal with situations where we have more than two classes.
The CFPB complaints data set in this chapter has nine different \texttt{product} classes. In decreasing frequency, they are:

\begin{itemize}
\item
  Credit reporting, credit repair services, or other personal consumer reports
\item
  Debt collection
\item
  Credit card or prepaid card
\item
  Mortgage
\item
  Checking or savings account
\item
  Student loan
\item
  Vehicle loan or lease
\item
  Money transfer, virtual currency, or money service
\item
  Payday loan, title loan, or personal loan
\end{itemize}

We assume that there is a reason why these product classes have been created in this fashion by this government agency.
Perhaps complaints from different classes are handled by different people or organizations.
Whatever the reason, in this section we would like to build a multiclass classifier to identify these nine specific product classes.

We need to create a new split of the data using \texttt{initial\_split()} on the unmodified \texttt{complaints} data set.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\NormalTok{multicomplaints\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(complaints, }\AttributeTok{strata =}\NormalTok{ product)}

\NormalTok{multicomplaints\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(multicomplaints\_split)}
\NormalTok{multicomplaints\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(multicomplaints\_split)}
\end{Highlighting}
\end{Shaded}

Before we continue, let us take a look at the number of cases in each of the classes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multicomplaints\_train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(product, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(n, product)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 9 x 2
#>       n product                                                                 
#>   <int> <chr>                                                                   
#> 1 41628 Credit reporting, credit repair services, or other personal consumer re~
#> 2 16722 Debt collection                                                         
#> 3  8695 Credit card or prepaid card                                             
#> 4  7067 Mortgage                                                                
#> 5  5238 Checking or savings account                                             
#> 6  2960 Student loan                                                            
#> 7  2028 Vehicle loan or lease                                                   
#> 8  1926 Money transfer, virtual currency, or money service                      
#> 9  1647 Payday loan, title loan, or personal loan
\end{verbatim}

There is significant imbalance between the classes that we must address, with over twenty times more cases of the majority class than there is of the smallest class.
This kind of imbalance is a common problem\index{classification!challenges} with multiclass classification, with few multiclass data sets in the real world exhibiting balance between classes.

Compared to binary classification, there are several additional issues to keep in mind when working with multiclass classification:

\begin{itemize}
\item
  Many machine learning algorithms do not handle imbalanced data well and are likely to have a hard time predicting minority classes.
\item
  Not all machine learning algorithms are built for multiclass classification at all.
\item
  Many evaluation metrics need to be reformulated to describe multiclass predictions.
\end{itemize}

When you have multiple classes in your data, it is possible to formulate the multiclass problem in two ways. With one approach, any given observation can belong to multiple classes. With the other approach, an observation can belong to one and only one class. We will be sticking to the second, ``one class per observation'' model formulation in this section.

There are many different ways to deal with imbalanced data.
We will demonstrate one of the simplest methods, downsampling\index{downsampling}, where observations from the majority classes are removed during training to achieve a balanced class distribution.
We will be using the \textbf{themis} \citep{R-themis} add-on package for recipes which provides the \texttt{step\_downsample()} function to perform downsampling.

\begin{rmdpackage}
The \textbf{themis} package provides many more algorithms to deal with
imbalanced data during data preprocessing.
\end{rmdpackage}

We have to create a new recipe specification from scratch, since we are dealing with new training data this time.
The specification \texttt{multicomplaints\_rec} is similar to what we created in Section \ref{classfirstattemptlookatdata}. The only changes are that different data is passed to the \texttt{data} argument in the \texttt{recipe()} function (it is now \texttt{multicomplaints\_train}) and we have added \texttt{step\_downsample(product)} to the end of the recipe specification to downsample after all the text preprocessing. We want to downsample last so that we still generate features on the full training data set. The downsampling\index{downsampling} will then \emph{only} affect the modeling step, not the preprocessing\index{preprocessing} steps, with hopefully better results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(themis)}

\NormalTok{multicomplaints\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(product }\SpecialCharTok{\textasciitilde{}}\NormalTok{ consumer\_complaint\_narrative,}
         \AttributeTok{data =}\NormalTok{ multicomplaints\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(consumer\_complaint\_narrative, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_downsample}\NormalTok{(product)}
\end{Highlighting}
\end{Shaded}

We also need a new cross-validation object since we are using a different data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multicomplaints\_folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(multicomplaints\_train)}
\end{Highlighting}
\end{Shaded}

We cannot reuse the tuneable lasso classification specification from Section \ref{tunelasso} because it only works for binary classification. Some model algorithms and computational engines (examples are most random forests and SVMs) automatically detect when we perform multiclass classification from the number of classes in the outcome variable and do not require any changes to our model specification. For lasso regularization, we need to create a new special model specification just for the multiclass class using \texttt{multinom\_reg()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multi\_spec }\OtherTok{\textless{}{-}} \FunctionTok{multinom\_reg}\NormalTok{(}\AttributeTok{penalty =} \FunctionTok{tune}\NormalTok{(), }\AttributeTok{mixture =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glmnet"}\NormalTok{)}

\NormalTok{multi\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Multinomial Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

We used the same arguments for \texttt{penalty} and \texttt{mixture} as in Section \ref{tunelasso}, as well as the same mode and engine, but this model specification is set up to handle more than just two classes. We can combine this model specification with our preprocessing recipe for multiclass data in a \texttt{workflow()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multi\_lasso\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(multicomplaints\_rec, }\AttributeTok{blueprint =}\NormalTok{ sparse\_bp) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(multi\_spec)}

\NormalTok{multi\_lasso\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: multinom_reg()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 4 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> * step_downsample()
#> 
#> -- Model -----------------------------------------------------------------------
#> Multinomial Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

Now we have everything we need to tune the regularization penalty and find an appropriate value. Note that we specify \texttt{save\_pred\ =\ TRUE}, so we can create ROC curves and a confusion matrix later. This is especially beneficial for multiclass classification.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multi\_lasso\_rs }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{  multi\_lasso\_wf,}
\NormalTok{  multicomplaints\_folds,}
  \AttributeTok{grid =}\NormalTok{ smaller\_lambda,}
  \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}

\NormalTok{multi\_lasso\_rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Tuning results
#> # 10-fold cross-validation 
#> # A tibble: 10 x 5
#>    splits          id     .metrics          .notes         .predictions         
#>    <list>          <chr>  <list>            <list>         <list>               
#>  1 <split [79119/~ Fold01 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,84~
#>  2 <split [79120/~ Fold02 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#>  3 <split [79120/~ Fold03 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#>  4 <split [79120/~ Fold04 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#>  5 <split [79120/~ Fold05 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#>  6 <split [79120/~ Fold06 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#>  7 <split [79120/~ Fold07 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#>  8 <split [79120/~ Fold08 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#>  9 <split [79120/~ Fold09 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
#> 10 <split [79120/~ Fold10 <tibble[,5] [40 ~ <tibble[,1] [~ <tibble[,14] [175,82~
\end{verbatim}

What do we see, in terms of performance metrics?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_acc }\OtherTok{\textless{}{-}}\NormalTok{ multi\_lasso\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}

\NormalTok{best\_acc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 5 x 7
#>    penalty .metric  .estimator  mean     n std_err .config              
#>      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                
#> 1 0.00234  accuracy multiclass 0.755    10 0.00220 Preprocessor1_Model10
#> 2 0.00428  accuracy multiclass 0.751    10 0.00238 Preprocessor1_Model11
#> 3 0.00127  accuracy multiclass 0.749    10 0.00273 Preprocessor1_Model09
#> 4 0.00785  accuracy multiclass 0.740    10 0.00219 Preprocessor1_Model12
#> 5 0.000695 accuracy multiclass 0.740    10 0.00451 Preprocessor1_Model08
\end{verbatim}

The accuracy metric naturally extends to multiclass tasks, but even the very best value is quite low at 75.5\%, significantly lower than for the binary case in Section \ref{tunelasso}. This is expected since multiclass classification is a harder task than binary classification.

\begin{rmdwarning}
In binary classification, there is one right answer and one wrong
answer; in this case, there is one right answer and \emph{eight} wrong
answers.
\end{rmdwarning}

To get a more detailed view of how our classifier is performing, let us look at one of the confusion matrices\index{matrix!confusion} in Figure \ref{fig:multiheatmap}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multi\_lasso\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(penalty }\SpecialCharTok{==}\NormalTok{ best\_acc}\SpecialCharTok{$}\NormalTok{penalty) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(id }\SpecialCharTok{==} \StringTok{"Fold01"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(product, .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_discrete}\NormalTok{(}\AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{str\_wrap}\NormalTok{(x, }\DecValTok{20}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{str\_wrap}\NormalTok{(x, }\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/multiheatmap-1} 

}

\caption{Confusion matrix for multiclass lasso regularized classifier, with most of the classifications along the diagonal}\label{fig:multiheatmap}
\end{figure}

The diagonal is fairly well populated, which is a good sign. This means that the model generally predicted the right class.
The off-diagonals numbers are all the failures and where we should direct our focus.
It is a little hard to see these cases well since the majority class affects the scale.
A trick to deal with this problem is to remove all the correctly predicted observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multi\_lasso\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(penalty }\SpecialCharTok{==}\NormalTok{ best\_acc}\SpecialCharTok{$}\NormalTok{penalty) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(id }\SpecialCharTok{==} \StringTok{"Fold01"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.pred\_class }\SpecialCharTok{!=}\NormalTok{ product) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(product, .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_discrete}\NormalTok{(}\AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{str\_wrap}\NormalTok{(x, }\DecValTok{20}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{str\_wrap}\NormalTok{(x, }\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/multiheatmapminusdiag-1} 

}

\caption{Confusion matrix for multiclass lasso regularized classifier without diagonal}\label{fig:multiheatmapminusdiag}
\end{figure}

Now we can more clearly see where our model breaks down in \index{matrix!confusion}Figure \ref{fig:multiheatmapminusdiag}. Some of the most common errors are ``Credit reporting, credit repair services, or other personal consumer reports'' complaints being wrongly being predicted as ``Debt collection'' or ``Credit card of prepaid card'' complaints. Those mistakes by the model are not hard to understand since all deal with credit and debt and do have overlap in vocabulary.
Knowing what the problem is helps us figure out how to improve our model.
The next step for improving our model is to revisit the data preprocessing\index{preprocessing} steps and model selection.
We can look at different models or model engines that might be able to more easily separate the classes.

Now that we have an idea of where the model isn't working, we can look more closely at the data and attempt to create features that could distinguish between these classes. In Section \ref{customfeatures} we will demonstrate how you can create your own custom features.

\hypertarget{case-study-including-non-text-data}{%
\section{Case study: including non-text data}\label{case-study-including-non-text-data}}

We are building a model from a data set that includes more than text data alone. Annotations and labels have been added by the CFPB that we can use during modeling, but we need to ensure that only information that would be available at the time of prediction is included in the model.
Otherwise we we will be very disappointed once our model is used to predict on new data!
The variables we identify as available for use as predictors are:

\begin{itemize}
\item
  \texttt{date\_received}
\item
  \texttt{issue}
\item
  \texttt{sub\_issue}
\item
  \texttt{consumer\_complaint\_narrative}
\item
  \texttt{company}
\item
  \texttt{state}
\item
  \texttt{zip\_code}
\item
  \texttt{tags}
\item
  \texttt{submitted\_via}
\end{itemize}

Let's try including \texttt{date\_received} in our modeling, along with the text variable we have already used \texttt{consumer\_complaint\_narrative} and a new variable \texttt{tags}.
The \texttt{submitted\_via} variable could have been a viable candidate, but all the entries are ``web''.
The other variables like ZIP code could be of use too, but they are categorical variables with many values so we will exclude them for now.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more\_vars\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(product }\SpecialCharTok{\textasciitilde{}}\NormalTok{ date\_received }\SpecialCharTok{+}\NormalTok{ tags }\SpecialCharTok{+}\NormalTok{ consumer\_complaint\_narrative,}
         \AttributeTok{data =}\NormalTok{ complaints\_train)}
\end{Highlighting}
\end{Shaded}

How should we preprocess the \texttt{date\_received} variable? We can use the \texttt{step\_date()} function to extract the month and day of the week (\texttt{"dow"}). Then we remove the original date variable and convert the new month and day-of-the-week columns to \index{variables!dummy}indicator variables with \texttt{step\_dummy()}.

\begin{rmdnote}
Categorical variables like the month can be stored as strings or
factors, but for some kinds of models, they must be converted to
indicator or dummy variables. These are numeric binary variables for the
levels of the original categorical variable. For example, a variable
called \texttt{December} would be created that is all zeroes and ones
specifying which complaints were submitted in December, plus a variable
called \texttt{November}, a variable called \texttt{October}, and so on.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more\_vars\_rec }\OtherTok{\textless{}{-}}\NormalTok{ more\_vars\_rec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_date}\NormalTok{(date\_received, }\AttributeTok{features =} \FunctionTok{c}\NormalTok{(}\StringTok{"month"}\NormalTok{, }\StringTok{"dow"}\NormalTok{), }\AttributeTok{role =} \StringTok{"dates"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_rm}\NormalTok{(date\_received) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_dummy}\NormalTok{(}\FunctionTok{has\_role}\NormalTok{(}\StringTok{"dates"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The \texttt{tags} variable has some missing data. We can deal with this by using \texttt{step\_unknown()}, which adds a new level to this factor variable for cases of missing data. Then we ``dummify'' (create dummy/indicator variables) the variable with \texttt{step\_dummy()}.\index{variables!dummy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more\_vars\_rec }\OtherTok{\textless{}{-}}\NormalTok{ more\_vars\_rec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_unknown}\NormalTok{(tags) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_dummy}\NormalTok{(tags)}
\end{Highlighting}
\end{Shaded}

Now we add steps to process the text of the complaints, as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more\_vars\_rec }\OtherTok{\textless{}{-}}\NormalTok{ more\_vars\_rec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(consumer\_complaint\_narrative, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(consumer\_complaint\_narrative)}
\end{Highlighting}
\end{Shaded}

Let's combine this more extensive preprocessing recipe that handles more variables together with the tuneable lasso regularized classification model specification.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more\_vars\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(more\_vars\_rec, }\AttributeTok{blueprint =}\NormalTok{ sparse\_bp) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(tune\_spec)}

\NormalTok{more\_vars\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: logistic_reg()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 8 Recipe Steps
#> 
#> * step_date()
#> * step_rm()
#> * step_dummy()
#> * step_unknown()
#> * step_dummy()
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> 
#> -- Model -----------------------------------------------------------------------
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

Let's tune this \texttt{workflow()} with our resampled data sets, find a good value for the regularization penalty, and estimate the model's performance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{more\_vars\_rs }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{  more\_vars\_wf,}
\NormalTok{  complaints\_folds,}
  \AttributeTok{grid =}\NormalTok{ smaller\_lambda,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can extract the metrics for the best-performing regularization penalties from these results with \texttt{show\_best()} with an option like \texttt{"roc\_auc"} or \texttt{"accuracy"} if we prefer. How did our chosen performance metric turn out for our model that included more than just the text data?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more\_vars\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{show\_best}\NormalTok{(}\StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 5 x 7
#>     penalty .metric .estimator  mean     n  std_err .config              
#>       <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>                
#> 1 0.000695  roc_auc binary     0.953    10 0.000824 Preprocessor1_Model08
#> 2 0.000379  roc_auc binary     0.953    10 0.000818 Preprocessor1_Model07
#> 3 0.000207  roc_auc binary     0.953    10 0.000814 Preprocessor1_Model06
#> 4 0.000113  roc_auc binary     0.953    10 0.000813 Preprocessor1_Model05
#> 5 0.0000616 roc_auc binary     0.953    10 0.000812 Preprocessor1_Model04
\end{verbatim}

We see here that including more predictors did not measurably improve our model performance but it did change the regularization a bit. With only text features in Section \ref{casestudysparseencoding} and the same grid and sparse encoding, we achieved an accuracy of 0.953, the same as what we see now by including the features dealing with dates and tags as well. The best regularization penalty in Section \ref{casestudysparseencoding} was 0.0007 but here it is a bit higher, indicating that our model learned to regularize more strongly once we added these extra features. This makes sense, and we can use \texttt{tidy()} and some \textbf{dplyr} manipulation to find at what rank (\texttt{term\_rank}) any of the date or tag variables were included in the regularized results, by absolute value of the model coefficient.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{finalize\_workflow}\NormalTok{(more\_vars\_wf, }
                  \FunctionTok{select\_best}\NormalTok{(more\_vars\_rs, }\StringTok{"roc\_auc"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(complaints\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull\_workflow\_fit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{abs}\NormalTok{(estimate)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{term\_rank =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(term, }\StringTok{"tfidf"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 21 x 4
#>    term                    estimate  penalty term_rank
#>    <chr>                      <dbl>    <dbl>     <int>
#>  1 date_received_month_Dec -0.319   0.000695       726
#>  2 (Intercept)              0.256   0.000695       734
#>  3 date_received_dow_Mon    0.129   0.000695       758
#>  4 date_received_month_Apr  0.101   0.000695       763
#>  5 date_received_month_Aug -0.0923  0.000695       768
#>  6 date_received_dow_Fri    0.0422  0.000695       782
#>  7 date_received_month_Jul -0.0302  0.000695       785
#>  8 date_received_month_Feb -0.0270  0.000695       787
#>  9 tags_Servicemember      -0.0176  0.000695       789
#> 10 date_received_dow_Wed   -0.00257 0.000695       795
#> # ... with 11 more rows
\end{verbatim}

In our example here, some of the non-text predictors are included in the model with non-zero coefficients but ranked down in the 700s of all model terms, with smaller coefficients than many text terms. They are not that important.

\begin{rmdnote}
This whole book focuses on supervised machine learning for text data,
but models can combine \emph{both} text predictors and other kinds of
predictors.
\end{rmdnote}

\hypertarget{case-study-data-censoring}{%
\section{Case study: data censoring}\label{case-study-data-censoring}}

The complaints data set already has sensitive information (PII)\index{PII} censored or protected using strings such as ``XXXX'' and ``XX''.
This data censoring\index{censoring} can be viewed as data \emph{annotation}; specific account numbers and birthdays are protected but we know they were there. These values would be mostly unique anyway, and likely filtered out in their original form.

Figure \ref{fig:censoredtrigram} shows the most frequent trigrams (Section \ref{tokenizingngrams}) in our training data set.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}

\NormalTok{complaints\_train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(trigrams, }
\NormalTok{                consumer\_complaint\_narrative, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
                \AttributeTok{collapse =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(trigrams, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{censored =} \FunctionTok{str\_detect}\NormalTok{(trigrams, }\StringTok{"xx"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n, }\FunctionTok{reorder}\NormalTok{(trigrams, n), }\AttributeTok{fill =}\NormalTok{ censored)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"grey40"}\NormalTok{, }\StringTok{"firebrick"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Trigrams"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/censoredtrigram-1} 

}

\caption{Many of the most frequent trigrams feature censored information}\label{fig:censoredtrigram}
\end{figure}

The vast majority of trigrams in Figure \ref{fig:censoredtrigram} include one or more censored words.
Not only do the most used trigrams include some kind of censoring,
but the censoring itself is informative as it is not used uniformly across the product classes.
In Figure \ref{fig:trigram25}, we take the top 25 most frequent trigrams that include censoring,
and plot the proportions for ``Credit'' and ``Other''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_censored\_trigrams }\OtherTok{\textless{}{-}}\NormalTok{ complaints\_train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(trigrams, }
\NormalTok{                consumer\_complaint\_narrative, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
                \AttributeTok{collapse =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(trigrams, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(trigrams, }\StringTok{"xx"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{25}\NormalTok{)}

\NormalTok{plot\_data }\OtherTok{\textless{}{-}}\NormalTok{ complaints\_train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(trigrams, }
\NormalTok{                consumer\_complaint\_narrative, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
                \AttributeTok{collapse =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{right\_join}\NormalTok{(top\_censored\_trigrams, }\AttributeTok{by =} \StringTok{"trigrams"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(trigrams, product, }\AttributeTok{.drop =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{plot\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n, trigrams, }\AttributeTok{fill =}\NormalTok{ product)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position =} \StringTok{"fill"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/trigram25-1} 

}

\caption{Many of the most frequent trigrams feature censored words, but there is a difference in how often they are used within each class}\label{fig:trigram25}
\end{figure}

There is a difference in these proportions across classes. Tokens like ``on xx xx'' and ``of xx xx'' are used when referencing a date, e.g., ``we had a problem on 06/25/2018''.
Remember that the current tokenization engine strips punctuation before tokenizing.
This means that the above example will be turned into ``we had a problem on 06 25 2018'' before creating n-grams\footnote{The censored\index{censoring} trigrams that include ``oh'' seem mysterious but upon closer examination, they come from censored addresses, with ``oh'' representing the US state of Ohio. Most two-letter state abbreviations are censored but this one is not, since it is ambiguous. This highlights the real challenge of anonymizing text.}.

To crudely simulate what the data might look like before it was censored, we can replace all cases of ``XX'' and ``XXXX'' with random integers.
This isn't quite right since dates will be given values between \texttt{00} and \texttt{99} and we don't know for sure that only numerals have been censored, but it gives us a place to start.
Below is a simple function \texttt{uncensor\_vec()} that locates all instances of \texttt{"XX"} and replaces them with a number between 11 and 99.
We don't need to handle the special case of \texttt{XXXX} as it automatically being handled.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uncensor }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n) \{}
  \FunctionTok{as.character}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{10} \SpecialCharTok{\^{}}\NormalTok{ (n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), }\DecValTok{10} \SpecialCharTok{\^{}}\NormalTok{ n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), }\DecValTok{1}\NormalTok{))}
\NormalTok{\}}

\NormalTok{uncensor\_vec }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  locs }\OtherTok{\textless{}{-}} \FunctionTok{str\_locate\_all}\NormalTok{(x, }\StringTok{"XX"}\NormalTok{)}
  \FunctionTok{map2\_chr}\NormalTok{(x, locs, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ \{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(.y))) \{}
      \FunctionTok{str\_sub}\NormalTok{(.x, .y[i, }\DecValTok{1}\NormalTok{], .y[i, }\DecValTok{2}\NormalTok{]) }\OtherTok{\textless{}{-}} \FunctionTok{uncensor}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{    .x}
\NormalTok{  \})}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can run a quick test to see how it works.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{uncensor\_vec}\NormalTok{(}\StringTok{"In XX/XX/XXXX I leased a XXXX vehicle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "In 33/64/4458 I leased a 7595 vehicle"
\end{verbatim}

Now we can produce the same visualization as Figure \ref{fig:censoredtrigram} but also applying our uncensoring function to the text before tokenizing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints\_train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{text =} \FunctionTok{uncensor\_vec}\NormalTok{(consumer\_complaint\_narrative)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(trigrams, text, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
                \AttributeTok{collapse =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(trigrams, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{censored =} \FunctionTok{str\_detect}\NormalTok{(trigrams, }\StringTok{"xx"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n, }\FunctionTok{reorder}\NormalTok{(trigrams, n), }\AttributeTok{fill =}\NormalTok{ censored)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"grey40"}\NormalTok{, }\StringTok{"firebrick"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Trigrams"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/uncensoredtrigram-1} 

}

\caption{Trigrams without numbers float to the top as the uncensored tokens are too spread out}\label{fig:uncensoredtrigram}
\end{figure}

Here in Figure \ref{fig:uncensoredtrigram}, we see the same trigrams that appeared in Figure \ref{fig:censoredtrigram}.
However, none of the uncensored words appear, because of our uncensoring function.
This is expected, because while \texttt{"xx\ xx\ 2019"} appears in the first plot indicating a date in the year 2019, after we uncensor it, it is split into 365 buckets (actually more, since we used numerical values between \texttt{00} and \texttt{99}).
Censoring the dates in these complaints gives more power to a date as a general construct.

\begin{rmdwarning}
What happens when we use these censored dates as a feature in supervised
machine learning? We have a higher chance of understanding if dates in
the complaint text are important to predicting the class, but we are
blinded to the possibility that certain dates and months are more
important.
\end{rmdwarning}

Data censoring\index{censoring} can be a form of preprocessing in your data pipeline.
For example, it is highly unlikely to be useful (or ethical/legal) to have any specific person's social security number, credit card number, or any other kind of PII\index{PII} embedded into your model. Such values appear rarely and are most likely highly correlated with other known variables in your data set.
More importantly, that information can become embedded in your model and begin to leak as demonstrated by \citet{carlini2018secret}, \citet{Fredrikson2014}, and \citet{Fredrikson2015}.
Both of these issues are important, and one of them could land you in a lot of legal trouble.
Exposing such PII to modeling is an example of where we should all stop to ask, ``Should we even be doing this?'' as we discussed in the foreword to these chapters.

If you have social security numbers in text data, you should definitely not pass them on to your machine learning model, but you may consider the option of annotating the \emph{presence} of a social security number.
Since a social security number has a very specific form, we can easily construct a \index{regex}regular expression (Appendix \ref{regexp}) to locate them.

\begin{rmdnote}
A social security number comes in the form \texttt{AAA-BB-CCCC} where
\texttt{AAA} is a number between \texttt{001} and \texttt{899} excluding
\texttt{666}, \texttt{BB} is a number between \texttt{01} and
\texttt{99} and \texttt{CCCC} is a number between \texttt{0001} and
\texttt{9999}. This gives us the following regex:

\texttt{(?!000\textbar{}666){[}0-8{]}{[}0-9{]}\{2\}-(?!00){[}0-9{]}\{2\}-(?!0000){[}0-9{]}\{4\}}
\end{rmdnote}

We can use a function to replace each social security number with an indicator that can be detected later by \index{preprocessing}preprocessing steps.
It's a good idea to use a ``word'' that won't be accidentally broken up by a tokenizer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssn\_text }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"My social security number is 498{-}08{-}6333"}\NormalTok{,}
              \StringTok{"No way, mine is 362{-}60{-}9159"}\NormalTok{,}
              \StringTok{"My parents numbers are 575{-}32{-}6985 and 576{-}36{-}5202"}\NormalTok{)}

\NormalTok{ssn\_pattern }\OtherTok{\textless{}{-}}  \StringTok{"(?!000|666)[0{-}8][0{-}9]\{2\}{-}(?!00)[0{-}9]\{2\}{-}(?!0000)[0{-}9]\{4\}"}

\FunctionTok{str\_replace\_all}\NormalTok{(}\AttributeTok{string =}\NormalTok{ ssn\_text,}
                \AttributeTok{pattern =}\NormalTok{ ssn\_pattern,}
                \AttributeTok{replacement =} \StringTok{"ssnindicator"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "My social security number is ssnindicator"           
#> [2] "No way, mine is ssnindicator"                        
#> [3] "My parents numbers are ssnindicator and ssnindicator"
\end{verbatim}

This technique isn't useful only for personally identifiable information\index{PII} but can be used anytime you want to gather similar words in the same bucket; hashtags, email addresses, and usernames can sometimes benefit from being annotated in this way.

\begin{rmdwarning}
The practice of data re-identification or de-anonymization, where seemingly or partially ``anonymized'' data sets are mined to identify individuals, is out of scope for this section and our book. However, this is a significant and important issue for any data practitioner dealing with PII and we encourage readers to familiarize themselves with results such as \citet{Sweeney2000}, and current best practices to protect against such mining.
\end{rmdwarning}

\hypertarget{customfeatures}{%
\section{Case study: custom features}\label{customfeatures}}

Most of what we have looked at so far has boiled down to counting tokens and weighting them in one way or another.
This approach is quite broad and domain agnostic, but you as a data practitioner often have specific knowledge about your data set that you should use in feature engineering.\index{feature engineering}
Your domain knowledge allows you to build more predictive features than the naive search of simple tokens.
As long as you can reasonably formulate what you are trying to count, chances are you can write a function that can detect it.
This is where having a little bit of knowledge about regular expressions pays off.

\begin{rmdpackage}
The \textbf{textfeatures} \citep{R-textfeatures} package includes functions to extract useful features from text, from the number of digits to the number of second person pronouns and more. These features can be used in textrecipes data preprocessing with the \texttt{step\_textfeature()} function.
\end{rmdpackage}

Your specific domain knowledge may provide specific guidance about feature engineering\index{feature engineering} for text.
Such custom features can be simple such as the number of URLs or the number of punctuation marks.
They can also be more engineered such as the percentage of capitalization, whether the text ends with a hashtag, or whether two people's names are both mentioned in a document.

For our CFPB complaints data, certain patterns may not have adequately been picked up by our model so far, such as the data censoring and the curly bracket annotation for monetary amounts that we saw in Section \ref{classfirstattemptlookatdata}. Let's walk through how to create data preprocessing functions to build the features to:

\begin{itemize}
\item
  detect credit cards,
\item
  calculate percentage censoring, and
\item
  detect monetary amounts.
\end{itemize}

\hypertarget{detect-credit-cards}{%
\subsection{Detect credit cards}\label{detect-credit-cards}}

A credit card number is represented as four groups of four capital Xs in this data set.
Since the data is fairly well processed we are fairly sure that spacing will not be an issue and all credit cards will be represented as ``XXXX XXXX XXXX XXXX''.
A first naive attempt may be to use \texttt{str\_detect()} with ``XXXX XXXX XXXX XXXX'' to find all the credit cards.

\index{regex}

\begin{rmdnote}
It is a good idea to create a small example regular expression where you
know the answer, and then prototype your function before moving to the
main data set.
\end{rmdnote}

We start by creating a vector with two positives, one negative, and one potential false positive.
The last string is more tricky since it has the same shape as a credit card but has one too many groups.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{credit\_cards }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"my XXXX XXXX XXXX XXXX balance, and XXXX XXXX XXXX XXXX."}\NormalTok{,}
                  \StringTok{"card with number XXXX XXXX XXXX XXXX."}\NormalTok{,}
                  \StringTok{"at XX/XX 2019 my first"}\NormalTok{,}
                  \StringTok{"live at XXXX XXXX XXXX XXXX XXXX SC"}\NormalTok{)}


\FunctionTok{str\_detect}\NormalTok{(credit\_cards, }\StringTok{"XXXX XXXX XXXX XXXX"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE  TRUE FALSE  TRUE
\end{verbatim}

As we feared, the last vector was falsely detected to be a credit card.
Sometimes you will have to accept a certain number of false positives and/or false negatives, depending on the data and what you are trying to detect.
In this case, we can make the regex a little more complicated to avoid that specific false positive.
We need to make sure that the word coming before the X's doesn't end in a capital X and the word following the last X doesn't start with a capital X.
We place spaces around the credit card and use some negated character classes (Appendix \ref{character-classes}) to detect anything BUT a capital X.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_detect}\NormalTok{(credit\_cards, }\StringTok{"[\^{}X] XXXX XXXX XXXX XXXX [\^{}X]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE FALSE FALSE FALSE
\end{verbatim}

Hurray! This fixed the false positive.
But it gave us a false negative in return.
Turns out that this regex\index{regex} doesn't allow the credit card to be followed by a period since it requires a space.
We can fix this with an alteration to match for a period or a space and a non-X.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_detect}\NormalTok{(credit\_cards, }\StringTok{"[\^{}X] +XXXX XXXX XXXX XXXX(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.| [\^{}X])"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE  TRUE FALSE FALSE
\end{verbatim}

Now that we have a regular expression we are happy with we can wrap it up in a function we can use.
We can extract the presence of a credit card with \texttt{str\_detect()} and the number of credit cards with \texttt{str\_count()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{creditcard\_indicator }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{str\_detect}\NormalTok{(x, }\StringTok{"[\^{}X] +XXXX XXXX XXXX XXXX(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.| [\^{}X])"}\NormalTok{)}
\NormalTok{\}}

\NormalTok{creditcard\_count }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{str\_count}\NormalTok{(x, }\StringTok{"[\^{}X] +XXXX XXXX XXXX XXXX(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.| [\^{}X])"}\NormalTok{)}
\NormalTok{\}}

\FunctionTok{creditcard\_indicator}\NormalTok{(credit\_cards)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE  TRUE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{creditcard\_count}\NormalTok{(credit\_cards)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 2 1 0 0
\end{verbatim}

\hypertarget{calculate-percentage-censoring}{%
\subsection{Calculate percentage censoring}\label{calculate-percentage-censoring}}

Some of the complaints contain a high proportion of censoring\index{censoring}, and we can build a feature to measure the percentage of the text that is censored.

\begin{rmdwarning}
There are often many ways to get to the same solution when working with
regular expressions.
\end{rmdwarning}

\index{regex}

Let's attack this problem by counting the number of X's in each string, then count the number of alphanumeric characters and divide the two to get a percentage.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_count}\NormalTok{(credit\_cards, }\StringTok{"X"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 32 16  4 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_count}\NormalTok{(credit\_cards, }\StringTok{"[:alnum:]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 44 30 17 28
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_count}\NormalTok{(credit\_cards, }\StringTok{"X"}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{str\_count}\NormalTok{(credit\_cards, }\StringTok{"[:alnum:]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.7272727 0.5333333 0.2352941 0.7142857
\end{verbatim}

We can finish up by creating a function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{percent\_censoring }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{str\_count}\NormalTok{(x, }\StringTok{"X"}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{str\_count}\NormalTok{(x, }\StringTok{"[:alnum:]"}\NormalTok{)}
\NormalTok{\}}

\FunctionTok{percent\_censoring}\NormalTok{(credit\_cards)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.7272727 0.5333333 0.2352941 0.7142857
\end{verbatim}

\hypertarget{detect-monetary-amounts}{%
\subsection{Detect monetary amounts}\label{detect-monetary-amounts}}

We have already constructed a \index{regex}regular expression that detects the monetary amount from the text in Section \ref{classfirstattemptlookatdata}, so now we can look at how to use this information.
Let's start by creating a little example and see what we can extract.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dollar\_texts }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"That will be \{$20.00\}"}\NormalTok{,}
                  \StringTok{"\{$3.00\}, \{$2.00\} and \{$7.00\}"}\NormalTok{,}
                  \StringTok{"I have no money"}\NormalTok{)}

\FunctionTok{str\_extract\_all}\NormalTok{(dollar\_texts, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$[0{-}9}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.]*}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "{$20.00}"
#> 
#> [[2]]
#> [1] "{$3.00}" "{$2.00}" "{$7.00}"
#> 
#> [[3]]
#> character(0)
\end{verbatim}

We can create a function that simply detects the dollar amount, and we can count the number of times each amount appears.
Each occurrence also has a value, so it would be nice to include that information as well, such as the mean, minimum, or maximum.

First, let's extract the number from the strings. We could write a regular expression for this, but the \texttt{parse\_number()} function from the readr package does a really good job of pulling out numbers.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract\_all}\NormalTok{(dollar\_texts, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$[0{-}9}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.]*}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map}\NormalTok{(readr}\SpecialCharTok{::}\NormalTok{parse\_number)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] 20
#> 
#> [[2]]
#> [1] 3 2 7
#> 
#> [[3]]
#> numeric(0)
\end{verbatim}

Now that we have the numbers we can iterate over them with the function of our choice.
Since we are going to have texts with no monetary amounts, we need to handle the case with zero numbers. Defaults for some functions with vectors of length zero can be undesirable; we don't want \texttt{-Inf} to be a value. Let's extract the maximum value and give cases with no monetary amounts a maximum of zero.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_money }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{str\_extract\_all}\NormalTok{(x, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$[0{-}9}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.]*}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\}"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{map}\NormalTok{(readr}\SpecialCharTok{::}\NormalTok{parse\_number) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{map\_dbl}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{length}\NormalTok{(.x) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FunctionTok{max}\NormalTok{(.x)))}
\NormalTok{\}}

\FunctionTok{max\_money}\NormalTok{(dollar\_texts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 20  7  0
\end{verbatim}

Now that we have created some \index{feature engineering}feature engineering functions, we can use them to (hopefully) make our classification model better.

\hypertarget{what-evaluation-metrics-are-appropriate-1}{%
\section{What evaluation metrics are appropriate?}\label{what-evaluation-metrics-are-appropriate-1}}

We have focused on using accuracy and ROC AUC as metrics for our classification models so far. These are not the only classification metrics available and your choice will often depend on how much you care about false positives compared to false negatives.

If you know before you fit your model that you want to compute one or more metrics, you can specify them in a call to \texttt{metric\_set()}. Let's set up a tuning grid for two new classification metrics, \texttt{recall} and \texttt{precision}, that focus not on the overall proportion of observations that are predicted correctly but instead on false positives and false negatives.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_rs }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{  nb\_wf,}
\NormalTok{  complaints\_folds,}
  \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(recall, precision)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you have already fit your model, you can still compute and explore non-default metrics as long as you saved the predictions for your resampled data sets using \texttt{control\_resamples(save\_pred\ =\ TRUE)}.

Let's go back to the naive Bayes model we tuned in Section \ref{classfirstmodel}, with predictions stored in \texttt{nb\_rs\_predictions}. We can compute the overall recall.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_rs\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{recall}\NormalTok{(product, .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 recall  binary         0.722
\end{verbatim}

We can also compute the recall for each resample using \texttt{group\_by()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb\_rs\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{recall}\NormalTok{(product, .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 4
#>    id     .metric .estimator .estimate
#>    <chr>  <chr>   <chr>          <dbl>
#>  1 Fold01 recall  binary         0.791
#>  2 Fold02 recall  binary         0.690
#>  3 Fold03 recall  binary         0.674
#>  4 Fold04 recall  binary         0.8  
#>  5 Fold05 recall  binary         0.719
#>  6 Fold06 recall  binary         0.735
#>  7 Fold07 recall  binary         0.713
#>  8 Fold08 recall  binary         0.655
#>  9 Fold09 recall  binary         0.717
#> 10 Fold10 recall  binary         0.725
\end{verbatim}

Many of the metrics used for classification are functions of the true positive, true negative, false positive, and false negative rates.
The \index{matrix!confusion}confusion matrix, a contingency table of observed classes and predicted classes, gives us information on these rates directly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{conf\_mat\_resampled}\NormalTok{(nb\_rs, }\AttributeTok{tidy =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>        Credit  Other
#> Credit 3009.5  549.1
#> Other  1157.4 4075.1
\end{verbatim}

It is possible with many data sets to achieve high accuracy just by predicting the majority class all the time, but such a model is not useful in the real world. Accuracy alone is often not a good way to assess the performance of classification models.

\begin{rmdnote}
For the full set of classification metric options, see the
\href{https://yardstick.tidymodels.org/reference/}{yardstick
documentation}.
\end{rmdnote}

\hypertarget{mlclassificationfull}{%
\section{The full game: classification}\label{mlclassificationfull}}

We have come a long way from our first classification model in Section \ref{classfirstmodel} and it is time to see how we can use what we have learned to improve it.
We started this chapter with a simple naive Bayes model and token counts.
Since then have we looked at different models, preprocessing techniques, and domain-specific feature engineering.
For our final model, let's use some of the domain-specific features\index{feature engineering} we developed in Section \ref{customfeatures} along with our lasso regularized classification model and tune both the regularization penalty as well as the number of tokens to include. For this final model we will:

\begin{itemize}
\item
  train on the same set of cross-validation resamples used throughout this chapter,
\item
  include text (but not \texttt{tags} or date features, since those did not result in better performance),
\item
  tune the number of tokens used in the model,
\item
  include unigrams only,
\item
  include custom-engineered features,
\item
  finally evaluate on the testing set, which we have not touched at all yet.
\end{itemize}

\hypertarget{feature-selection}{%
\subsection{Feature selection}\label{feature-selection}}

We start by creating a new preprocessing recipe, using only the text of the complaints for feature engineering.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints\_rec\_v2 }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(product }\SpecialCharTok{\textasciitilde{}}\NormalTok{ consumer\_complaint\_narrative, }\AttributeTok{data =}\NormalTok{ complaints\_train)}
\end{Highlighting}
\end{Shaded}

After exploring this text data more in Section \ref{customfeatures}, we want to add these custom features to our final model.
To do this, we use \texttt{step\_textfeature()} to compute custom text features.
We create a list of the custom text features and pass this list to \texttt{step\_textfeature()} via the \texttt{extract\_functions} argument.
Note how we have to take a copy of \texttt{consumer\_complaint\_narrative} using \texttt{step\_mutate()} as \texttt{step\_textfeature()} consumes the column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extract\_funs }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{creditcard\_count =}\NormalTok{ creditcard\_count,}
                     \AttributeTok{percent\_censoring =}\NormalTok{ percent\_censoring,}
                     \AttributeTok{max\_money =}\NormalTok{ max\_money)}

\NormalTok{complaints\_rec\_v2 }\OtherTok{\textless{}{-}}\NormalTok{ complaints\_rec\_v2 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_mutate}\NormalTok{(}\AttributeTok{narrative\_copy =}\NormalTok{ consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_textfeature}\NormalTok{(narrative\_copy, }\AttributeTok{extract\_functions =}\NormalTok{ extract\_funs)}
\end{Highlighting}
\end{Shaded}

The tokenization will be similar to the other models in this chapter.
In our original model, we only included 1000 tokens; for our final model, let's treat the number of tokens as a hyperparameter that we vary when we tune the final model.
Let's also set the \texttt{min\_times} argument to 100, to throw away tokens that appear less than 100 times in the entire corpus.
We want our model to be robust and a token needs to appear enough times before we include it.

\begin{rmdnote}
This data set has many more than 100 of even the most common 5000 or
more tokens, but it can still be good practice to specify
\texttt{min\_times} to be safe. Your choice for \texttt{min\_times}
should depend on your data and how robust you need your model to be.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints\_rec\_v2 }\OtherTok{\textless{}{-}}\NormalTok{ complaints\_rec\_v2 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(consumer\_complaint\_narrative,}
                   \AttributeTok{max\_tokens =} \FunctionTok{tune}\NormalTok{(), }\AttributeTok{min\_times =} \DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(consumer\_complaint\_narrative)}
\end{Highlighting}
\end{Shaded}

\hypertarget{specify-the-model-1}{%
\subsection{Specify the model}\label{specify-the-model-1}}

We use a lasso regularized classifier since it performed well throughout this chapter. We can reuse parts of the old workflow \texttt{sparse\_wf} from Section \ref{casestudysparseencoding} and update the recipe specification.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sparse\_wf\_v2 }\OtherTok{\textless{}{-}}\NormalTok{ sparse\_wf }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{update\_recipe}\NormalTok{(complaints\_rec\_v2, }\AttributeTok{blueprint =}\NormalTok{ sparse\_bp)}

\NormalTok{sparse\_wf\_v2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: logistic_reg()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 5 Recipe Steps
#> 
#> * step_mutate()
#> * step_textfeature()
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> 
#> -- Model -----------------------------------------------------------------------
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

Before we tune the model, we need to set up a set of possible parameter values to try.

\begin{rmdwarning}
There are \emph{two} tunable parameters in this model, the
regularization parameter and the maximum number of tokens included in
the model.
\end{rmdwarning}

Let's include different possible values for each parameter, for a combination of 60 models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}
  \FunctionTok{penalty}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
  \FunctionTok{max\_tokens}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\FloatTok{1e3}\NormalTok{, }\FloatTok{3e3}\NormalTok{)),}
  \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\AttributeTok{penalty =} \DecValTok{20}\NormalTok{, }\AttributeTok{max\_tokens =} \DecValTok{3}\NormalTok{)}
\NormalTok{)}

\NormalTok{final\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 60 x 2
#>     penalty max_tokens
#>       <dbl>      <int>
#>  1 0.0001         1000
#>  2 0.000162       1000
#>  3 0.000264       1000
#>  4 0.000428       1000
#>  5 0.000695       1000
#>  6 0.00113        1000
#>  7 0.00183        1000
#>  8 0.00298        1000
#>  9 0.00483        1000
#> 10 0.00785        1000
#> # ... with 50 more rows
\end{verbatim}

\begin{rmdpackage}
We used \texttt{grid\_regular()} here where we fit a model at every combination of parameters, but if you have a model with many tuning parameters, you may wish to try a space-filling grid instead, such as \texttt{grid\_max\_entropy()} or \texttt{grid\_latin\_hypercube()}. The \textbf{tidymodels} package for creating and handling tuning parameters and parameter grids is \textbf{dials} \citep{R-dials}.
\end{rmdpackage}

Now it's time to set up our tuning grid. Let's save the predictions so we can explore them in more detail, and let's also set custom metrics instead of using the defaults. Let's compute accuracy, sensitivity, and specificity during tuning. Sensitivity and specificity are closely related to recall and precision.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2020}\NormalTok{)}
\NormalTok{tune\_rs }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{  sparse\_wf\_v2,}
\NormalTok{  complaints\_folds,}
  \AttributeTok{grid =}\NormalTok{ final\_grid,}
  \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(accuracy, sensitivity, specificity)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have fitted these classification models!

\hypertarget{classification-final-evaluation}{%
\subsection{Evaluate the modeling}\label{classification-final-evaluation}}

Now that all of the models with possible parameter values have been trained, we can compare their performance. Figure \ref{fig:complaintsfinaltunevis} shows us the relationship between performance (as measured by the metrics we chose), the number of tokens, and regularization.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(tune\_rs) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{color =} \StringTok{"Number of tokens"}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Model performance across regularization penalties and tokens"}\NormalTok{,}
    \AttributeTok{subtitle =} \FunctionTok{paste}\NormalTok{(}\StringTok{"We can choose a simpler model with higher regularization"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/complaintsfinaltunevis-1} 

}

\caption{Model performance is similar for the higher token options so we can choose a simpler model. Note the logarithmic scale on the x-axis for the regularization penalty.}\label{fig:complaintsfinaltunevis}
\end{figure}

Since this is our final version of this model, we want to choose final parameters and update our model object so we can use it with new data. We have several options for choosing our final parameters, such as selecting the numerically best model. Instead, let's choose a simpler model within some limit around that numerically best result, with more regularization that gives close-to-best performance. Let's choose by percent loss compared to the best model (the default choice is 2\% loss), and let's say we care most about overall accuracy (rather than sensitivity or specificity).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{choose\_acc }\OtherTok{\textless{}{-}}\NormalTok{ tune\_rs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select\_by\_pct\_loss}\NormalTok{(}\AttributeTok{metric =} \StringTok{"accuracy"}\NormalTok{, }\SpecialCharTok{{-}}\NormalTok{penalty)}

\NormalTok{choose\_acc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 10
#>   penalty max_tokens .metric  .estimator  mean     n std_err .config .best .loss
#>     <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>   <dbl> <dbl>
#> 1 0.00483       1000 accuracy binary     0.882    10 9.44e-4 Prepro~ 0.898  1.78
\end{verbatim}

After we have those parameters, \texttt{penalty} and \texttt{max\_tokens}, we can finalize our earlier tunable workflow, by updating it with this value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_wf }\OtherTok{\textless{}{-}} \FunctionTok{finalize\_workflow}\NormalTok{(sparse\_wf\_v2, choose\_acc)}
\NormalTok{final\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: logistic_reg()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 5 Recipe Steps
#> 
#> * step_mutate()
#> * step_textfeature()
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> 
#> -- Model -----------------------------------------------------------------------
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = 0.00483293023857175
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

The \texttt{final\_wf} workflow now has finalized values for \texttt{max\_tokens} and \texttt{penalty}.

We can now fit this finalized workflow on training data and \emph{finally} return to our testing data.

\begin{rmdwarning}
Notice that this is the first time we have used our testing data during
this entire chapter; we tuned and compared models using resampled data
sets instead of touching the testing set.
\end{rmdwarning}

We can use the function \texttt{last\_fit()} to \textbf{fit} our model one last time on our training data and \textbf{evaluate} it on our testing data. We only have to pass this function our finalized model/workflow and our data split.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_fitted }\OtherTok{\textless{}{-}} \FunctionTok{last\_fit}\NormalTok{(final\_wf, complaints\_split)}

\FunctionTok{collect\_metrics}\NormalTok{(final\_fitted)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 4
#>   .metric  .estimator .estimate .config             
#>   <chr>    <chr>          <dbl> <chr>               
#> 1 accuracy binary         0.885 Preprocessor1_Model1
#> 2 roc_auc  binary         0.949 Preprocessor1_Model1
\end{verbatim}

The metrics for the test set look about the same as the resampled training data and indicate we did not overfit during tuning. The accuracy of our final model has improved compared to our earlier models, both because we are combining multiple preprocessing steps and because we have tuned the number of tokens.

The \index{matrix!confusion}confusion matrix on the testing data in Figure \ref{fig:finalheatmap} also yields pleasing results. It appears symmetric with a strong presence on the diagonal, showing that there isn't any strong bias towards either of the classes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_predictions}\NormalTok{(final\_fitted) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ product, }\AttributeTok{estimate =}\NormalTok{ .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/finalheatmap-1} 

}

\caption{Confusion matrix on the test set for final lasso regularized classifier}\label{fig:finalheatmap}
\end{figure}

Figure \ref{fig:finalroccurve} shows the ROC curve for testing set, to demonstrate how well this final classification model can distinguish between the two classes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{collect\_predictions}\NormalTok{(final\_fitted)  }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ product, .pred\_Credit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{color =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"ROC curve for US Consumer Finance Complaints"}\NormalTok{,}
    \AttributeTok{subtitle =} \StringTok{"With final tuned lasso regularized classifier on the test set"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/finalroccurve-1} 

}

\caption{ROC curve with the test set for final lasso regularized classifier}\label{fig:finalroccurve}
\end{figure}

The output of \texttt{last\_fit()} also contains a fitted model (a \texttt{workflow}, to be more specific), that has been trained on the \emph{training} data. We can use the vip package to understand what the most important variables are in the predictions, shown in Figure \ref{fig:complaintsvip}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vip)}

\NormalTok{complaints\_imp }\OtherTok{\textless{}{-}} \FunctionTok{pull\_workflow\_fit}\NormalTok{(final\_fitted}\SpecialCharTok{$}\NormalTok{.workflow[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{vi}\NormalTok{(}\AttributeTok{lambda =}\NormalTok{ choose\_acc}\SpecialCharTok{$}\NormalTok{penalty)}

\NormalTok{complaints\_imp }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{Sign =} \FunctionTok{case\_when}\NormalTok{(Sign }\SpecialCharTok{==} \StringTok{"POS"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Less about credit reporting"}\NormalTok{,}
\NormalTok{                     Sign }\SpecialCharTok{==} \StringTok{"NEG"} \SpecialCharTok{\textasciitilde{}} \StringTok{"More about credit reporting"}\NormalTok{),}
    \AttributeTok{Importance =} \FunctionTok{abs}\NormalTok{(Importance),}
    \AttributeTok{Variable =} \FunctionTok{str\_remove\_all}\NormalTok{(Variable, }\StringTok{"tfidf\_consumer\_complaint\_narrative\_"}\NormalTok{),}
    \AttributeTok{Variable =} \FunctionTok{str\_remove\_all}\NormalTok{(Variable, }\StringTok{"textfeature\_narrative\_copy\_"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Sign) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{20}\NormalTok{, Importance) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  ungroup }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Importance,}
             \AttributeTok{y =} \FunctionTok{fct\_reorder}\NormalTok{(Variable, Importance),}
             \AttributeTok{fill =}\NormalTok{ Sign)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Sign, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{y =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Variable importance for predicting the topic of a CFPB complaint"}\NormalTok{,}
    \AttributeTok{subtitle =} \FunctionTok{paste}\NormalTok{(}\StringTok{"These features are the most important in predicting"}\NormalTok{,}
                     \StringTok{"whether a complaint is about credit or not"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{07_ml_classification_files/figure-latex/complaintsvip-1} 

}

\caption{Some words increase a CFPB complaint's probability of being about credit reporting while some decrease that probability}\label{fig:complaintsvip}
\end{figure}

Tokens like ``interest'', ``bank'', and ``escrow'' contribute in this model away from a classification as about credit reporting, while tokens like the names of the credit reporting agencies, ``reporting'', and ``report'' and contribute in this model \emph{toward} classification as about credit reporting.

\begin{rmdnote}
The top features we see here are all tokens learned directly from the
text. None of our hand-crafted custom features, like
\texttt{percent\_censoring} or \texttt{max\_money} are top features in
terms of variable importance. In many cases, it can be difficult to
create features from text that perform better than the tokens
themselves.
\end{rmdnote}

We can gain some final insight into our model by looking at observations from the test set that it \emph{misclassified}. Let's bind together the predictions on the test set with the original \texttt{complaints\_test} data. Then let's look at complaints that were labeled as about credit reporting in the original data but that our final model thought had a low probability of being about credit reporting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints\_bind }\OtherTok{\textless{}{-}} \FunctionTok{collect\_predictions}\NormalTok{(final\_fitted) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(complaints\_test }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{product))}

\NormalTok{complaints\_bind }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(product }\SpecialCharTok{==} \StringTok{"Credit"}\NormalTok{, .pred\_Credit }\SpecialCharTok{\textless{}} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 1
#>    consumer_complaint_narrative                                                 
#>    <chr>                                                                        
#>  1 "Spoke with Mr XXXX today at XXXX. Inquired about medical bill and date. Inf~
#>  2 "I opened a credit card account with GE Financial to finance an air conditio~
#>  3 "I lost my debit card and had to use my checks until I received my new card.~
#>  4 "Loan was for {$2500.00} balance is showing {$4200.00} because they included~
#>  5 "Chase XXXX Reward card was activated in my name without my consent. Card # ~
#>  6 "I have had this service for more the seven years, the more I use them the m~
#>  7 "Ive had severe issues with the student loan process for at least 10 years. ~
#>  8 "Chase Card Address : XXXX XXXX XXXX City/ State/ Zip : XXXX, DE XXXX Date :~
#>  9 "I received a notice that stated that I was currently in debt in the amount ~
#> 10 "Hi About 2 months ago ( XXXX ) I received an email from a company represent~
\end{verbatim}

We can see why some of these would be difficult for our model to classify as about credit reporting, since some are about other topics as well. The original label may also be incorrect in some cases.

What about misclassifications in the other direction, observations in the test set that were \emph{not} labeled as about credit reporting but that our final model gave a high probability of being about credit reporting?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{complaints\_bind }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(product }\SpecialCharTok{==} \StringTok{"Other"}\NormalTok{, .pred\_Credit }\SpecialCharTok{\textgreater{}} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(consumer\_complaint\_narrative) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 1
#>    consumer_complaint_narrative                                                 
#>    <chr>                                                                        
#>  1 "Paid in full collections to CBE Group amount of {$360.00} paid in XXXX of 2~
#>  2 "Back in 2013, my purse was stolen containing all of my personal belongings.~
#>  3 "I Have contacted Credit Bureaus on numerous occasions to have incorrect or ~
#>  4 "I contacted the company, I reported to police department, sent in police re~
#>  5 "FCRA states information reporting has to be 100 % verifiable and 100 % accu~
#>  6 "I have attempted on numerous times to dispute an account that has ERRORS. E~
#>  7 "XXXX  OF XXXX FLORIDA IS TAKING ADVANTAGE OF THEIR ABILITY TO REPORT TO THE~
#>  8 "Late payment reported ( {$16.00} ) to my credit report that caused my credi~
#>  9 "MIDLAND FUNDING XXXX as of XX/XX/2019 reporting for identity fraud, was rep~
#> 10 "I have contacted this company through the credit bureaus multiple times to ~
\end{verbatim}

Again, these are ``mistakes''\index{models!challenges} on the part of the model that we can understand based on the content of these complaints. The original labeling on the complaints looks to be not entirely correct or consistent, typical of real data from the real world.

\hypertarget{mlclassificationsummary}{%
\section{Summary}\label{mlclassificationsummary}}

You can use classification modeling to predict labels or categorical variables from a data set, including data sets that include text.
Naive Bayes models can perform well with text data since each feature is handled independently and thus large numbers of features are computational feasible.
This is important as bag-of-word text models can involve thousands of tokens.
We also saw that regularized linear models, such as lasso, often work well for text data sets.
Your own domain knowledge about your text data is valuable, and using that knowledge in careful engineering of custom features can improve your model in some cases.

\hypertarget{in-this-chapter-you-learned-6}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-6}}

\begin{itemize}
\item
  how text data can be used in a classification model
\item
  to tune hyperparameters of a model
\item
  how to compare different model types
\item
  that models can combine both text and non-text predictors
\item
  about engineering custom features for machine learning
\item
  about performance metrics for classification models
\end{itemize}

\hypertarget{part-deep-learning-methods}{%
\part{Deep Learning Methods}\label{part-deep-learning-methods}}

\hypertarget{dlforeword}{%
\chapter*{Foreword}\label{dlforeword}}


\thispagestyle{myheadings}

In Chapters \ref{mlregression} and \ref{mlclassification}, we use algorithms such as regularized linear models, support vector machines, and naive Bayes models to predict outcomes from predictors including text data. Deep learning models approach the same tasks and have the same goals, but the algorithms involved are different. Deep learning models are ``deep'' in the sense that they use multiple layers to learn how to map from input features to output outcomes; this is in contrast to the kinds of models we used in the previous two chapters which use a shallow (single) mapping.

\begin{rmdnote}
Deep learning models can be effective for text prediction problems
because they use these multiple layers to capture complex relationships
in language.
\end{rmdnote}

The layers in a deep learning model are connected in a network and these models are called \textbf{neural networks}, although they do not work much like a human brain. The layers can be connected in different configurations called network architectures, which sometimes incorporate word embeddings\index{embeddings}, as described in Chapter \ref{embeddings}. We will cover three network architectures\index{network architecture} in the following chapters:

\begin{itemize}
\item
  Chapter \ref{dldnn} starts our exploration of deep learning for text with a densely connected neural network. Think of this more straightforward architecture as a bridge between the ``shallow'' learning approaches of Chapters \ref{mlregression} and \ref{mlclassification} that treated text as a bag of words and the more complex architectures to come.
\item
  Chapter \ref{dllstm} continues by walking through how to train and evaluate a more advanced architecture, a long short-term memory (LSTM) network. LSTMs are among the most common architectures used for text data because they model text as a long sequence of words or characters.
\item
  Chapter \ref{dlcnn} wraps up our treatment of deep learning for text with the convolutional neural network (CNN) architecture. CNNs are another advanced architecture appropriate for text data because they can capture specific local patterns.
\end{itemize}

Our discussion of network architectures\index{network architecture} is fairly specific to text data; in other situations you may do best using a different architecture, for example, when working with dense, tabular data.

For the following chapters, we will use tidymodels packages along with \href{https://www.tensorflow.org/}{Tensorflow} and the R interface to Keras \citep{R-keras} for preprocessing, modeling, and evaluation.

\thispagestyle{myheadings}

\begin{rmdpackage}
The \textbf{keras} R package provides an interface for R users to Keras,
a high-level API for building neural networks.
\end{rmdpackage}

The following table presents some key differences between deep learning and what, in this book, we call machine learning methods.\index{machine learning!comparing}\index{deep learning!comparing}

\begin{tabular}{>{\raggedright\arraybackslash}p{55mm}>{\raggedright\arraybackslash}p{55mm}}
\toprule
\textbf{Machine learning} & \textbf{Deep learning}\\
\midrule
Faster to train & Takes more time to train\\
Software is typically easier to install & Software can be more challenging to install\\
Can achieve good performance with less data & Requires more data for good performance\\
Depends on preprocessing to model more than very simple relationships & Can model highly complex relationships\\
\bottomrule
\end{tabular}

Deep learning and more traditional machine learning algorithms are different, but the structure of the modeling process is largely the same, no matter what the specific details of prediction or algorithm are.

\hypertarget{spending-your-data-budget}{%
\section*{Spending your data budget}\label{spending-your-data-budget}}


A limited amount of data is available for any given modeling project, and this data must be allocated to different tasks to balance competing priorities. We espouse an approach of first splitting data in testing and training sets, holding the testing set back until all modeling tasks are completed, including feature engineering and tuning. This testing set is then used as a final check on model performance, to estimate how the final model will perform on new data.

The training data is available for tasks from model parameter estimation to determining which features are important and more. To compare or tune model options or parameters, this training set can be further split so that models can be evaluated on a validation set, or it can be resampled as described in Section \ref{firstregressionevaluation} to create new simulated data sets for the purpose of evaluation.

\hypertarget{feature-engineering}{%
\section*{Feature engineering}\label{feature-engineering}}


\thispagestyle{myheadings}

\index{feature engineering}Text data requires extensive processing\index{preprocessing} to be appropriate for modeling, whether via an algorithm like regularized regression or a neural network. Chapters \ref{language} through \ref{embeddings} covered several of the most important techniques that are used to transform language into a representation appropriate for computation. This feature engineering part of the modeling process can be intensive for text, sometimes more computationally expensive than the fitting a model algorithm.

We espouse an approach of implementing feature engineering on training data only, typically using resampled data sets, to avoid obtaining an overly optimistic estimate of model performance. Feature engineering can sometimes be a part of the machine learning process where subtle \emph{data leakage}\index{data leakage} occurs, when practitioners use information (say, to preprocess data or engineer features) that will not be available at prediction time. One example of this is tf-idf, which we introduced in Chapter \ref{embeddings} and used in both Chapters \ref{mlregression} and \ref{mlclassification}. As a reminder, the term frequency\index{term frequency} of a word is how frequently a word occurs in a document, and the inverse document frequency\index{inverse document frequency} of a word is a weight, typically defined as:

\[idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}\]

These two quantities are multiplied together to compute a term's tf-idf\index{tf-idf}, a statistic that measures the frequency of a term adjusted for how rarely it is used. Computing inverse document frequency involves the whole corpus\index{corpus} or collection of documents. When you are fitting a model, how should that corpus be defined? We strongly advise that you should use your \emph{training data only} in such a situation. Using all the data available to you (training plus testing) involves leakage of information from the testing set to the model, and any estimates you may make of how your model may perform on new data may be overly optimistic.

\index{feature engineering}

\begin{rmdwarning}
This specific example focused on tf-idf, but data leakage is a serious
challenge in general for building machine learning models and systems.
The tidymodels framework is designed to encourage good statistical
practice, such as learning feature engineering transformations from
training data and then applying those transformation to othe data sets.
\end{rmdwarning}

\hypertarget{fitting-and-tuning}{%
\section*{Fitting and tuning}\label{fitting-and-tuning}}


\thispagestyle{myheadings}

Many different kinds of models are appropriate for text data, from more straightforward models like the linear models explored deeply in Chapter \ref{mlregression} to the neural network models we cover in Chapters \ref{dlcnn} and \ref{dllstm}. Some of these models have hyperparameters which cannot be learned from data during fitting, like the regularization parameter of the models in Chapter \ref{mlregression}; these hyperparameters can be tuned using resampled data sets.

\hypertarget{model-evaluation}{%
\section*{Model evaluation}\label{model-evaluation}}


\index{models!analysis}Once models are trained and perhaps tuned, we can evaluate their performance quantitatively using metrics appropriate for the kind of practical problem being dealt with. Model explanation analysis, such as feature importance, also helps us understand how well and why models are behaving the way they do.

\hypertarget{putting-the-model-process-in-context}{%
\section*{Putting the model process in context}\label{putting-the-model-process-in-context}}


This outline of the model process depends on us as data practitioners coming prepared for modeling with a healthy understanding of our data sets from exploratory data analysis. \citet{Silge2017} provide a guide for exploratory data analysis for text.

Also, in practice, the structure of a real modeling project is iterative. After fitting and tuning a first model or set of a models, a practitioner will often return to build more or better features, then refit models, and evaluate in a more detailed way. Notice that we take this approach in each chapter, both for more straightforward machine learning and deep learning; we start with a simpler model and then go back again and again to improve it in several ways. This iterative approach is healthy and appropriate, as long as good practices in data ``spending'' are observed. The testing set cannot be used during this iterative back-and-forth, and using resampled data sets can set us as practitioners up for more accurate estimates of performance.

\thispagestyle{myheadings}

\hypertarget{dldnn}{%
\chapter{Dense neural networks}\label{dldnn}}

Like we discussed in the previous foreword, these three chapters on deep learning for text are organized by \index{network architecture}network architecture, rather than by outcome type as we did in Chapters \ref{mlregression} and \ref{mlclassification}.
We'll use Keras with its Tensorflow backend for these deep learning models; Keras is a well-established framework for deep learning with bindings in Python\index{Python} and, via reticulate \citep{R-reticulate}, R.
Keras provides an extensive, high-level API for creating and training many kinds of neural networks, but less support for resampling and preprocessing. Throughout this and the next chapters, we will demonstrate how to use tidymodels packages together with Keras to address these tasks.

\begin{rmdpackage}
The \textbf{tidymodels} framework of R packages is modular, so we can
use it for certain parts of our modeling analysis without committing to
it entirely, when appropriate.
\end{rmdpackage}

This chapter explores one of the most straightforward configurations for a deep learning model, a \index{neural network!densely connected}\textbf{densely connected neural network}. This is typically not a model that will achieve the highest performance on text data, but it is a good place to start to understand the process of building and evaluating deep learning models for text. We can also use this type of network architecture\index{network architecture} as a bridge between the bag-of-words approaches we explored in detail in Chapters \ref{mlregression} and \ref{mlclassification} to the approaches beyond bag-of-words we will use in Chapters \ref{dllstm} and \ref{dlcnn}. Deep learning allows us to incorporate not just word counts but also word sequences and positions.

Figure \ref{fig:dnndiag} depicts a densely connected neural network architecture \index{neural network!feed forward}\emph{feed-forward}. The input comes in to the network all at once and is densely (in this case, fully) connected to the first hidden layer. A layer is ``hidden'' in the sense that it doesn't connect to the outside world; the input and output layers take care of this. The neurons in any given layer are only connected to the next layer. The numbers of layers and nodes within each layer are variable and are hyperparameters of the model selected by the practitioner.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{diagram-files/dnn-architecture} 

}

\caption{A high-level diagram of a feed-forward neural network. The lines connecting the nodes are shaded differently to illustrate the different weights connecting units.}\label{fig:dnndiag}
\end{figure}

\index{neural network!feed forward}Figure \ref{fig:dnndiag} shows the input units with words, but this is not an entirely accurate representation of a neural network. These words will in practice be represented by embedding vectors because these networks can only work with numeric variables.

\hypertarget{kickstarter}{%
\section{Kickstarter data}\label{kickstarter}}

For all our chapters on deep learning, we will build binary classification models, much like we did in Chapter \ref{mlclassification}, but we will use neural networks instead of shallow learning models. As we discussed in the foreword to these deep learning chapters, much of the overall model process will look the same, but we will use a different kind of algorithm. We will use a data set of descriptions or ``blurbs'' for campaigns from the crowdfunding platform \href{https://www.kickstarter.com/}{Kickstarter}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{kickstarter }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/kickstarter.csv.gz"}\NormalTok{)}
\NormalTok{kickstarter}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 269,790 x 3
#>    blurb                                                        state created_at
#>    <chr>                                                        <dbl> <date>    
#>  1 Exploring paint and its place in a digital world.                0 2015-03-17
#>  2 Mike Fassio wants a side-by-side photo of me and Hazel eati~     0 2014-07-11
#>  3 I need your help to get a nice graphics tablet and Photosho~     0 2014-07-30
#>  4 I want to create a Nature Photograph Series of photos of wi~     0 2015-05-08
#>  5 I want to bring colour to the world in my own artistic skil~     0 2015-02-01
#>  6 We start from some lovely pictures made by us and we decide~     0 2015-11-18
#>  7 Help me raise money to get a drawing tablet                      0 2015-04-03
#>  8 I would like to share my art with the world and to do that ~     0 2014-10-15
#>  9 Post Card don’t set out to simply decorate stories. Our goa~     0 2015-06-25
#> 10 My name is Siu Lon Liu and I am an illustrator seeking fund~     0 2014-07-19
#> # ... with 269,780 more rows
\end{verbatim}

The \texttt{state} of each observation records whether the campaign was successful in its crowdfunding goal; a value of 1 means it was successful and a value of 0 means it was not successful. The texts for the campaign descriptions, contained in \texttt{blurb}, are short, less than a few hundred characters. What is the distribution of characters?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb))) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of characters per campaign blurb"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of campaign blurbs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/kickstartercharhist-1} 

}

\caption{Distribution of character count for Kickstarter campaign blurbs}\label{fig:kickstartercharhist}
\end{figure}

Figure \ref{fig:kickstartercharhist} shows that the distribution of characters per blurb is right-skewed, with two thresholds. Individuals creating campaigns don't have much space to make an impression, so most people choose to use most of it! There is an oddity in this chart, a steep drop somewhere between 130 and 140 with another threshold around 150 characters. Let's investigate to see if we can find the reason.

We can use \texttt{count()} to find the most common blurb length.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb), }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 151 x 2
#>    `nchar(blurb)`     n
#>             <int> <int>
#>  1            135 26827
#>  2            134 18726
#>  3            133 14913
#>  4            132 13559
#>  5            131 11322
#>  6            130 10083
#>  7            129  8786
#>  8            128  7874
#>  9            127  7239
#> 10            126  6590
#> # ... with 141 more rows
\end{verbatim}

Let's use our own eyes to see what happens around this cutoff point. We can use \texttt{slice\_sample()} to draw a few random blurbs.

Were the blurbs truncated at 135 characters? Let's look at some blurbs with exactly 135 characters.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb) }\SpecialCharTok{==} \DecValTok{135}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(blurb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "A science fiction/drama about a young man and woman encountering beings
not of this earth. Armed with only their minds to confront this"
#> [2] "No, not my virginity. That was taken by a girl named Ramona the night
of my senior prom. I'm talking about my novel, THE USE OF REGRET."
#> [3] "In a city where the sun has stopped rising, the music never stops. Now
only a man and his guitar can free the people from the Red King."
#> [4] "First Interfaith & Community FM Radio Station needs transmitter in
Menifee, CA Programs online, too CLICK PHOTO ABOVE FOR OUR CAT VIDEO"
#> [5] "This documentary asks if the twenty-four hour news cycle has altered
people's opinions of one another. We explore unity in one another."
\end{verbatim}

All of these blurbs appear coherent and some of them even end with a period to end the sentence. Let's now look at blurbs with more than 135 characters to see if they are different.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb) }\SpecialCharTok{\textgreater{}} \DecValTok{135}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(blurb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "This is a puzzle game for the Atari 2600. The unique thing about this
is that (some) of the cartridge cases will be made out of real wood, hand
carved"
#> [2] "Art supplies for 10 girls on the east side of Detroit to make drawings
of their neighborhood, which is also home to LOVELAND's Plymouth microhood"
#> [3] "Help us make a video for 'Never', one of the most popular songs on
Songs To Wear Pants To and the lead single from Your Heart's upcoming album
Autumn."
#> [4] "Pyramid Cocoon is an interactive sculpture to be installed during the
Burning Man Festival 2010. Users can rest, commune or cocoon in the piece"
#> [5] "Back us to own, wear, or see a show of great student art we've
collected from Artloop partner schools in NYC. The $ goes right back to art
programs!"
\end{verbatim}

All of these blurbs also look fine so the strange distribution doesn't seem like a data collection issue.

The \texttt{kickstarter} data set also includes a \texttt{created\_at} variable; let's explore that next. Figure \ref{fig:kickstarterheatmap} is a heatmap of the lengths of blurbs and the time the campaign was posted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(created\_at, }\FunctionTok{nchar}\NormalTok{(blurb))) }\SpecialCharTok{+}
  \FunctionTok{geom\_bin2d}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of characters per campaign blurb"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/kickstarterheatmap-1} 

}

\caption{Distribution of character count for Kickstarter campaign blurbs over time}\label{fig:kickstarterheatmap}
\end{figure}

That looks like the explanation! It appears that at the end of 2010 there was a policy change in the blurb length, shortening from 150 characters to 135 characters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb) }\SpecialCharTok{\textgreater{}} \DecValTok{135}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{max}\NormalTok{(created\_at))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 1
#>   `max(created_at)`
#>   <date>           
#> 1 2010-10-20
\end{verbatim}

We can't say for sure if the change happened on 2010-10-20, but that is the last day a campaign was launched with more than 135 characters.

\hypertarget{firstdlclassification}{%
\section{A first deep learning model}\label{firstdlclassification}}

Like all our previous modeling, our first step is to split our data into training and testing sets. We will still use our training set to build models and save the testing set for a final estimate of how our model will perform on new data.

\index{overfitting}\index{bias}

\begin{rmdwarning}
It is very easy to overfit deep learning models, so an unbiased estimate
of future performance from a test set is more important than ever.
\end{rmdwarning}

We use \texttt{initial\_split()} to define the training and testing splits. We will focus on modeling the blurb alone in these deep learning chapters. Also, we will restrict our modeling analysis to only include blurbs with more than 15 characters, because the shortest blurbs tend to consist of uninformative single words.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{kickstarter\_split }\OtherTok{\textless{}{-}}\NormalTok{ kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb) }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{initial\_split}\NormalTok{()}

\NormalTok{kickstarter\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(kickstarter\_split)}
\NormalTok{kickstarter\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(kickstarter\_split)}
\end{Highlighting}
\end{Shaded}

There are 202,093 blurbs in the training set and 67,364 in the testing set.

\hypertarget{dnnrecipe}{%
\subsection{Preprocessing for deep learning}\label{dnnrecipe}}

\index{preprocessing}Preprocessing for deep learning models is different than preprocessing for most other text models. These neural networks model \emph{sequences}, so we have to choose the length of sequences we would like to include. Documents that are longer than this length are truncated (information is thrown away) and documents that are shorter than this length are padded with zeroes (an empty, non-informative value) to get to the chosen sequence length. This sequence length is a hyperparameter of the model and we need to select this value such that we don't:

\begin{itemize}
\item
  overshoot and introduce a lot of padded zeroes which would make the model hard to train, or
\item
  undershoot and cut off too much informative text from our documents.
\end{itemize}

We can use the \texttt{count\_words()} function from the tokenizers package to calculate the number of words and generate a histogram in Figure \ref{fig:kickstarterwordlength}. Notice how we are only using the training data set to avoid data leakage\index{data leakage} when selecting this value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter\_train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{n\_words =}\NormalTok{ tokenizers}\SpecialCharTok{::}\FunctionTok{count\_words}\NormalTok{(blurb)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n\_words)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of words per campaign blurb"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of campaign blurbs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/kickstarterwordlength-1} 

}

\caption{Distribution of word count for Kickstarter campaign blurbs}\label{fig:kickstarterwordlength}
\end{figure}

Given that we don't have many words for this particular data set to begin with, let's err on the side of longer sequences so we don't lose valuable data. Let's try 30 words for our threshold \texttt{max\_length}, and let's include 20,000 words in our vocabulary.

\begin{rmdpackage}
We will use the \textbf{recipes} and \textbf{textrecipes} packages for data preprocessing and feature engineering for our deep learning models, just like we did for our models in Chapters \ref{mlregression} and \ref{mlclassification}. To use a recipe, we first specify it with the variables we want to include and the steps we want to use in feature engineering.
\end{rmdpackage}
\index{feature engineering}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textrecipes)}

\NormalTok{max\_words }\OtherTok{\textless{}{-}} \FloatTok{2e4}
\NormalTok{max\_length }\OtherTok{\textless{}{-}} \DecValTok{30}

\NormalTok{kick\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =}\NormalTok{ max\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(blurb, }\AttributeTok{sequence\_length =}\NormalTok{ max\_length)}

\NormalTok{kick\_rec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>  predictor          1
#> 
#> Operations:
#> 
#> Tokenization for blurb
#> Text filtering for blurb
#> Sequence 1 hot encoding for blurb
\end{verbatim}

The formula used to specify this recipe \texttt{\textasciitilde{}\ blurb} does not have an outcome, because we are using \textbf{recipes} and \textbf{textrecipes} functions on their own, outside of the rest of the tidymodels framework; we don't need to know about the outcome here.
This \index{preprocessing}preprocessing recipe tokenizes our text (Chapter \ref{tokenization}) and filters to keep only the top 20,000 words, but then it transforms the tokenized text in a new way to prepare for deep learning that we have not used in this book before, using \texttt{step\_sequence\_onehot()}.

\hypertarget{onehotsequence}{%
\subsection{One-hot sequence embedding of text}\label{onehotsequence}}

The function \texttt{step\_sequence\_onehot()} transforms tokens into a numeric format appropriate for modeling, like \texttt{step\_tf()} and \texttt{step\_tfidf()}. However, it is different in that it takes into account the order of the tokens, unlike \texttt{step\_tf()} and \texttt{step\_tfidf()} which do not take order into account.

\begin{rmdnote}
Steps like \texttt{step\_tf()} and \texttt{step\_tfidf()} are used for
approaches called ``bag of words'', meaning the words are treated like
they are just thrown in a bag without attention paid to their order.
\end{rmdnote}

Let's take a closer look at how \texttt{step\_sequence\_onehot()} works and how its parameters will change the output.

When we use \texttt{step\_sequence\_onehot()}, two things happen. First, each word is assigned an\index{integer index} \emph{integer index}. You can think of this as a key-value pair of the vocabulary. Next, the sequence of tokens is replaced with the corresponding indices; this sequence of integers makes up the final numeric representation. Let's illustrate with a small example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{small\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{text =} \FunctionTok{c}\NormalTok{(}\StringTok{"Adventure Dice Game"}\NormalTok{,}
           \StringTok{"Spooky Dice Game"}\NormalTok{,}
           \StringTok{"Illustrated Book of Monsters"}\NormalTok{,}
           \StringTok{"Monsters, Ghosts, Goblins, Me, Myself and I"}\NormalTok{)}
\NormalTok{)}

\NormalTok{small\_spec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ small\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(text, }\AttributeTok{sequence\_length =} \DecValTok{6}\NormalTok{, }\AttributeTok{prefix =} \StringTok{""}\NormalTok{)}

\FunctionTok{prep}\NormalTok{(small\_spec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>  predictor          1
#> 
#> Training data contained 4 data points and no missing data.
#> 
#> Operations:
#> 
#> Tokenization for text [trained]
#> Sequence 1 hot encoding for text [trained]
\end{verbatim}

\begin{rmdwarning}
What does the function \texttt{prep()} do? Before when we have used
recipes, we put them in a \texttt{workflow()} which handles low-level
processing. The \texttt{prep()} function will compute or estimate
statistics from the training set; the output of \texttt{prep()} is a
prepped recipe.
\end{rmdwarning}

Once we have the prepped recipe, we can \texttt{tidy()} it to extract the vocabulary, represented in the \texttt{vocabulary} and \texttt{token} columns\footnote{The \texttt{terms} column refers to the column we have applied \texttt{step\_sequence\_onehot()} to and \texttt{id} is its unique identifier. Note that \textbf{textrecipes} allows \texttt{step\_sequence\_onehot()} to be applied to multiple text variables independently and they will have their own vocabularies.}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prep}\NormalTok{(small\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 14 x 4
#>    terms vocabulary token       id                   
#>    <chr>      <int> <chr>       <chr>                
#>  1 text           1 adventure   sequence_onehot_9SVGf
#>  2 text           2 and         sequence_onehot_9SVGf
#>  3 text           3 book        sequence_onehot_9SVGf
#>  4 text           4 dice        sequence_onehot_9SVGf
#>  5 text           5 game        sequence_onehot_9SVGf
#>  6 text           6 ghosts      sequence_onehot_9SVGf
#>  7 text           7 goblins     sequence_onehot_9SVGf
#>  8 text           8 i           sequence_onehot_9SVGf
#>  9 text           9 illustrated sequence_onehot_9SVGf
#> 10 text          10 me          sequence_onehot_9SVGf
#> 11 text          11 monsters    sequence_onehot_9SVGf
#> 12 text          12 myself      sequence_onehot_9SVGf
#> 13 text          13 of          sequence_onehot_9SVGf
#> 14 text          14 spooky      sequence_onehot_9SVGf
\end{verbatim}

If we take a look at the resulting matrix, we have one row per observation. The first row starts with some padded zeroes but then contains 1, 4, and 5, which we can use together with the vocabulary to construct the original sentence.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prep}\NormalTok{(small\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      _text_1 _text_2 _text_3 _text_4 _text_5 _text_6
#> [1,]       0       0       0       1       4       5
#> [2,]       0       0       0      14       4       5
#> [3,]       0       0       9       3      13      11
#> [4,]       6       7      10      12       2       8
\end{verbatim}

\begin{rmdwarning}
When we \texttt{bake()} a prepped recipe, we apply the preprocessing to
a data set. We can get out the training set that we started with by
specifying \texttt{new\_data\ =\ NULL} or apply it to another set via
\texttt{new\_data\ =\ my\_other\_data\_set}. The output of
\texttt{bake()} is a data set like a tibble or a matrix, depending on
the \texttt{composition} argument.
\end{rmdwarning}

But wait, the 4th line should have started with an 11 since the sentence starts with ``monsters''! The entry in \texttt{\_text\_1} is 6 instead. This is happening because the sentence is too long to fit inside the specified sequence length. We must answer three questions before using \texttt{step\_sequence\_onehot()}:\index{preprocessing!challenges}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How long should the output sequence be?
\item
  What happens to sequences that are too long?
\item
  What happens to sequences that are too short?
\end{enumerate}

Choosing the right sequence length is a balancing act. You want the length to be long enough such that you don't truncate too much of your text data, but still short enough to keep the size of the data manageable and to avoid excessive padding. Truncating, having large training data, and excessive padding all lead to worse model performance. This parameter is controlled by the \texttt{sequence\_length} argument in \texttt{step\_sequence\_onehot()}.

If the sequence is too long, then it must be truncated. This can be done by removing values from the beginning (\texttt{"pre"}) or the end (\texttt{"post"}) of the sequence. This choice is mostly influenced by the data, and you need to evaluate where most of the useful information of the text is located. News articles typically start with the main points and then go into detail. If your goal is to detect the broad category, then you may want to keep the beginning of the texts, whereas if you are working with speeches or conversational text, then you might find that the last thing to be said carries more information.

Lastly, we need to decide how to pad a document that is too short. Pre-padding tends to be more popular, especially when working with RNN and LSTM models (Chapter \ref{dllstm}) since having post-padding could result in the hidden states getting flushed out by the zeroes before getting to the text itself (Section \ref{lstmpadding}).

The defaults for \texttt{step\_sequence\_onehot()} are \texttt{sequence\_length\ =\ 100}, \texttt{padding\ =\ "pre"}, and \texttt{truncating\ =\ "pre"}. If we change the truncation to happen at the end with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ small\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(text, }\AttributeTok{sequence\_length =} \DecValTok{6}\NormalTok{, }\AttributeTok{prefix =} \StringTok{""}\NormalTok{,}
                       \AttributeTok{padding =} \StringTok{"pre"}\NormalTok{, }\AttributeTok{truncating =} \StringTok{"post"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      _text_1 _text_2 _text_3 _text_4 _text_5 _text_6
#> [1,]       0       0       0       1       4       5
#> [2,]       0       0       0      14       4       5
#> [3,]       0       0       9       3      13      11
#> [4,]      11       6       7      10      12       2
\end{verbatim}

then we see the 11 at the beginning of the last row representing the ``monsters''. The starting points are not aligned since we are still padding on the left side. We can left-align all the sequences by setting \texttt{padding\ =\ "post"}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ small\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(text, }\AttributeTok{sequence\_length =} \DecValTok{6}\NormalTok{, }\AttributeTok{prefix =} \StringTok{""}\NormalTok{,}
                       \AttributeTok{padding =} \StringTok{"post"}\NormalTok{, }\AttributeTok{truncating =} \StringTok{"post"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      _text_1 _text_2 _text_3 _text_4 _text_5 _text_6
#> [1,]       1       4       5       0       0       0
#> [2,]      14       4       5       0       0       0
#> [3,]       9       3      13      11       0       0
#> [4,]      11       6       7      10      12       2
\end{verbatim}

Now we have that all digits representing the first characters neatly aligned in the first column.

Let's now prepare and apply our feature engineering recipe \texttt{kick\_rec} so we can use it in for our deep learning model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_prep }\OtherTok{\textless{}{-}}  \FunctionTok{prep}\NormalTok{(kick\_rec)}
\NormalTok{kick\_train }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(kick\_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 202093     30
\end{verbatim}

The matrix \texttt{kick\_train} has 202,093 rows, corresponding to the rows of the training data, and 30 columns, corresponding to our chosen sequence length.

\hypertarget{simple-flattened-dense-network}{%
\subsection{Simple flattened dense network}\label{simple-flattened-dense-network}}

Our first deep learning model embeds these Kickstarter blurbs in sequences of vectors, flattens them, and then trains a dense network layer to predict whether the campaign was successful or not.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}

\NormalTok{dense\_model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{,}
                  \AttributeTok{output\_dim =} \DecValTok{12}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{dense\_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Model
#> Model: "sequential"
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> embedding (Embedding)               (None, 30, 12)                  240012      
#> ________________________________________________________________________________
#> flatten (Flatten)                   (None, 360)                     0           
#> ________________________________________________________________________________
#> dense_1 (Dense)                     (None, 32)                      11552       
#> ________________________________________________________________________________
#> dense (Dense)                       (None, 1)                       33          
#> ================================================================================
#> Total params: 251,597
#> Trainable params: 251,597
#> Non-trainable params: 0
#> ________________________________________________________________________________
\end{verbatim}

Let us step through this model specification one layer at a time.

\begin{itemize}
\item
  We initiate the Keras model by using \texttt{keras\_model\_sequential()} to indicate that we want to compose a linear stack of layers.
\item
  Our first \texttt{layer\_embedding()} is equipped to handle the preprocessed data we have in \texttt{kick\_train}. It will take each observation/row in \texttt{kick\_train} and make dense vectors from our word sequences. This turns each observation into an \texttt{embedding\_dim} \(\times\) \texttt{sequence\_length} matrix, 12 \(\times\) 30 matrix in our case. In total, we will create a \texttt{number\_of\_observations} \(\times\) \texttt{embedding\_dim} \(\times\) \texttt{sequence\_length} data cube.
\item
  The next \texttt{layer\_flatten()} layer takes the matrix for each observation and flattens them down into one dimension. This will create a \texttt{30\ *\ 12\ =\ 360} long vector for each observation.
\item
  Lastly, we have 2 densely connected layers. The last layer has a sigmoid activation function to give us an output between 0 and 1, since we want to model a probability for a binary classification problem.
\end{itemize}

We still have a few things left to add to this model before we can fit it to the data. A Keras model requires an \emph{optimizer}\index{optimization algorithm} and a \emph{loss function} to be able to compile.

When the neural network finishes passing a batch of data through the network, it needs a way to use the difference between the predicted values and true values to update the network's weights. The algorithm that determines those weights is known as the optimization algorithm. Many optimizers are available within Keras itself\footnote{\url{https://keras.io/api/optimizers/}}; you can even create custom optimizers if what you need isn't on the list. We will start by using the Adam optimizer, a good default optimizer for many problems.

\begin{rmdnote}
An optimizer can either be set with the name of the optimizer as a
character or by supplying the function \texttt{optimizer\_foo()} where
\texttt{foo} is the name of the optimizer. If you use the function then
you can specify parameters for the optimizer.
\end{rmdnote}

\index{optimization algorithm}

During training a neural network, there must be some quantity that we want to have minimized; this is called the loss function. Again, many loss functions are available within Keras\footnote{\url{https://keras.io/api/losses/}}. These loss functions typically have two arguments, the true value and the predicted value, and return a measure of how close they are.
Since we are working on a binary classification task and the final layer of the network returns a probability, binary cross-entropy\index{binary cross-entropy} is an appropriate loss function. Binary cross-entropy does well at dealing with probabilities because it measures the ``distance'' between probability distributions. In our case, this would be the ground-truth distribution and the predictions.

We can also add any number of metrics\footnote{\url{https://keras.io/api/metrics/}} to be calculated and reported during training. These metrics will not affect the training loop, which is controlled by the optimizer and loss function. The metrics' only job is to report back a single number that will inform you how well the model is performing. We will select accuracy\index{accuracy} as a reported metric for now.

Let's set these 3 options (\texttt{optimizer}, \texttt{loss}, and \texttt{metrics}) using the \texttt{compile()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
Notice how the \texttt{compile()} function modifies the model \emph{in
place}. This is different than how objects are conventionally handled in
R so be vigilant about model definition and modification in your code.
This is a
\href{https://keras.rstudio.com/articles/faq.html\#why-are-keras-objects-modified-in-place-}{conscious
decision} that was made when creating the \textbf{keras} R package to
match the data structures and behavior of the underlying Keras library.
\end{rmdnote}

Finally, we can fit this model! We need to supply the data for training as a matrix of predictors \texttt{x} and a numeric vector of labels \texttt{y}.
This is sufficient information to get started training the model, but we are going to specify a few more arguments to get better control of the training loop. First, we set the number of observations to pass through at a time with \texttt{batch\_size}, and we set \texttt{epochs\ =\ 20} to tell the model to pass all the training data through the training loop 20 times. Lastly, we set \texttt{validation\_split\ =\ 0.25} to specify an internal validation split; this will keep 25\% of the data for validation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_history }\OtherTok{\textless{}{-}}\NormalTok{ dense\_model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ kick\_train,}
    \AttributeTok{y =}\NormalTok{ kickstarter\_train}\SpecialCharTok{$}\NormalTok{state,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{epochs =} \DecValTok{20}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.25}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can visualize the results of the training loop by plotting the \texttt{dense\_history} in Figure \ref{fig:densemodelhistoryplot}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(dense\_history)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/densemodelhistoryplot-1} 

}

\caption{Training and validation metrics for dense network}\label{fig:densemodelhistoryplot}
\end{figure}

\begin{rmdnote}
We have dealt with accuracy in other chapters; remember that a higher
value (a value near one) is better. Loss is new in these deep learning
chapters, and a lower value is better.
\end{rmdnote}

The loss and accuracy both improve with more training epochs on the training data; this dense network more and more closely learns the characteristics of the training data as its trains longer. The same is not true of the validation data, the held-out 25\% specified by \texttt{validation\_split\ =\ 0.25}. The performance is worse on the validation data than the testing data, and \emph{degrades} somewhat as training continues. If we wanted to use this model, we would want to only train it about 7 or 8 epochs.

\hypertarget{evaluate-dnn}{%
\subsection{Evaluation}\label{evaluate-dnn}}

For our first deep learning model, we used the Keras defaults for creating a validation split and tracking metrics, but we can use tidymodels functions to be more specific about these model characteristics. Instead of using the \texttt{validation\_split} argument to \texttt{fit()}, we can create our own validation set using tidymodels and use \texttt{validation\_data} argument for \texttt{fit()}. We create our validation split from the \emph{training} set.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{234}\NormalTok{)}
\NormalTok{kick\_val }\OtherTok{\textless{}{-}} \FunctionTok{validation\_split}\NormalTok{(kickstarter\_train, }\AttributeTok{strata =}\NormalTok{ state)}
\NormalTok{kick\_val}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Validation Set Split (0.75/0.25)  using stratification 
#> # A tibble: 1 x 2
#>   splits                 id        
#>   <list>                 <chr>     
#> 1 <split [151571/50522]> validation
\end{verbatim}

The \texttt{split} object contains the information necessary to extract the data we will use for training/analysis and the data we will use for validation/assessment. We can extract these data sets in their raw, unprocessed form from the split using the helper functions \texttt{analysis()} and \texttt{assessment()}. Then, we can apply our prepped preprocessing recipe \texttt{kick\_prep} to both to transform this data to the appropriate format for our neural network architecture.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                      \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(kick\_analysis)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 151571     30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_assess }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                    \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(kick\_assess)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 50522    30
\end{verbatim}

These are each matrices now appropriate for a deep learning model like the one we trained in the previous section. We will also need the outcome variables for both sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}
\NormalTok{state\_assess }\OtherTok{\textless{}{-}} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}
\end{Highlighting}
\end{Shaded}

Let's set up our same dense neural network architecture.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{,}
                  \AttributeTok{output\_dim =} \DecValTok{12}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{dense\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can fit this model to \texttt{kick\_analysis} and validate on \texttt{kick\_assess}. Let's only fit for 10 epochs this time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_history }\OtherTok{\textless{}{-}}\NormalTok{ dense\_model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ kick\_analysis,}
    \AttributeTok{y =}\NormalTok{ state\_analysis,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{val\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.03504
#>     accuracy: 0.9919
#>     val_loss: 1.069
#> val_accuracy: 0.8051
\end{verbatim}

Figure \ref{fig:valhistoryplot} still shows that significant overfitting at 10 epochs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(val\_history)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/valhistoryplot-1} 

}

\caption{Training and validation metrics for dense network with validation set}\label{fig:valhistoryplot}
\end{figure}

Using our own validation set also allows us to flexibly measure performance using tidymodels functions from the \textbf{yardstick} package. We do need to set up a few transformations between Keras and tidymodels to make this work.
The following function \texttt{keras\_predict()} creates a little bridge between the two frameworks, combining a Keras model with baked (i.e.~preprocessed) data and returning the predictions in a tibble format.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{keras\_predict }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model, baked\_data, response) \{}
\NormalTok{  predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, baked\_data)[, }\DecValTok{1}\NormalTok{]}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{.pred\_1 =}\NormalTok{ predictions,}
    \AttributeTok{.pred\_class =} \FunctionTok{if\_else}\NormalTok{(.pred\_1 }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{state =}\NormalTok{ response}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(state, .pred\_class),            }\DocumentationTok{\#\# create factors}
                  \SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(.x, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))))  }\DocumentationTok{\#\# with matching levels}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
This function only works with binary classification models that take a
preprocessed matrix as input and return a single probability for each
observation. It returns both the predicted probability as well as the
predicted class, using a 50\% probability threshold.
\end{rmdwarning}

This function creates prediction results that seamlessly connect with tidymodels and \textbf{yardstick} functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(dense\_model, kick\_assess, state\_assess)}
\NormalTok{val\_res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 50,522 x 3
#>      .pred_1 .pred_class state
#>        <dbl> <fct>       <fct>
#>  1 0.00101   0           0    
#>  2 0.000167  0           0    
#>  3 0.0139    0           0    
#>  4 0.00725   0           0    
#>  5 0.0237    0           0    
#>  6 1.00      1           0    
#>  7 0.000536  0           0    
#>  8 0.000308  0           0    
#>  9 0.000419  0           0    
#> 10 0.0000417 0           0    
#> # ... with 50,512 more rows
\end{verbatim}

We can calculate the standard metrics with \texttt{metrics()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{metrics}\NormalTok{(val\_res, state, .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.805
#> 2 kap      binary         0.609
\end{verbatim}

This matches what we saw when we looked at the output of \texttt{val\_history}.

Since we have access to tidymodels' full capacity for model evaluation, we can also compute confusion matrices\index{matrix!confusion} and ROC curves.
The heatmap in Figure \ref{fig:dnnheatmap} shows that there isn't any dramatic bias in how the model performs for the two classes, success and failure for the crowdfunding campaigns. The model certainly isn't perfect; its accuracy is a little over 80\%, but at least it is more or less evenly good at predicting both classes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(state, .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/dnnheatmap-1} 

}

\caption{Confusion matrix for first DNN model predictions of Kickstarter campaign success}\label{fig:dnnheatmap}
\end{figure}

The ROC curve in Figure \ref{fig:dnnroccurve} shows how the model performs at different thresholds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ state, .pred\_1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Receiver operator curve for Kickstarter blurbs"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/dnnroccurve-1} 

}

\caption{ROC curve for first DNN model predictions of Kickstarter campaign success}\label{fig:dnnroccurve}
\end{figure}

\hypertarget{using-bag-of-words-features}{%
\section{Using bag-of-words features}\label{using-bag-of-words-features}}

Before we move on with neural networks and this new way to represent the text sequences, let's explore what happens if we use the \emph{same} \index{preprocessing}preprocessing as in Chapters \ref{mlregression} and \ref{mlclassification}. We will employ a bag-of-words preprocessing and input word counts only to the neural network. This model will not use any location-based information about the tokens, just the counts.

For this, we need to create a new recipe to transform the data into counts.

\begin{rmdnote}
The objects in this chapter are named using \texttt{bow} to indicate
that they are using \textbf{b}ag \textbf{o}f \textbf{w}ord data.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_bow\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_stopwords}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =} \FloatTok{1e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tf}\NormalTok{(blurb)}
\end{Highlighting}
\end{Shaded}

We will \texttt{prep()} and \texttt{bake()} this recipe to get out our processed data. The result will be quite sparse, since the blurbs are short and we are counting only the most frequent 1000 tokens after removing the Snowball stop word list.\index{stop word lists!Snowball}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_bow\_prep }\OtherTok{\textless{}{-}}  \FunctionTok{prep}\NormalTok{(kick\_bow\_rec)}

\NormalTok{kick\_bow\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_bow\_prep, }
                          \AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                          \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}

\NormalTok{kick\_bow\_assess }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_bow\_prep, }
                        \AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                        \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now that we have the analysis and assessment data sets calculated, we can define the \index{network architecture}neural network architecture. We won't be using an embedding layer this time; we will input the word count data directly into the first dense layer. This dense layer is followed by another hidden layer and then a final layer with a sigmoid activation to leave us with a value between 0 and 1 which we treat as the probability.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bow\_model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{, }\AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\FloatTok{1e3}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{bow\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In many ways, this model architecture is not that different than the model we used in section \ref{firstdlclassification}. The main difference here is the \index{preprocessing}\emph{preprocessing}; the shape and information of the data from \texttt{kick\_bow\_prep} are different than what we saw before since the matrix elements represent counts (something that Keras can handle directly) and not indicators for words in the vocabulary. Keras handles the indicators with \texttt{layer\_embedding()}, by mapping them through an embedding layer.

The fitting procedure remains unchanged.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bow\_history }\OtherTok{\textless{}{-}}\NormalTok{ bow\_model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ kick\_bow\_analysis,}
    \AttributeTok{y =}\NormalTok{ state\_analysis,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_bow\_assess, state\_assess),}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{bow\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.3342
#>     accuracy: 0.8543
#>     val_loss: 0.6743
#> val_accuracy: 0.7206
\end{verbatim}

We use \texttt{keras\_predict()} again to get predictions, and calculate the standard metrics with \texttt{metrics()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bow\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(bow\_model, kick\_bow\_assess, state\_assess)}

\FunctionTok{metrics}\NormalTok{(bow\_res, state, .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.721
#> 2 kap      binary         0.441
\end{verbatim}

This model does not perform as well as the model we used in section \ref{firstdlclassification}. This suggests that a model incorporating more than word counts alone is useful here. This model did outperform a baseline linear model (shown in Appendix \ref{appendixbaseline}), which achieved an accuracy of 0.684; that linear baseline is a regularized linear model trained on the same data set, using tf-idf weights and 5000 tokens.

This simpler model does not outperform our initial model in this chapter, but it is typically worthwhile to investigate if a simpler model can rival or beat a model we are working with.

\hypertarget{using-pre-trained-word-embeddings}{%
\section{Using pre-trained word embeddings}\label{using-pre-trained-word-embeddings}}

\index{embeddings!pre-trained}The models in Section \ref{firstdlclassification} included an embedding layer to make dense vectors from our word sequences that the model learned, along with the rest of the model as a whole. This is not the only way to handle this task. In Chapter \ref{embeddings}, we examined how word embeddings are created and how they are used. Instead of having the embedding layer start randomly and be trained alongside the other parameters, let's try to \emph{provide} the embeddings.

\begin{rmdwarning}
This section serves to show how to use pre-trained word embeddings, but
in most realistic situations, your data and pre-trained embeddings may
not match well. The main takeaways from this section should be that this
approach is possible and how you can get started with it. Keep in mind
that it may not be appropriate for your data and problem.
\end{rmdwarning}

We start by obtaining pre-trained embeddings. The GloVe embeddings\index{embeddings!GloVe} that we used in Section \ref{glove} are a good place to start. Setting \texttt{dimensions\ =\ 50} and only selecting the first 12 dimensions will make it easier for us to compare to our previous models directly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textdata)}

\NormalTok{glove6b }\OtherTok{\textless{}{-}} \FunctionTok{embedding\_glove6b}\NormalTok{(}\AttributeTok{dimensions =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{13}\NormalTok{)}
\NormalTok{glove6b}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 400,000 x 13
#>    token     d1      d2     d3      d4     d5      d6     d7      d8        d9
#>    <chr>  <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>     <dbl>
#>  1 "the" 0.418   0.250  -0.412  0.122  0.345  -0.0445 -0.497 -0.179  -0.000660
#>  2 ","   0.0134  0.237  -0.169  0.410  0.638   0.477  -0.429 -0.556  -0.364   
#>  3 "."   0.152   0.302  -0.168  0.177  0.317   0.340  -0.435 -0.311  -0.450   
#>  4 "of"  0.709   0.571  -0.472  0.180  0.544   0.726   0.182 -0.524   0.104   
#>  5 "to"  0.680  -0.0393  0.302 -0.178  0.430   0.0322 -0.414  0.132  -0.298   
#>  6 "and" 0.268   0.143  -0.279  0.0163 0.114   0.699  -0.513 -0.474  -0.331   
#>  7 "in"  0.330   0.250  -0.609  0.109  0.0364  0.151  -0.551 -0.0742 -0.0923  
#>  8 "a"   0.217   0.465  -0.468  0.101  1.01    0.748  -0.531 -0.263   0.168   
#>  9 "\""  0.258   0.456  -0.770 -0.377  0.593  -0.0635  0.205 -0.574  -0.290   
#> 10 "'s"  0.237   0.405  -0.205  0.588  0.655   0.329  -0.820 -0.232   0.274   
#> # ... with 399,990 more rows, and 3 more variables: d10 <dbl>, d11 <dbl>,
#> #   d12 <dbl>
\end{verbatim}

The \texttt{embedding\_glove6b()} function returns a tibble; this isn't the right format for Keras. Also, notice how many rows are present in this embedding, far more than what the trained recipe is expecting. The vocabulary can be extracted from the trained recipe using \texttt{tidy()}. Let's apply \texttt{tidy()} to \texttt{kick\_prep} to get the list of steps that the recipe contains.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tidy}\NormalTok{(kick\_prep)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 3 x 6
#>   number operation type            trained skip  id                   
#>    <int> <chr>     <chr>           <lgl>   <lgl> <chr>                
#> 1      1 step      tokenize        TRUE    FALSE tokenize_eDrDa       
#> 2      2 step      tokenfilter     TRUE    FALSE tokenfilter_zDVeF    
#> 3      3 step      sequence_onehot TRUE    FALSE sequence_onehot_TaBPG
\end{verbatim}

We see that the third step is the \texttt{sequence\_onehot} step, so by setting \texttt{number\ =\ 3} we can extract the embedding vocabulary.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tidy}\NormalTok{(kick\_prep, }\AttributeTok{number =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 20,000 x 4
#>    terms vocabulary token id                   
#>    <chr>      <int> <chr> <chr>                
#>  1 blurb          1 0     sequence_onehot_TaBPG
#>  2 blurb          2 00    sequence_onehot_TaBPG
#>  3 blurb          3 000   sequence_onehot_TaBPG
#>  4 blurb          4 00pm  sequence_onehot_TaBPG
#>  5 blurb          5 01    sequence_onehot_TaBPG
#>  6 blurb          6 02    sequence_onehot_TaBPG
#>  7 blurb          7 03    sequence_onehot_TaBPG
#>  8 blurb          8 05    sequence_onehot_TaBPG
#>  9 blurb          9 06    sequence_onehot_TaBPG
#> 10 blurb         10 07    sequence_onehot_TaBPG
#> # ... with 19,990 more rows
\end{verbatim}

We can then use \texttt{left\_join()} to combine these tokens to the \texttt{glove6b} embedding tibble and only keep the tokens of interest. We replace any tokens from the vocabulary not found in \texttt{glove6b} with 0 using \texttt{mutate\_all()} and \texttt{replace\_na()}. We can transform the results into a matrix, and add a row of zeroes at the top of the matrix to account for the out-of-vocabulary words.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glove6b\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(kick\_prep, }\DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(token) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(glove6b, }\AttributeTok{by =} \StringTok{"token"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_all}\NormalTok{(replace\_na, }\DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{token) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rbind}\NormalTok{(}\DecValTok{0}\NormalTok{, .)}
\end{Highlighting}
\end{Shaded}

We'll keep the model architecture itself as unchanged as possible. The \texttt{output\_dim} argument is set equal to\texttt{ncol(glove6b\_matrix)} to make sure that all the dimensions line up correctly, but everything else stays the same.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model\_pte }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{,}
                  \AttributeTok{output\_dim =} \FunctionTok{ncol}\NormalTok{(glove6b\_matrix),}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we use \texttt{get\_layer()} to access the first layer (which is the embedding layer), set the weights with \texttt{set\_weights()}, and then freeze the weights with \texttt{freeze\_weights()}.

\begin{rmdnote}
Freezing the weights stops them from being updated during the training
loop.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model\_pte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{get\_layer}\NormalTok{(}\AttributeTok{index =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_weights}\NormalTok{(}\FunctionTok{list}\NormalTok{(glove6b\_matrix)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{freeze\_weights}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we compile and fit the model just like the last one we looked at.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model\_pte }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\NormalTok{dense\_pte\_history }\OtherTok{\textless{}{-}}\NormalTok{ dense\_model\_pte }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ kick\_analysis,}
    \AttributeTok{y =}\NormalTok{ state\_analysis,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{epochs =} \DecValTok{20}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{dense\_pte\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.5996
#>     accuracy: 0.6739
#>     val_loss: 0.672
#> val_accuracy: 0.6086
\end{verbatim}

This model is not performing well at all! We can confirm by computing metrics on our validation set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pte\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(dense\_model\_pte, kick\_assess, state\_assess)}
\FunctionTok{metrics}\NormalTok{(pte\_res, state, .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.609
#> 2 kap      binary         0.214
\end{verbatim}

\index{embeddings!pre-trained}Why did this happen? Part of the training loop for a model like this one typically \emph{adjusts} the weights in the network.
When we froze the weights in this network, we froze them at values that did not perform very well.
These pre-trained \index{embeddings!GloVe}GloVe embeddings \citep{Pennington2014} are trained on a Wikipedia dump and \href{https://catalog.ldc.upenn.edu/LDC2011T07}{Gigaword 5}, a comprehensive archive of newswire text.
The text contained on Wikipedia and in news articles both follow certain styles and semantics.\index{semantics}
Both will tend to be written formally and in the past tense, with longer and complete sentences.
There are many more distinct features of both Wikipedia text and news articles, but the relevant aspect here is how similar they are to the data we are trying to model.
These Kickstarter blurbs are very short, lack punctuation, stop words, narrative, and tense. Many of the blurbs simply try to pack as many buzz words as possible into the allowed character count while keeping the sentence readable.
Perhaps it should not surprise us that these word embeddings don't perform well in this model, since the text used to train the embeddings is so different from the text is it being applied to (Section \ref{glove}).

\begin{rmdwarning}
Although this approach didn't work well with our data set didn't, that
doesn't mean that using pre-trained word embeddings is always a bad
idea.
\end{rmdwarning}

The key point is how well the \index{embeddings!pre-trained}embeddings match the data you are modeling.
Also, there is another way we can use these particular embeddings in our network architecture; we can load them in as a starting point as before but \emph{not} freeze the weights.
This allows the model to adjust the weights to better fit the data. The intention here is that we as the modeling practitioners think these pre-trained embeddings offer a better starting point than the randomly generated embedding we get if we don't set the weights at all.

We specify a new model to get started on this approach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model\_pte2 }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{,}
                  \AttributeTok{output\_dim =} \FunctionTok{ncol}\NormalTok{(glove6b\_matrix),}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, we set the weights with \texttt{set\_weights()} but we \emph{don't} freeze them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model\_pte2 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{get\_layer}\NormalTok{(}\AttributeTok{index =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_weights}\NormalTok{(}\FunctionTok{list}\NormalTok{(glove6b\_matrix))}
\end{Highlighting}
\end{Shaded}

We compile and fit the model as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dense\_model\_pte2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\NormalTok{dense\_pte2\_history }\OtherTok{\textless{}{-}}\NormalTok{ dense\_model\_pte2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ kick\_analysis,}
  \AttributeTok{y =}\NormalTok{ state\_analysis,}
  \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
  \AttributeTok{epochs =} \DecValTok{20}\NormalTok{,}
  \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
  \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

How did this version of using pre-trained embeddings do?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pte2\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(dense\_model\_pte2, kick\_assess, state\_assess)}
\FunctionTok{metrics}\NormalTok{(pte2\_res, state, .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.765
#> 2 kap      binary         0.528
\end{verbatim}

This performs quite a bit better than when we froze the weights, although not as well as when we did not use pre-trained embeddings at all.

\begin{rmdnote}
If you have enough text data in the field you are working in, then it is worth considering training a word embedding yourself that better captures the structure of the domain you are trying to work with, both for the reasons laid out here and for the issues highlighted in Section \ref{fairnessembeddings}.
\end{rmdnote}

\hypertarget{dnncross}{%
\section{Cross-validation for deep learning models}\label{dnncross}}

The Kickstarter data set we are using is big enough that we have adequate data to use a single training set, validation set, and testing set that all contain enough observations in them to give reliable performance metrics. In some situations, you may not have that much data or you may want to compute more precise performance metrics. In those cases, it is time to turn to resampling. For example, we can create cross-validation folds.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{345}\NormalTok{)}
\NormalTok{kick\_folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(kickstarter\_train, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\NormalTok{kick\_folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> #  5-fold cross-validation 
#> # A tibble: 5 x 2
#>   splits                 id   
#>   <list>                 <chr>
#> 1 <split [161674/40419]> Fold1
#> 2 <split [161674/40419]> Fold2
#> 3 <split [161674/40419]> Fold3
#> 4 <split [161675/40418]> Fold4
#> 5 <split [161675/40418]> Fold5
\end{verbatim}

Each of these folds has an analysis/training set and an assessment/validation set. Instead of training our model one time and getting one measure of performance, we can train our model \texttt{v} times and get \texttt{v} measures, for more reliability.

In our previous chapters, we used models with full tidymodels support and functions like \texttt{add\_recipe()} and \texttt{workflow()}. Deep learning models are more modular and unique, so we will need to create our own function to handle preprocessing, fitting, and evaluation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_split }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(split, prepped\_rec) \{}
  \DocumentationTok{\#\# preprocessing}
\NormalTok{  x\_train }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(prepped\_rec, }\AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(split),}
                  \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{  x\_val   }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(prepped\_rec, }\AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(split),}
                  \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}

  \DocumentationTok{\#\# create model}
\NormalTok{  y\_train }\OtherTok{\textless{}{-}} \FunctionTok{analysis}\NormalTok{(split) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}
\NormalTok{  y\_val   }\OtherTok{\textless{}{-}} \FunctionTok{assessment}\NormalTok{(split) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}

\NormalTok{  mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{,}
                    \AttributeTok{output\_dim =} \DecValTok{12}\NormalTok{,}
                    \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
      \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
      \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
      \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{    )}

  \DocumentationTok{\#\# fit model}
\NormalTok{  mod }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{fit}\NormalTok{(}
\NormalTok{      x\_train,}
\NormalTok{      y\_train,}
      \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
      \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(x\_val, y\_val),}
      \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
      \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{    )}

  \DocumentationTok{\#\# evaluate model}
  \FunctionTok{keras\_predict}\NormalTok{(mod, x\_val, y\_val) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can \texttt{map()} this function across all our cross-validation folds. This takes longer than our previous models to train, since we are training for 10 epochs each on five folds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_fitted }\OtherTok{\textless{}{-}}\NormalTok{ kick\_folds }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{validation =} \FunctionTok{map}\NormalTok{(splits, fit\_split, kick\_prep))}

\NormalTok{cv\_fitted}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> #  5-fold cross-validation 
#> # A tibble: 5 x 3
#>   splits                 id    validation          
#>   <list>                 <chr> <list>              
#> 1 <split [161674/40419]> Fold1 <tibble[,3] [4 x 3]>
#> 2 <split [161674/40419]> Fold2 <tibble[,3] [4 x 3]>
#> 3 <split [161674/40419]> Fold3 <tibble[,3] [4 x 3]>
#> 4 <split [161675/40418]> Fold4 <tibble[,3] [4 x 3]>
#> 5 <split [161675/40418]> Fold5 <tibble[,3] [4 x 3]>
\end{verbatim}

Now we can use \texttt{unnest()} to find the metrics we computed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_fitted }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(validation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 20 x 5
#>    splits                 id    .metric     .estimator .estimate
#>    <list>                 <chr> <chr>       <chr>          <dbl>
#>  1 <split [161674/40419]> Fold1 accuracy    binary         0.817
#>  2 <split [161674/40419]> Fold1 kap         binary         0.633
#>  3 <split [161674/40419]> Fold1 mn_log_loss binary         1.10 
#>  4 <split [161674/40419]> Fold1 roc_auc     binary         0.856
#>  5 <split [161674/40419]> Fold2 accuracy    binary         0.819
#>  6 <split [161674/40419]> Fold2 kap         binary         0.638
#>  7 <split [161674/40419]> Fold2 mn_log_loss binary         1.01 
#>  8 <split [161674/40419]> Fold2 roc_auc     binary         0.859
#>  9 <split [161674/40419]> Fold3 accuracy    binary         0.818
#> 10 <split [161674/40419]> Fold3 kap         binary         0.635
#> 11 <split [161674/40419]> Fold3 mn_log_loss binary         1.01 
#> 12 <split [161674/40419]> Fold3 roc_auc     binary         0.856
#> 13 <split [161675/40418]> Fold4 accuracy    binary         0.817
#> 14 <split [161675/40418]> Fold4 kap         binary         0.633
#> 15 <split [161675/40418]> Fold4 mn_log_loss binary         1.03 
#> 16 <split [161675/40418]> Fold4 roc_auc     binary         0.856
#> 17 <split [161675/40418]> Fold5 accuracy    binary         0.817
#> 18 <split [161675/40418]> Fold5 kap         binary         0.633
#> 19 <split [161675/40418]> Fold5 mn_log_loss binary         1.04 
#> 20 <split [161675/40418]> Fold5 roc_auc     binary         0.855
\end{verbatim}

We can summarize the unnested results to match what we normally would get from \texttt{collect\_metrics()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_fitted }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(validation) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(.metric) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(.estimate),}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{std\_err =} \FunctionTok{sd}\NormalTok{(.estimate) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 4
#>   .metric      mean     n  std_err
#>   <chr>       <dbl> <int>    <dbl>
#> 1 accuracy    0.818     5 0.000473
#> 2 kap         0.634     5 0.000988
#> 3 mn_log_loss 1.04      5 0.0160  
#> 4 roc_auc     0.856     5 0.000661
\end{verbatim}

This data set is large enough that we probably wouldn't need to take this approach, and the fold-to-fold metrics have little variance. However resampling can, at times, be an important piece of the modeling toolkit even for deep learning models.

\index{models!challenges}\index{computational speed}

\begin{rmdnote}
Training deep learning models typically takes more time than other kinds
of machine learning, so resampling may be an unfeasible choice. There is
special hardware available that speeds up deep learning because it is
particularly well-suited to fitting such models. GPUs (graphics
processing units) are used for displaying graphics (as indicated in
their name) and gaming, but also for deep learning because of their
highly parallel computational ability. GPUs can make solving deep
learning problems faster, or even tractable to start with. Be aware,
though, that you might not need a GPU for even real-world deep learning
modeling. All the models in this book were trained on a CPU only.
\end{rmdnote}

\hypertarget{compare-and-evaluate-dnn-models}{%
\section{Compare and evaluate DNN models}\label{compare-and-evaluate-dnn-models}}

Let's return to the results we evaluated on a single validation set. We can combine all the predictions on these last three models to more easily compare the results between them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_dense\_model\_res }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
\NormalTok{  val\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =} \StringTok{"dense"}\NormalTok{),}
\NormalTok{  pte\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =} \StringTok{"pte (locked weights)"}\NormalTok{),}
\NormalTok{  pte2\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =} \StringTok{"pte (not locked weights)"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now that the results are combined in \texttt{all\_dense\_model\_res}, we can calculate group-wise evaluation statistics by grouping by the \texttt{model} variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_dense\_model\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(model) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{metrics}\NormalTok{(state, .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 6 x 4
#>   model                    .metric  .estimator .estimate
#>   <chr>                    <chr>    <chr>          <dbl>
#> 1 dense                    accuracy binary         0.805
#> 2 pte (locked weights)     accuracy binary         0.609
#> 3 pte (not locked weights) accuracy binary         0.765
#> 4 dense                    kap      binary         0.609
#> 5 pte (locked weights)     kap      binary         0.214
#> 6 pte (not locked weights) kap      binary         0.528
\end{verbatim}

We can also do this for ROC curves. Figure \ref{fig:alldnnroccurve} shows the three different ROC curves together in one chart. As we know, the model using pre-trained word embeddings with locked weights didn't perform very well at all and its ROC curve is the lowest of the three. The other two models perform more similarly but the model using an embedding learned from scratch ends up being the best.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_dense\_model\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(model) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ state, .pred\_1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Receiver operator curve for Kickstarter blurbs"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{08_dl_dnn_files/figure-latex/alldnnroccurve-1} 

}

\caption{ROC curve for all DNN models' predictions of Kickstarter campaign success}\label{fig:alldnnroccurve}
\end{figure}

\index{embeddings!pre-trained}\index{models!pre-trained}
\begin{rmdnote}
Using pre-trained embeddings is not the only way to take advantage of ready-to-use, state-of-the-art deep learning models. You can also use whole pre-trained models in your analyses, such as the \texttt{transformers} models available from Hugging Face. Check out \href{https://blogs.rstudio.com/ai/posts/2020-07-30-state-of-the-art-nlp-models-from-r/}{this blog post for a tutorial} on how to use Hugging Face \texttt{transfomers} in R with Keras. Large language models like these are subject to many of the same concerns as embeddings discussed in Section \ref{fairnessembeddings}.
\end{rmdnote}

We compared these three model options using the validation set we created. Let's return to the testing set now that we know which model we expect to perform best and obtain a final estimate for how we expect it to perform on new data. For this final evaluation, we will:

\begin{itemize}
\item
  preprocess the test data using the feature engineering recipe \texttt{kick\_prep} so it is in the correct format for our deep learning model,
\item
  find the predictions for the processed testing data, and
\item
  compute metrics for these results.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_test }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =}\NormalTok{ kickstarter\_test,}
                  \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{final\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(dense\_model, kick\_test, kickstarter\_test}\SpecialCharTok{$}\NormalTok{state)}
\NormalTok{final\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.804
#> 2 kap         binary         0.608
#> 3 mn_log_loss binary         1.08 
#> 4 roc_auc     binary         0.845
\end{verbatim}

The metrics we see here are about the same as what we achieved in Section \ref{evaluate-dnn} on the validation data, so we can be confident that we have not overfit during our training or model choosing process.

Just like we did toward the end of both Sections \ref{regression-final-evaluation} and \ref{classification-final-evaluation}, we can look at some examples of test set observations that our model did a bad job at predicting. Let's bind together the predictions on the test set with the original \texttt{kickstarter\_test} data. Then let's look at blurbs that were successful but that our final model thought had a low probability of being successful.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter\_bind }\OtherTok{\textless{}{-}}\NormalTok{ final\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(kickstarter\_test }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{state))}

\NormalTok{kickstarter\_bind }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(state }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, .pred\_1 }\SpecialCharTok{\textless{}} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 1
#>    blurb                                                                        
#>    <chr>                                                                        
#>  1 The popular YouTube channel Blimey Cow wants to start an audio network and n~
#>  2 Seventh Night is a Romantic, Comic, Action, Adventure Fantasy novel that sho~
#>  3 3 guest conductors, 32 singers, and $3,600 to hire professional instrumental~
#>  4 Electronic music goes beyond electronic music.                               
#>  5 The clip for your duvet to help you put on the cover easily, keep it in plac~
#>  6 Lepe Cellars is an artisan winery, operated by Miguel Lepe. The dream is to ~
#>  7 It turns out not everyone loves Vinyl haha , so by request we are going to d~
#>  8 Have friends in your area deliver food or anything else you need to you in o~
#>  9 T-shirts and clothing made to show off your favorite car designs!            
#> 10 A mother's worth is calculated by the deposits of love exchanged between her~
\end{verbatim}

What about misclassifications in the other direction, observations in the test set that were \emph{not} successful but that our final model gave a high probability of being successful?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter\_bind }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(state }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, .pred\_1 }\SpecialCharTok{\textgreater{}} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 1
#>    blurb                                                                        
#>    <chr>                                                                        
#>  1 "Cobar Community Radio is licensed to go on-air but needs additional funds t~
#>  2 "I design and produce wooden signs and wall vinyl's customized to the client~
#>  3 "A volume of short children's stories based on a character inspired by my gr~
#>  4 "Growing Pains is a short film following the story of an unemployed imaginar~
#>  5 "This is a retro style Role Playing game designed for mobile devices includi~
#>  6 "Assemble yourself seamless and without competence of connected objects such~
#>  7 "Bruce has entered our VOTA House Party tour contest and invited you to join~
#>  8 "Permettre au sport d'évoluer en facilitant la prise de données lors des mat~
#>  9 "Beyond Eden's new record funded by you and the band..... not a record compa~
#> 10 "It's a coming of age story , about a sixteen year who crosses the U.S borde~
\end{verbatim}

Notice that although some steps for model fitting are different now that we are using deep learning, model evaluation is much the same as it was in Chapters \ref{mlregression} and \ref{mlclassification}.

\hypertarget{dllimitations}{%
\section{Limitations of deep learning}\label{dllimitations}}

\index{models!challenges}Deep learning models achieve excellent performance on many tasks; the flexibility and potential complexity of their architecture is part of the reason why. One of the main downsides of deep learning models is that the interpretability of the models themselves is poor.

\begin{rmdwarning}
Notice that we have not talked about which words are more associated
with success or failure for the Kickstarter campaigns in this whole
chapter!
\end{rmdwarning}

This means that practitioners who work in fields where interpretability is vital, such as some parts of health care, shy away from deep learning models since they are hard to understand and interpret.

Another limitation of deep learning models is that they do not facilitate a comprehensive theoretical understanding or learning of their inner organization \citep{shwartzziv2017opening}.
These two points together lead to deep learning models often being called ``black box''\index{"black box"} models \citep{shrikumar2019learning}, models where is it hard to peek into the inner workings to understand what they are doing.
Not being able to reason about the inner workings of a model means that we will have a hard time explaining why a model is working well. It also means it will be hard to remedy a biased model that performs well in some settings but badly in other settings.
This is a problem since it can hide biases\index{bias} from the training set which may lead to unfair, wrong, or even illegal decisions based on protected classes \citep{guidotti2018survey}.

Practitioners have built approaches to understand local feature importance for deep learning models which we demonstrate in Section \ref{lime}, but these are limited tools compared to the interpretability of other kinds of models.
Lastly, deep learning models tend to require more training data than traditional statistical machine learning methods. This means that that it can be hard to train a deep learning model if you have a very small data set \citep{lampinen2018oneshot}.

\hypertarget{dldnnsummary}{%
\section{Summary}\label{dldnnsummary}}

You can use deep learning to build classification models to predict labels or categorical variables from a data set, including data sets that include text.
Dense neural networks are the most straightforward network architecture that can be used to fit classification models for text features and are a good bridge for understanding the more complex model architectures that are used more often in practice for text modeling.
These models have many parameters compared to the models we trained in earlier chapters, and require different \index{preprocessing}preprocessing than those models.
We can tokenize and create features for modeling that capture the order of the tokens in the original text. Doing this can allow a model to learn from patterns in sequences and order, something not possible in the models we saw in \ref{mlregression} and Chapters \ref{mlclassification}.
We gave up some of the fine control over feature engineering\index{feature engineering}, such as hand-crafting features using domain knowledge, in the hope that the network could learn important features on its own.
However, feature engineering is not completely out of our hands as practitioners, since we still make decisions about tokenization and normalization before the tokens are passed into the network.

\hypertarget{in-this-chapter-you-learned-7}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-7}}

\begin{itemize}
\item
  that you can tokenize and preprocess text to retain the order of the tokens
\item
  how to build and train a dense neural network with Keras
\item
  that you can evaluate deep learning models with the same approaches used for other types of models
\item
  how to train word embeddings alongside your model
\item
  how to use pre-trained word embeddings in a neural network
\item
  about resampling strategies for deep learning models
\item
  about the low interpretability of deep learning models
\end{itemize}

\hypertarget{dllstm}{%
\chapter{Long short-term memory (LSTM) networks}\label{dllstm}}

In Chapter \ref{dldnn}, we trained our first deep learning models, with straightforward dense network architectures\index{network architecture} that provide a bridge for our understanding as we move from shallow learning algorithms to more complex network architectures. Those first neural network architectures are not simple compared to the kinds of models we used in Chapters \ref{mlregression} and \ref{mlclassification}, but it is possible to build many more different and more complex kinds of networks for prediction with text data. This chapter will focus on the family of \textbf{long short-term memory} networks\index{neural network!long short-term memory} (LSTMs) \citep{Hochreiter1997}.

\hypertarget{firstlstm}{%
\section{A first LSTM model}\label{firstlstm}}

We will be using the same data from the previous chapter, described in Sections \ref{kickstarter} and \ref{kickstarter-blurbs}. This data contains short text blurbs for prospective crowdfunding campaigns and whether those campaigns were successful. Our modeling goal is to predict whether a Kickstarter crowdfunding campaign was successful or not, based on the text blurb describing the campaign. Let's start by splitting our data into training and testing sets.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{kickstarter }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/kickstarter.csv.gz"}\NormalTok{)}
\NormalTok{kickstarter}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 269,790 x 3
#>    blurb                                                        state created_at
#>    <chr>                                                        <dbl> <date>    
#>  1 Exploring paint and its place in a digital world.                0 2015-03-17
#>  2 Mike Fassio wants a side-by-side photo of me and Hazel eati~     0 2014-07-11
#>  3 I need your help to get a nice graphics tablet and Photosho~     0 2014-07-30
#>  4 I want to create a Nature Photograph Series of photos of wi~     0 2015-05-08
#>  5 I want to bring colour to the world in my own artistic skil~     0 2015-02-01
#>  6 We start from some lovely pictures made by us and we decide~     0 2015-11-18
#>  7 Help me raise money to get a drawing tablet                      0 2015-04-03
#>  8 I would like to share my art with the world and to do that ~     0 2014-10-15
#>  9 Post Card don’t set out to simply decorate stories. Our goa~     0 2015-06-25
#> 10 My name is Siu Lon Liu and I am an illustrator seeking fund~     0 2014-07-19
#> # ... with 269,780 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{kickstarter\_split }\OtherTok{\textless{}{-}}\NormalTok{ kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb) }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{state =} \FunctionTok{as.integer}\NormalTok{(state)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{initial\_split}\NormalTok{()}

\NormalTok{kickstarter\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(kickstarter\_split)}
\NormalTok{kickstarter\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(kickstarter\_split)}
\end{Highlighting}
\end{Shaded}

Just as described in Chapter \ref{dldnn}, the \index{preprocessing}preprocessing needed for deep learning network architectures is somewhat different than for the models we used in Chapters \ref{mlregression} and \ref{mlclassification}. The first step is still to tokenize the text, as described in Chapter \ref{tokenization}. After we tokenize, we filter to keep only how many words we'll include in the analysis; \texttt{step\_tokenfilter()} keeps the top tokens based on frequency in this data set.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textrecipes)}

\NormalTok{max\_words }\OtherTok{\textless{}{-}} \FloatTok{2e4}
\NormalTok{max\_length }\OtherTok{\textless{}{-}} \DecValTok{30}

\NormalTok{kick\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =}\NormalTok{ max\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(blurb, }\AttributeTok{sequence\_length =}\NormalTok{ max\_length)}
\end{Highlighting}
\end{Shaded}

After tokenizing, the preprocessing is different. We use \texttt{step\_sequence\_onehot()} to encode the sequences of words as integers representing each token in the vocabulary of 20,000 words, as described in detail in Section \ref{onehotsequence}. This is different than the representations we used in Chapters \ref{mlregression} and \ref{mlclassification}, mainly because information about word sequence is encoded in this representation.

\begin{rmdnote}
Using \texttt{step\_sequence\_onehot()} to preprocess text data records and encodes \emph{sequence} information, unlike the document-term matrix and/or bag-of-tokens approaches we used in Chapters \ref{mlclassification} and \ref{mlregression}.
\end{rmdnote}

There are 202,093 blurbs in the training set and 67,364 in the testing set.

\begin{rmdpackage}
Like we discussed in the last chapter, we are using \textbf{recipes} and
\textbf{textrecipes} for preprocessing before modeling. When we
\texttt{prep()} a recipe, we compute or estimate statistics from the
training set; the output of \texttt{prep()} is a recipe. When we
\texttt{bake()} a recipe, we apply the preprocessing to a data set,
either the training set that we started with or another set like the
testing data or new data. The output of \texttt{bake()} is a data set
like a tibble or a matrix.
\end{rmdpackage}

We could have applied these \texttt{prep()} and \texttt{bake()} functions to any preprocessing recipes throughout this book, but we typically didn't need to because our modeling workflows automated these steps.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(kick\_rec)}
\NormalTok{kick\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}

\FunctionTok{dim}\NormalTok{(kick\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 202093     30
\end{verbatim}

Here we use \texttt{composition\ =\ "matrix"} because the Keras modeling functions operate on matrices, rather than a dataframe or tibble.

\hypertarget{building-an-lstm}{%
\subsection{Building an LSTM}\label{building-an-lstm}}

An LSTM\index{neural network!long short-term memory} is a specific kind of network architecture with feedback loops that allow information to persist through steps\footnote{Vanilla neural networks do not have this ability for information to persist at all; they start learning from scratch at every step.} and memory cells that can learn to ``remember'' and ``forget'' information through sequences. LSTMs are well-suited for text because of this ability to process text as a long sequence of words or characters, and can model structures within text like word dependencies. LSTMs are useful in text modeling because of this memory through long sequences; they are also used for time series, machine \index{translation}translation, and similar problems.

Figure \ref{fig:rnndiag} depicts a high-level diagram of how the LSTM unit of a network works. In the diagram, part of the neural network, \(A\), operates on some of the input and outputs a value. During this process, some information is held inside \(A\) to make the network ``remember'' this updated network. Network \(A\) is then applied to the next input where it predicts new output and its memory is updated.

\index{neural network!recurrent}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{diagram-files/rnn-architecture} 

}

\caption{High-level diagram of an unrolled recurrent neural network. The recurrent neural network is the backbone of LSTM networks.}\label{fig:rnndiag}
\end{figure}

The exact shape and function of network \(A\) are beyond the reach of this book. For further study, Christopher Olah's blog post \href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}{``Understanding LSTM Networks''} gives a more technical overview of how LSTM networks work.

The Keras library has convenient functions for broadly used architectures like LSTMs so we don't have to build it from scratch using layers; we can instead use \texttt{layer\_lstm()}. This comes \emph{after} an embedding layer that makes dense vectors from our word sequences and \emph{before} a densely-connected layer for output.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}

\NormalTok{lstm\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{lstm\_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Model
#> Model: "sequential"
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> embedding (Embedding)               (None, None, 32)                640032      
#> ________________________________________________________________________________
#> lstm (LSTM)                         (None, 32)                      8320        
#> ________________________________________________________________________________
#> dense (Dense)                       (None, 1)                       33          
#> ================================================================================
#> Total params: 648,385
#> Trainable params: 648,385
#> Non-trainable params: 0
#> ________________________________________________________________________________
\end{verbatim}

\begin{rmdwarning}
Notice the number of parameters in this LSTM model, about twice as many as the dense neural networks in Chapter \ref{dldnn}. It is easier to overfit an LSTM model, and it takes more time and memory to train, because of the large number of parameters.
\end{rmdwarning}

Because we are training a binary classification model, we use \texttt{activation\ =\ "sigmoid"} for the last layer; we want to fit and predict to class probabilities.

Next we \texttt{compile()} the model, which configures the model for training with a specific optimizer and set of metrics.

\begin{rmdnote}
A good default optimizer for many problems is \texttt{"adam"}
{[}@kingma2017adam{]}, and a good loss function for binary
classification is \texttt{"binary\_crossentropy"}.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lstm\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
As we noted in Chapter \ref{dldnn}, the neural network model is modified \textbf{in place}; the object \texttt{lstm\_mod} is different after we compile it, even though we didn't assign the object to anything. This is different from how most objects in R work, so pay special attention to the state of your model objects.
\end{rmdwarning}

After the model is compiled, we can fit it. The \texttt{fit()} method for Keras models has an argument \texttt{validation\_split} that will set apart a fraction of the training data for evaluation and assessment. The performance metrics are evaluated on the validation set at the \emph{end} of each epoch.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lstm\_history }\OtherTok{\textless{}{-}}\NormalTok{ lstm\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_matrix,}
\NormalTok{    kickstarter\_train}\SpecialCharTok{$}\NormalTok{state,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.25}\NormalTok{,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{lstm\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.2452
#>     accuracy: 0.8881
#>     val_loss: 0.5849
#> val_accuracy: 0.7912
\end{verbatim}

The loss on the training data (called \texttt{loss} here) is much better than the loss on the validation data (\texttt{val\_loss}), indicating that we are overfitting\index{overfitting} pretty dramatically. We can see this by plotting the history as well in Figure \ref{fig:firstlstmhistory}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(lstm\_history)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{09_dl_lstm_files/figure-latex/firstlstmhistory-1} 

}

\caption{Training and validation metrics for LSTM}\label{fig:firstlstmhistory}
\end{figure}

\begin{rmdnote}
Remember that lower loss indicates a better fitting model, and higher
accuracy (closer to 1) indicates a better model.
\end{rmdnote}

This model continues to improve epoch after epoch on the training data, but performs worse on the validation set than the training set after the first few epochs and eventually starts to exhibit \emph{worsening} performance on the validation set as epochs pass, demonstrating how extremely it is overfitting to the training data. This is very common for powerful deep learning models, including LSTMs.

\hypertarget{lstmevaluation}{%
\subsection{Evaluation}\label{lstmevaluation}}

We used some Keras defaults for model evaluation in the previous section, but just like we demonstrated in Section \ref{evaluate-dnn}, we can take more control if we want or need to. Instead of using the \texttt{validation\_split} argument, we can use the \texttt{validation\_data} argument and send in our own validation set creating with rsample.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{234}\NormalTok{)}
\NormalTok{kick\_val }\OtherTok{\textless{}{-}} \FunctionTok{validation\_split}\NormalTok{(kickstarter\_train, }\AttributeTok{strata =}\NormalTok{ state)}
\NormalTok{kick\_val}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Validation Set Split (0.75/0.25)  using stratification 
#> # A tibble: 1 x 2
#>   splits                 id        
#>   <list>                 <chr>     
#> 1 <split [151571/50522]> validation
\end{verbatim}

We can access the two data sets specified by this \texttt{split} via the functions \texttt{analysis()} (the analog to training) and \texttt{assessment()} (the analog to testing). We need to apply our prepped preprocessing recipe \texttt{kick\_prep} to both to transform this data to the appropriate format for our neural network architecture.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                      \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(kick\_analysis)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 151571     30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_assess }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                    \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(kick\_assess)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 50522    30
\end{verbatim}

These are each matrices appropriate for a Keras model. We will also need the outcome variables for both sets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}
\NormalTok{state\_assess }\OtherTok{\textless{}{-}} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}
\end{Highlighting}
\end{Shaded}

Let's also think about our LSTM model architecture. We saw evidence for significant overfitting\index{overfitting} with our first LSTM, and we can counteract that by including dropout, both in the regular sense (\texttt{dropout}) and in the feedback loops (\texttt{recurrent\_dropout}).

\begin{rmdwarning}
When we include some dropout, we temporarily remove some units together with their connections from the network. The purpose of this is typically to reduce overfitting \citep{Srivastava2014}. Dropout is not exclusive to LSTM models, and can also be used in many other kinds of network architectures. Another way to add dropout to a network is with \texttt{layer\_dropout()}.
\end{rmdwarning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lstm\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{lstm\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{val\_history }\OtherTok{\textless{}{-}}\NormalTok{ lstm\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_analysis,}
\NormalTok{    state\_analysis,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{val\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.3847
#>     accuracy: 0.8192
#>     val_loss: 0.5991
#> val_accuracy: 0.7311
\end{verbatim}

The \index{overfitting}overfitting has been reduced, and Figure \ref{fig:lstmvalhistory} shows that the difference between our model's performance on training and validation data is now smaller.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(val\_history)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{09_dl_lstm_files/figure-latex/lstmvalhistory-1} 

}

\caption{Training and validation metrics for LSTM with dropout}\label{fig:lstmvalhistory}
\end{figure}

Remember that this is specific validation data that we have chosen ahead of time, so we can evaluate metrics flexibly in any way we need to, for example, using yardstick functions. We can create a tibble with the true and predicted values for the validation set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(lstm\_mod, kick\_assess, state\_assess)}
\NormalTok{val\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.731
#> 2 kap         binary         0.458
#> 3 mn_log_loss binary         0.599
#> 4 roc_auc     binary         0.804
\end{verbatim}

A regularized linear model trained on this data set achieved results of accuracy of 0.684 and an AUC for the ROC curve of 0.752 (Appendix \ref{appendixbaseline}). This first LSTM with dropout is already performing better than such a linear model. We can plot the ROC curve in Figure \ref{fig:lstmvalroc} to evaluate the performance across the range of thresholds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(state, .pred\_1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{09_dl_lstm_files/figure-latex/lstmvalroc-1} 

}

\caption{ROC curve for LSTM with dropout predictions of Kickstarter campaign success}\label{fig:lstmvalroc}
\end{figure}

\hypertarget{compare-to-a-recurrent-neural-network}{%
\section{Compare to a recurrent neural network}\label{compare-to-a-recurrent-neural-network}}

\index{neural network!recurrent}An LSTM is actually a specific kind of recurrent neural network (RNN) \citep{ELMAN1990179}. Simple RNNs have feedback loops and hidden state that allow information to persist through steps but do not have memory cells like LSTMs. This difference between RNNs and LSTMs amounts to what happens in network \(A\) in Figure \ref{fig:rnndiag}. RNNs tend to have a very simple structure, typically just a single \texttt{tanh()} layer, much simpler than what happens in LSTMs.

\begin{rmdwarning}
Simple RNNs can only connect very recent information and structure in
sequences, but LSTMS can learn long-range dependencies and broader
context.
\end{rmdwarning}

Let's train an RNN to see how it compares to the LSTM.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rnn\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_simple\_rnn}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{rnn\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{rnn\_history }\OtherTok{\textless{}{-}}\NormalTok{ rnn\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_analysis,}
\NormalTok{    state\_analysis,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{rnn\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.5064
#>     accuracy: 0.7601
#>     val_loss: 0.5881
#> val_accuracy: 0.7114
\end{verbatim}

Looks like more \index{overfitting}overfitting! We can see this by plotting the history as well in Figure \ref{fig:rnnhistory}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(rnn\_history)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{09_dl_lstm_files/figure-latex/rnnhistory-1} 

}

\caption{Training and validation metrics for RNN}\label{fig:rnnhistory}
\end{figure}

These results are pretty disappointing overall, with worse performance than our first LSTM. Simple RNNs like the ones in this section can be challenging to train well, and just cranking up the number of embedding dimensions, units, or other network characteristics usually does not fix the problem. Often, RNNs just don't work well compared to simpler deep learning architectures like the dense network introduced in Section \ref{firstdlclassification} \citep{Minaee2020}, or even other machine learning approaches like regularized linear models with good preprocessing.

Fortunately, we can build on the ideas of a simple RNN with more complex architectures like LSTMs to build better performing models.

\hypertarget{bilstm}{%
\section{Case study: bidirectional LSTM}\label{bilstm}}

The RNNs and LSTMs that we have fit so far have modeled text as sequences, specifically sequences where information and memory persists moving forward. These kinds of models can learn structures and dependencies moving forward \emph{only}. In language, the \index{language!structures}structures move both directions, though; the words that come \emph{after} a given structure or word can be just as important for understanding it as the ones that come before it.

We can build this into our neural network architecture with a \textbf{bidirectional} wrapper for RNNs or LSTMs.

\begin{rmdnote}
A bidirectional LSTM allows the network to have both the forward and
backward information about the sequences at each step.
\end{rmdnote}

The input sequences are passed through the network in two directions, both forward and backward, allowing the network to learn more context, structures, and dependencies.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bilstm\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bidirectional}\NormalTok{(}\FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{,}
                           \AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{bilstm\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{bilstm\_history }\OtherTok{\textless{}{-}}\NormalTok{ bilstm\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_analysis,}
\NormalTok{    state\_analysis,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{bilstm\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.3627
#>     accuracy: 0.831
#>     val_loss: 0.6186
#> val_accuracy: 0.7395
\end{verbatim}

The bidirectional LSTM is more able to represent the data well, but with the same amount of dropout, we do see more dramatic overfitting. Still, there is some improvement on the validation set as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bilstm\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(bilstm\_mod, kick\_assess, state\_assess)}
\NormalTok{bilstm\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.740
#> 2 kap         binary         0.475
#> 3 mn_log_loss binary         0.619
#> 4 roc_auc     binary         0.810
\end{verbatim}

This bidirectional LSTM, able to learn both forward and backward text structures, provides some improvement over the regular LSTM on the validation set (which had an accuracy of 0.731).

\hypertarget{case-study-stacking-lstm-layers}{%
\section{Case study: stacking LSTM layers}\label{case-study-stacking-lstm-layers}}

Deep learning architectures can be built up to create extremely complex networks. For example, RNN and/or LSTM layers can be stacked on top of each other, or together with other kinds of layers. The idea of this stacking is to increase the ability of a network to represent the data well.

\begin{rmdwarning}
Intermediate layers must be set up to return sequences (with
\texttt{return\_sequences\ =\ TRUE}) instead of the last output for each
sequence.
\end{rmdwarning}

Let's start by adding one single additional layer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stacked\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{,}
             \AttributeTok{return\_sequences =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{stacked\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{stacked\_history }\OtherTok{\textless{}{-}}\NormalTok{ stacked\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_analysis,}
\NormalTok{    state\_analysis,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{stacked\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.3881
#>     accuracy: 0.819
#>     val_loss: 0.6024
#> val_accuracy: 0.7325
\end{verbatim}

Adding another separate layer in the forward direction appears to have improved the network, about as much as extending the LSTM layer to handle information in the backward direction via the bidirectional LSTM.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stacked\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(stacked\_mod, kick\_assess, state\_assess)}
\NormalTok{stacked\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.732
#> 2 kap         binary         0.462
#> 3 mn_log_loss binary         0.602
#> 4 roc_auc     binary         0.804
\end{verbatim}

We can gradually improve a model by changing and adding to its architecture.

\hypertarget{lstmpadding}{%
\section{Case study: padding}\label{lstmpadding}}

\index{preprocessing!impact}One of the most important themes of this book is that text must be heavily preprocessed in order to be useful for machine learning algorithms, and these preprocessing decisions have big effects on model results. One decision that seems like it may not be all that important is how sequences are \emph{padded} for a deep learning model. The matrix that is used as input for a neural network must be rectangular, but the training data documents are typically all different lengths. Sometimes, like the case of the Supreme Court opinions, the lengths vary a lot; sometimes, like with the Kickstarter data, the lengths vary a little bit.

Either way, the sequences that are too long must be truncated and the sequences that are too short must be padded, typically with zeroes. This does literally mean that words or tokens are thrown away for the long documents and zeroes are added to the shorter documents, with the goal of creating a rectangular matrix that can be used for computation.

\begin{rmdnote}
It is possible to set up an LSTM network that works with sequences of
varied length; this can sometimes improve performance but takes more
work to set up and is outside the scope of this book.
\end{rmdnote}

The default in textrecipes, as well as most deep learning for text, is \texttt{padding\ =\ "pre"}, where zeroes are added at the beginning, and \texttt{truncating\ =\ "pre"}, where values at the beginning are removed. \index{preprocessing!impact}What happens if we change one of these defaults?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{padding\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =}\NormalTok{ max\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(blurb, }\AttributeTok{sequence\_length =}\NormalTok{ max\_length, }\AttributeTok{padding =} \StringTok{"post"}\NormalTok{)}

\NormalTok{padding\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(padding\_rec)}
\NormalTok{padding\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(padding\_prep, }\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(padding\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 202093     30
\end{verbatim}

This matrix has the same dimensions as \texttt{kick\_matrix} but instead of padding with zeroes at the beginning of these Kickstarter blurbs, this matrix is padded with zeroes at the end. (This preprocessing strategy still truncates longer sequences in the same way.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pad\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(padding\_prep, }\AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                     \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{pad\_assess }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(padding\_prep, }\AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                   \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, let's create and fit an LSTM to this preprocessed data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{padding\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{padding\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{padding\_history }\OtherTok{\textless{}{-}}\NormalTok{ padding\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    pad\_analysis,}
\NormalTok{    state\_analysis,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(pad\_assess, state\_assess),}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{padding\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.4412
#>     accuracy: 0.7793
#>     val_loss: 0.5788
#> val_accuracy: 0.7136
\end{verbatim}

This padding strategy results in noticeably worse performance than the default option!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{padding\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(padding\_mod, pad\_assess, state\_assess)}
\NormalTok{padding\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.714
#> 2 kap         binary         0.428
#> 3 mn_log_loss binary         0.579
#> 4 roc_auc     binary         0.790
\end{verbatim}

The same model architecture with default padding preprocessing resulted in an accuracy of 0.731 and an AUC of 0.804; changing to \texttt{padding\ =\ "post"} has resulted in a remarkable degrading of predictive capacity. This result is typically attributed to the RNN/LSTM's hidden states being flushed out by the added zeroes, before getting to the text itself.

\begin{rmdwarning}
Different preprocessing strategies have a huge impact on deep learning
results.
\end{rmdwarning}

\hypertarget{case-study-training-a-regression-model}{%
\section{Case study: training a regression model}\label{case-study-training-a-regression-model}}

All our deep learning models for text so far have used the Kickstarter crowdfunding blurbs to predict whether the campaigns were successful or not, a classification problem. In our experience, classification is more common than regression tasks with text data, but these techniques can be used for either kind of supervised machine learning question. Let's return to the regression problem of Chapter \ref{mlregression} and predict the year of United States Supreme Court decisions, starting out by splitting into training and testing sets.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(scotus)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{scotus\_split }\OtherTok{\textless{}{-}}\NormalTok{ scotus\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{year =}\NormalTok{ (}\FunctionTok{as.numeric}\NormalTok{(year) }\SpecialCharTok{{-}} \DecValTok{1920}\NormalTok{) }\SpecialCharTok{/} \DecValTok{50}\NormalTok{,}
    \AttributeTok{text =} \FunctionTok{str\_remove\_all}\NormalTok{(text, }\StringTok{"\textquotesingle{}"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{initial\_split}\NormalTok{(}\AttributeTok{strata =}\NormalTok{ year)}

\NormalTok{scotus\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(scotus\_split)}
\NormalTok{scotus\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(scotus\_split)}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
Notice that we also shifted (subtracted) and scaled (divided) the
\texttt{year} outcome by constant factors so all the values are centered
around zero and not too large. Neural networks for regression problems
typically behave better when dealing with outcomes that are roughly
between -1 and 1.
\end{rmdwarning}

Next, let's build a preprocessing recipe for these Supreme Court decisions. These documents are much longer than the Kickstarter blurbs, many thousands of words long instead of just a handful. Let's try keeping the size of our vocabulary the same (\texttt{max\_words}) but we will need to increase the sequence length information we store (\texttt{max\_length}) by a great deal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_words }\OtherTok{\textless{}{-}} \FloatTok{2e4}
\NormalTok{max\_length }\OtherTok{\textless{}{-}} \FloatTok{1e3}

\NormalTok{scotus\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ scotus\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =}\NormalTok{ max\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(text, }\AttributeTok{sequence\_length =}\NormalTok{ max\_length)}

\NormalTok{scotus\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(scotus\_rec)}
\NormalTok{scotus\_train\_baked }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(scotus\_prep,}
                           \AttributeTok{new\_data =}\NormalTok{ scotus\_train,}
                           \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{scotus\_test\_baked }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(scotus\_prep,}
                          \AttributeTok{new\_data =}\NormalTok{ scotus\_test,}
                          \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

What does our training data look like now?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(scotus\_train\_baked)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 7502 1000
\end{verbatim}

We only have 7502 rows of training data, and because these documents are so long and we want to keep more of each sequence, the training data has 1000 columns. You are probably starting to guess that we are going to run into problems.

Let's create an LSTM and see what we can do. We will need to use higher-dimensional embeddings, since our sequences are much longer (we may want to increase the number of \texttt{units} as well, but will leave that out for the time being). Because we are training a regression model, there is no activation function for the last layer; we want to fit and predict to arbitrary values for the year.

\begin{rmdnote}
A good default loss function for regression is mean squared error,
\texttt{"mse"}.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{64}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{)}

\NormalTok{scotus\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"mse"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"mean\_squared\_error"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{scotus\_history }\OtherTok{\textless{}{-}}\NormalTok{ scotus\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    scotus\_train\_baked,}
\NormalTok{    scotus\_train}\SpecialCharTok{$}\NormalTok{year,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.25}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

How does this model perform on the test data? Let's transform back to real values for \texttt{year} so our metrics will be on the same scale as in Chapter \ref{mlregression}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scotus\_res }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{year =}\NormalTok{ scotus\_test}\SpecialCharTok{$}\NormalTok{year,}
                     \AttributeTok{.pred =} \FunctionTok{predict}\NormalTok{(scotus\_mod, scotus\_test\_baked)[, }\DecValTok{1}\NormalTok{]) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{everything}\NormalTok{(), }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{*} \DecValTok{50} \SpecialCharTok{+} \DecValTok{1920}\NormalTok{))}

\NormalTok{scotus\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(year, .pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 3 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 rmse    standard      19.1  
#> 2 rsq     standard       0.857
#> 3 mae     standard      13.8
\end{verbatim}

This is much worse than the final regularized linear model trained in Section \ref{mlregressionfull}, with an RMSE almost a decade worth of years worse. It's possible we may be able to do a little better than this simple LSTM, but as this chapter has demonstrated, our improvements will likely not be enormous compared to the first LSTM baseline.

\begin{rmdwarning}
The main problem with this regression model is that there isn't that
much data to start with; this is an example where a deep learning model
is \emph{not} a good choice and we should stick with a different machine
learning algorithm like regularized regression.
\end{rmdwarning}

\hypertarget{case-study-vocabulary-size}{%
\section{Case study: vocabulary size}\label{case-study-vocabulary-size}}

In this chapter so far, we've worked with a vocabulary of 20,000 words or tokens. This is a \emph{hyperparameter} of the model, and could be tuned, as we show in detail in Section \ref{keras-hyperparameter}. Instead of tuning in this chapter, let's try a smaller value, corresponding to faster \index{preprocessing}preprocessing and model fitting but a less powerful model, and explore whether and how much it affects model performance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_words }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{max\_length }\OtherTok{\textless{}{-}} \DecValTok{30}

\NormalTok{smaller\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =}\NormalTok{ max\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(blurb, }\AttributeTok{sequence\_length =}\NormalTok{ max\_length)}

\NormalTok{kick\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(smaller\_rec)}
\NormalTok{kick\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                      \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{kick\_assess }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                    \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once our preprocessing is done and applied to our validation split \texttt{kick\_val}, we can set up our model, another straightforward LSTM neural network.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smaller\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_lstm}\NormalTok{(}\AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{smaller\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{smaller\_history }\OtherTok{\textless{}{-}}\NormalTok{ smaller\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_analysis,}
\NormalTok{    state\_analysis,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess),}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{smaller\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.4643
#>     accuracy: 0.771
#>     val_loss: 0.5834
#> val_accuracy: 0.7101
\end{verbatim}

How did this smaller model, based on a smaller vocabulary in the model, perform?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{smaller\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(smaller\_mod, kick\_assess, state\_assess)}
\NormalTok{smaller\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.710
#> 2 kap         binary         0.417
#> 3 mn_log_loss binary         0.583
#> 4 roc_auc     binary         0.783
\end{verbatim}

The original LSTM model with the larger vocabulary had an accuracy of 0.731 and an AUC of 0.804. Reducing the model's capacity to capture and learn text meaning by restricting its access to vocabulary does result in a corresponding reduction in model performance, but a small one.

\begin{rmdnote}
The relationship between this hyperparameter and model performance is
weak over this range. Notice that we cut the vocabulary in half, and saw
only modest reductions in accuracy.
\end{rmdnote}

\hypertarget{lstmfull}{%
\section{The full game: LSTM}\label{lstmfull}}

We've come a long way in this chapter, even though we've focused on a very specific kind of recurrent neural network, the LSTM. Let's step back and build one final model, incorporating what we have been able to learn.

\hypertarget{lstmfullpreprocess}{%
\subsection{Preprocess the data}\label{lstmfullpreprocess}}

\index{preprocessing}We know that we want to stick with the defaults for padding, and to use a larger vocabulary for our final model. For this final model, we are not going to use our validation split again, so we only need to preprocess the training data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_words }\OtherTok{\textless{}{-}} \FloatTok{2e4}
\NormalTok{max\_length }\OtherTok{\textless{}{-}} \DecValTok{30}

\NormalTok{kick\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =}\NormalTok{ max\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(blurb, }\AttributeTok{sequence\_length =}\NormalTok{ max\_length)}

\NormalTok{kick\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(kick\_rec)}
\NormalTok{kick\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}

\FunctionTok{dim}\NormalTok{(kick\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 202093     30
\end{verbatim}

\hypertarget{lstmfullmodel}{%
\subsection{Specify the model}\label{lstmfullmodel}}

We've learned a lot about how to model this data set over the course of this chapter.

\begin{itemize}
\item
  We can use \texttt{dropout} to reduce overfitting.
\item
  Let's stack several layers together, and in fact increase the number of LSTM layers to three.
\item
  The bidirectional LSTM performed better than the regular LSTM, so let's set up each LSTM layer to be able to learn sequences in both directions.
\end{itemize}

Instead of using specific validation data that we can then compute performance metrics for, let's go back to specifying \texttt{validation\_split\ =\ 0.1} and let the Keras model choose the validation set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{32}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bidirectional}\NormalTok{(}\FunctionTok{layer\_lstm}\NormalTok{(}
    \AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{,}
    \AttributeTok{return\_sequences =} \ConstantTok{TRUE}
\NormalTok{  )) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bidirectional}\NormalTok{(}\FunctionTok{layer\_lstm}\NormalTok{(}
    \AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}\NormalTok{,}
    \AttributeTok{return\_sequences =} \ConstantTok{TRUE}
\NormalTok{  )) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bidirectional}\NormalTok{(}\FunctionTok{layer\_lstm}\NormalTok{(}
    \AttributeTok{units =} \DecValTok{32}\NormalTok{, }\AttributeTok{dropout =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{recurrent\_dropout =} \FloatTok{0.4}
\NormalTok{  )) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{final\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{final\_history }\OtherTok{\textless{}{-}}\NormalTok{ final\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_matrix,}
\NormalTok{    kickstarter\_train}\SpecialCharTok{$}\NormalTok{state,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{final\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.3307
#>     accuracy: 0.8481
#>     val_loss: 0.5666
#> val_accuracy: 0.7801
\end{verbatim}

This looks promising! Let's finally turn to the testing set, for the first time during this chapter, to evaluate this last model on data that has never been touched as part of the fitting process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_matrix\_test }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =}\NormalTok{ kickstarter\_test,}
                         \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{final\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(final\_mod, kick\_matrix\_test, kickstarter\_test}\SpecialCharTok{$}\NormalTok{state)}
\NormalTok{final\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.761
#> 2 kap         binary         0.520
#> 3 mn_log_loss binary         0.609
#> 4 roc_auc     binary         0.834
\end{verbatim}

This is our best performing model in this chapter on LSTM models, although not by much. We can again create an ROC curve, this time using the test data in Figure \ref{fig:lstmfinalroc}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(state, .pred\_1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{09_dl_lstm_files/figure-latex/lstmfinalroc-1} 

}

\caption{ROC curve for final LSTM model predictions on testing set of Kickstarter campaign success}\label{fig:lstmfinalroc}
\end{figure}

We have been able to incrementally improve our model by adding to the structure and making good choices about \index{preprocessing}preprocessing. We can visualize this final LSTM model's performance using a \index{matrix!confusion}confusion matrix as well, in Figure \ref{fig:lstmheatmap}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(state, .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{09_dl_lstm_files/figure-latex/lstmheatmap-1} 

}

\caption{Confusion matrix for final LSTM model predictions on testing set of Kickstarter campaign success}\label{fig:lstmheatmap}
\end{figure}

Notice that this final model still does not perform as well as any of the best models of Chapter \ref{dldnn}.

\begin{rmdnote}
For this data set of Kickstarter campaign blurbs, an LSTM architecture
is not turning out to give a great result compared to other options.
However, LSTMs typically perform very well for text data and are an
important piece of the text modeling toolkit.
\end{rmdnote}

For the Kickstarter data, these less-than-spectacular results are likely due to the documents' short lengths. LSTMs often work well for text data, but this is not universally true for all kinds of text. Also, keep in mind that LSTMs take both more \index{computational speed}time and memory to train, compared to the simpler models discussed in Chapter \ref{dldnn}.

\hypertarget{dllstmsummary}{%
\section{Summary}\label{dllstmsummary}}

LSTMs are a specific kind of recurrent neural network that are capable of learning long-range dependencies and broader context. They are often an excellent choice for building supervised models for text because of this ability to model sequences and structures within text like word dependencies. Text must be heavily preprocessed\index{preprocessing} for LSTMs in much the same way it needs to be preprocessed for dense neural networks, with tokenization and one-hot encoding of sequences. A major characteristic of LSTMs, like other deep learning architectures, is their tendency to memorize the features of training data; we can use strategies like dropout and ensuring that the batch size is large enough to reduce overfitting.

\hypertarget{in-this-chapter-you-learned-8}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-8}}

\begin{itemize}
\item
  how to preprocess text data for LSTM models
\item
  about RNN, LSTM, and bidirectional LSTM network architectures
\item
  how to use dropout to reduce overfitting for deep learning models
\item
  that network layers (including RNNs and LSTMs) can be stacked for greater model capacity
\item
  about the importance of centering and scaling regression outcomes for neural networks
\item
  how to evaluate LSTM models for text
\end{itemize}

\hypertarget{dlcnn}{%
\chapter{Convolutional neural networks}\label{dlcnn}}

The first neural networks\index{network architecture} we built in Chapter \ref{dldnn} did not have the capacity to learn much about structure, sequences, or long-range dependencies in our text data. The LSTM networks we trained in Chapter \ref{dllstm} were especially suited to learning long-range dependencies. In this final chapter, we will focus on \index{neural network!convolutional} \textbf{convolutional neural network} (CNN) architecture \citep{kim2014}, which can learn local, spatial structure within data.

CNNs can be well-suited for modeling text data because text often contains quite a lot of local structure. A CNN does not learn long-range structure within a sequence like an LSTM, but instead detects local patterns. A CNN network layer takes data (like text) as input and then hopefully produces output that represents specific structures\index{language!structure} in the data.

\begin{rmdnote}
Let's take more time with CNNs in this chapter to explore their
construction, different features, and the hyperparameters we can tune.
\end{rmdnote}

\hypertarget{what-are-cnns}{%
\section{What are CNNs?}\label{what-are-cnns}}

CNNs can work with data of different dimensions (like two-dimensional images or three-dimensional video), but for text modeling, we typically work in one dimension. The illustrations and explanations in this chapter use only one dimension to match the text use case.
Figure \ref{fig:cnn-architecture} illustrates a typical CNN architecture.
A convolutional filter slides along the sequence to produce a new, smaller sequence. This is repeated multiple times, typically with different parameters for each layer, until we are left with a small data cube which we can transform into our required output shape, a value between 0 and 1 in the case of binary classification.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{diagram-files/cnn-architecture} 

}

\caption{A template CNN architecture for one-dimensional input data. A sequence of consecutive CNN layers incremently reduces the size, ending with single output value.}\label{fig:cnn-architecture}
\end{figure}

This figure isn't entirely accurate because we technically don't feed characters into a CNN, but instead use one-hot sequence encoding (Section \ref{onehotsequence}) with a possible word embedding.
Let's talk about two of the most important CNN concepts, \textbf{kernels} and \textbf{kernel size}.

\hypertarget{kernel}{%
\subsection{Kernel}\label{kernel}}

The kernel is a small vector that slides along the input. When it is sliding, it performs element-wise multiplication of the values in the input and its own weights, and then sums up the values to get a single value.
Sometimes an activation function is applied as well.
It is these weights that are trained via gradient descent to find the best fit.
In Keras, the \texttt{filters} represent how many different kernels are trained in each layer. You typically start with fewer \texttt{filters} at the beginning of your network and then increase them as you go along.

\hypertarget{kernel-size}{%
\subsection{Kernel size}\label{kernel-size}}

The most prominent hyperparameter is the kernel size.
The kernel size is the length of the vector that contains the weights. A kernel of size 5 will have five weights. These kernels can capture local information similarly to how n-grams capture location patterns. Increasing the size of the kernel decreases the size of the output, as shown in Figure \ref{fig:cnn-kernel-size}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{diagram-files/cnn-kernel-size} 

}

\caption{The kernel size affects the size of the output. A kernel size of 3 uses the information from three values to compute one value.}\label{fig:cnn-kernel-size}
\end{figure}

Larger kernels learn larger and less frequent patterns, while smaller kernels will find fine-grained features.
Notice how the choice of token affects how we think about kernel size.
For character tokenization, a kernel size of 5 will (in early layers) find patterns in subwords more often than patterns across words, since five characters will typically not span multiple words.
By contrast, a kernel size of 5 with word tokenization will learn patterns within sentences instead.

\hypertarget{firstcnn}{%
\section{A first CNN model}\label{firstcnn}}

\index{neural network!convolutional}We will be using the same data which we examine in Sections \ref{kickstarter} and \ref{kickstarter-blurbs}, and use throughout Chapters \ref{dldnn} and \ref{dllstm}. This data set contains short text blurbs for prospective crowdfunding campaigns on Kickstarter, along with if they were successful. Our goal of this modeling is to predict successful campaigns from the text contained in the blurb. We will also use the same \index{preprocessing}preprocessing and feature engineering recipe that we created and described in Sections \ref{dnnrecipe} and \ref{firstlstm}.

Our first CNN will look a lot like what is shown in Figure \ref{fig:cnn-architecture}.
We start with an embedding layer, followed by a single one-dimensional convolution layer \texttt{layer\_conv\_1d()}, then a global max pooling layer \texttt{layer\_global\_max\_pooling\_1d()}, a densely connected layer, and end with a dense layer with a sigmoid activation function to give us one value between 0 and 1 to use in our binary classification task.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}

\NormalTok{simple\_cnn\_model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{16}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{32}\NormalTok{, }\AttributeTok{kernel\_size =} \DecValTok{5}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_global\_max\_pooling\_1d}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{simple\_cnn\_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Model
#> Model: "sequential"
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> embedding (Embedding)               (None, 30, 16)                  320016      
#> ________________________________________________________________________________
#> conv1d (Conv1D)                     (None, 26, 32)                  2592        
#> ________________________________________________________________________________
#> global_max_pooling1d (GlobalMaxPool (None, 32)                      0           
#> ________________________________________________________________________________
#> dense_1 (Dense)                     (None, 64)                      2112        
#> ________________________________________________________________________________
#> dense (Dense)                       (None, 1)                       65          
#> ================================================================================
#> Total params: 324,785
#> Trainable params: 324,785
#> Non-trainable params: 0
#> ________________________________________________________________________________
\end{verbatim}

We are using the same embedding layer with the same \texttt{max\_length} as in the previous networks so there is nothing new there.
The \texttt{layer\_global\_max\_pooling\_1d()} layer collapses the remaining CNN output into one dimension so we can finish it off with a densely connected layer and the sigmoid activation function.

This might not end up being the best CNN configuration, but it is a good starting point.
One of the challenges when working with CNNs is to ensure that we manage the dimensionality correctly. The length of the sequence decreases by \texttt{(kernel\_size\ -\ 1)} for each layer. For this input, we have a sequence of length \texttt{max\_length\ =\ 30}, which is decreased by \texttt{(5\ -\ 1)\ =\ 4} resulting in a sequence of 26, as shown in the printed output of \texttt{simple\_cnn\_model}. We could create seven layers with \texttt{kernel\_size\ =\ 5}, since we would end with \texttt{30\ -\ 4\ -\ 4\ -\ 4\ -\ 4\ -\ 4\ -\ 4\ -\ 4\ =\ 2} elements in the resulting sequence. However, we would not be able to do a network with 3 layers of
\texttt{kernel\_size\ =\ 7} followed by 3 layers of \texttt{kernel\_size\ =\ 5} since the resulting sequence would be \texttt{30\ -\ 6\ -\ 6\ -\ 6\ -\ 4\ -\ 4\ -\ 4\ =\ 0} and we must have a positive length for our sequence.
Remember that \texttt{kernel\_size} is not the only argument that will change the length of the resulting sequence. \index{network architecture}

\begin{rmdnote}
Constructing a sequence layer by layer and using Keras' print method to
check the configuration is a great way to make sure your architecture is
valid.
\end{rmdnote}

The compilation and fitting are the same as we have seen before, using a validation split created with tidymodels as shown in Sections \ref{evaluate-dnn} and \ref{lstmevaluation}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_cnn\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\NormalTok{cnn\_history }\OtherTok{\textless{}{-}}\NormalTok{ simple\_cnn\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ kick\_analysis,}
  \AttributeTok{y =}\NormalTok{ state\_analysis,}
  \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
  \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\index{optimization algorithm}

\begin{rmdnote}
We are using the \texttt{"adam"} optimizer since it performs well for
many kinds of models. You may have to experiment to find the optimizer
that works best for your model and data.
\end{rmdnote}

Now that the model is done fitting, we can evaluate it on the validation data set using the same \texttt{keras\_predict()} function we created in Section \ref{evaluate-dnn} and used throughout Chapters \ref{dldnn} and \ref{dllstm}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(simple\_cnn\_model, kick\_assess, state\_assess)}
\NormalTok{val\_res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 50,522 x 3
#>         .pred_1 .pred_class state
#>           <dbl> <fct>       <fct>
#>  1 0.0000000596 0           0    
#>  2 0.0130       0           0    
#>  3 0.000214     0           0    
#>  4 0.00112      0           0    
#>  5 0.000127     0           0    
#>  6 0.992        1           0    
#>  7 0.000208     0           0    
#>  8 0.00109      0           0    
#>  9 0.00170      0           0    
#> 10 0.000959     0           0    
#> # ... with 50,512 more rows
\end{verbatim}

We can calculate some standard metrics with \texttt{metrics()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{metrics}\NormalTok{(val\_res, state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.813
#> 2 kap         binary         0.624
#> 3 mn_log_loss binary         0.971
#> 4 roc_auc     binary         0.863
\end{verbatim}

We already see improvement over the densely connected network from Chapter \ref{dldnn}, our best performing model on the Kickstarter data so far.

The heatmap\index{matrix!confusion} in Figure \ref{fig:cnnheatmap} shows that the model performs about the same for the two classes, success and failure for the crowdfunding campaigns; we are getting fairly good results from a baseline CNN model!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(state, .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/cnnheatmap-1} 

}

\caption{Confusion matrix for first CNN model predictions of Kickstarter campaign success}\label{fig:cnnheatmap}
\end{figure}

The ROC curve in Figure \ref{fig:cnnroccurve} shows how the model performs at different thresholds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ state, .pred\_1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Receiver operator curve for Kickstarter blurbs"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/cnnroccurve-1} 

}

\caption{ROC curve for first CNN model predictions of Kickstarter campaign success}\label{fig:cnnroccurve}
\end{figure}

\hypertarget{case-study-adding-more-layers}{%
\section{Case study: adding more layers}\label{case-study-adding-more-layers}}

Now that we know how our basic CNN performs, we can see what happens when we apply some common modifications to it.
This case study will examine:

\begin{itemize}
\item
  how we can add additional \emph{convolutional} layers to our base model and
\item
  how additional \emph{dense} layers can be added.
\end{itemize}

\index{network architecture}Let's start by adding another fully connected layer. We take the architecture we used in \texttt{simple\_cnn\_model} and add another \texttt{layer\_dense()} after the first \texttt{layer\_dense()} in the model.
Increasing the depth of the model via the fully connected layers allows the model to find more complex patterns.
There is, however, a trade-off. Adding more layers adds more weights to the model, making it more complex and harder to train. If you don't have enough data or the patterns you are trying to classify aren't that complex, then model performance will suffer since the model will start overfitting as it starts memorizing patterns in the training data that don't generalize to new data.

\begin{rmdwarning}
When working with CNNs, the different layers perform different tasks. A
convolutional layer extracts local patterns as it slides along the
sequences, while a fully connected layer finds global patterns.
\end{rmdwarning}

We can think of the convolutional layers as doing preprocessing\index{preprocessing} on the text, which is then fed into the dense neural network that tries to fit the best curve. Adding more fully connected layers allows the network to create more intricate curves, and adding more convolutional layers creates richer features that are used when fitting the curves. Your job when constructing a CNN is to make the architecture just complex enough to match the data without overfitting. One ad-hoc rule to follow when refining your network architecture is to start small and keep adding layers until the validation error does not improve anymore.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cnn\_double\_dense }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{16}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{32}\NormalTok{, }\AttributeTok{kernel\_size =} \DecValTok{5}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_global\_max\_pooling\_1d}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{cnn\_double\_dense}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Model
#> Model: "sequential_1"
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> embedding_1 (Embedding)             (None, 30, 16)                  320016      
#> ________________________________________________________________________________
#> conv1d_1 (Conv1D)                   (None, 26, 32)                  2592        
#> ________________________________________________________________________________
#> global_max_pooling1d_1 (GlobalMaxPo (None, 32)                      0           
#> ________________________________________________________________________________
#> dense_4 (Dense)                     (None, 64)                      2112        
#> ________________________________________________________________________________
#> dense_3 (Dense)                     (None, 64)                      4160        
#> ________________________________________________________________________________
#> dense_2 (Dense)                     (None, 1)                       65          
#> ================================================================================
#> Total params: 328,945
#> Trainable params: 328,945
#> Non-trainable params: 0
#> ________________________________________________________________________________
\end{verbatim}

We can compile and fit this new model. We will try to keep as much as we can constant as we compare the different models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cnn\_double\_dense }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\NormalTok{history }\OtherTok{\textless{}{-}}\NormalTok{ cnn\_double\_dense }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ kick\_analysis,}
  \AttributeTok{y =}\NormalTok{ state\_analysis,}
  \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
  \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res\_double\_dense }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(}
\NormalTok{  cnn\_double\_dense,}
\NormalTok{  kick\_assess,}
\NormalTok{  state\_assess}
\NormalTok{)}

\FunctionTok{metrics}\NormalTok{(val\_res\_double\_dense, state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.809
#> 2 kap         binary         0.618
#> 3 mn_log_loss binary         1.04 
#> 4 roc_auc     binary         0.859
\end{verbatim}

This model performs well, but it is not entirely clear that it is working much better than the first CNN model we tried. This could be an indication that the original model had enough fully connected layers for the amount of training data we have available.

\begin{rmdwarning}
If we have two models with nearly identical performance, we should
choose the less complex of the two, since it will have faster
performance.
\end{rmdwarning}

We can also change the number of convolutional layers, by adding more such layers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cnn\_double\_conv }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{16}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{32}\NormalTok{, }\AttributeTok{kernel\_size =} \DecValTok{5}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_max\_pooling\_1d}\NormalTok{(}\AttributeTok{pool\_size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{64}\NormalTok{, }\AttributeTok{kernel\_size =} \DecValTok{3}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_global\_max\_pooling\_1d}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{cnn\_double\_conv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Model
#> Model: "sequential_2"
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> embedding_2 (Embedding)             (None, 30, 16)                  320016      
#> ________________________________________________________________________________
#> conv1d_3 (Conv1D)                   (None, 26, 32)                  2592        
#> ________________________________________________________________________________
#> max_pooling1d (MaxPooling1D)        (None, 13, 32)                  0           
#> ________________________________________________________________________________
#> conv1d_2 (Conv1D)                   (None, 11, 64)                  6208        
#> ________________________________________________________________________________
#> global_max_pooling1d_2 (GlobalMaxPo (None, 64)                      0           
#> ________________________________________________________________________________
#> dense_6 (Dense)                     (None, 64)                      4160        
#> ________________________________________________________________________________
#> dense_5 (Dense)                     (None, 1)                       65          
#> ================================================================================
#> Total params: 333,041
#> Trainable params: 333,041
#> Non-trainable params: 0
#> ________________________________________________________________________________
\end{verbatim}

There are a lot of different ways we can extend the network by adding convolutional layers with \texttt{layer\_conv\_1d()}. We must consider the individual characteristics of each layer, with respect to kernel size, as well as other CNN parameters we have not discussed in detail yet like stride, padding, and dilation rate. We also have to consider the progression of these layers within the network itself.
The model is using an increasing number of filters in each layer, doubling the number of filters for each layer. This is to ensure that there are more filters later on to capture enough of the global information.

This model is using kernel size of 5 twice. There aren't any hard rules about how you structure kernel sizes, but the sizes you choose will change what features the model can detect.

\begin{rmdnote}
The early layers extract general or low-level features while the later
layers learn finer detail or high-level features in the data. The choice
of kernel size determines the size of these features.
\end{rmdnote}

Having a small kernel size in the first layer will let the model detect low-level features locally.

We are also including a max-pooling layer with \texttt{layer\_max\_pooling\_1d()} between the convolutional layers. This layer performs a pooling operation that calculates the maximum values in its pooling window; in this model, that is set to 2.
This is done in the hope that the pooled features will be able to perform better by weeding out the small weights.
This is another parameter you can tinker with when you are designing the network.

We compile this model like the others, again trying to keep as much as we can constant. The only thing that changed in this model compared to the first is the addition of a \texttt{layer\_max\_pooling\_1d()} and a \texttt{layer\_conv\_1d()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cnn\_double\_conv }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\NormalTok{history }\OtherTok{\textless{}{-}}\NormalTok{ cnn\_double\_conv }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ kick\_analysis,}
  \AttributeTok{y =}\NormalTok{ state\_analysis,}
  \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
  \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res\_double\_conv }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(}
\NormalTok{  cnn\_double\_conv,}
\NormalTok{  kick\_assess,}
\NormalTok{  state\_assess}
\NormalTok{)}

\FunctionTok{metrics}\NormalTok{(val\_res\_double\_conv, state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.807
#> 2 kap         binary         0.614
#> 3 mn_log_loss binary         1.09 
#> 4 roc_auc     binary         0.856
\end{verbatim}

This model also performs well compared to earlier results. Let us extract the the prediction using \texttt{keras\_predict()} we defined in \ref{evaluate-dnn}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_cnn\_model\_predictions }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{mutate}\NormalTok{(val\_res, }\AttributeTok{model =} \StringTok{"Basic CNN"}\NormalTok{),}
  \FunctionTok{mutate}\NormalTok{(val\_res\_double\_dense, }\AttributeTok{model =} \StringTok{"Double Dense"}\NormalTok{),}
  \FunctionTok{mutate}\NormalTok{(val\_res\_double\_conv, }\AttributeTok{model =} \StringTok{"Double Conv"}\NormalTok{)}
\NormalTok{)}

\NormalTok{all\_cnn\_model\_predictions}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 151,566 x 4
#>         .pred_1 .pred_class state model    
#>           <dbl> <fct>       <fct> <chr>    
#>  1 0.0000000596 0           0     Basic CNN
#>  2 0.0130       0           0     Basic CNN
#>  3 0.000214     0           0     Basic CNN
#>  4 0.00112      0           0     Basic CNN
#>  5 0.000127     0           0     Basic CNN
#>  6 0.992        1           0     Basic CNN
#>  7 0.000208     0           0     Basic CNN
#>  8 0.00109      0           0     Basic CNN
#>  9 0.00170      0           0     Basic CNN
#> 10 0.000959     0           0     Basic CNN
#> # ... with 151,556 more rows
\end{verbatim}

Now that the results are combined in \texttt{all\_cnn\_model\_predictions} we can calculate group-wise evaluation statistics by grouping them by the \texttt{model} variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_cnn\_model\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(model) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 12 x 4
#>    model        .metric     .estimator .estimate
#>    <chr>        <chr>       <chr>          <dbl>
#>  1 Basic CNN    accuracy    binary         0.813
#>  2 Double Conv  accuracy    binary         0.807
#>  3 Double Dense accuracy    binary         0.809
#>  4 Basic CNN    kap         binary         0.624
#>  5 Double Conv  kap         binary         0.614
#>  6 Double Dense kap         binary         0.618
#>  7 Basic CNN    mn_log_loss binary         0.971
#>  8 Double Conv  mn_log_loss binary         1.09 
#>  9 Double Dense mn_log_loss binary         1.04 
#> 10 Basic CNN    roc_auc     binary         0.863
#> 11 Double Conv  roc_auc     binary         0.856
#> 12 Double Dense roc_auc     binary         0.859
\end{verbatim}

We can also compute ROC curves for all our models so far. Figure \ref{fig:allcnnroccurve} shows the three different ROC curves together in one chart.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_cnn\_model\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(model) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ state, .pred\_1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Receiver operator curve for Kickstarter blurbs"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/allcnnroccurve-1} 

}

\caption{ROC curve for three CNN variants' predictions of Kickstarter campaign success}\label{fig:allcnnroccurve}
\end{figure}

The curves are \emph{very} close in this chart, indicating that we don't have much to gain by adding more layers and that they don't improve performance substantively.
This doesn't mean that we are done with CNNs! There are still many things we can explore, like different tokenization approaches and hyperparameters that can be trained.

\hypertarget{case-study-byte-pair-encoding}{%
\section{Case study: byte pair encoding}\label{case-study-byte-pair-encoding}}

\index{tokenization!subword}In our models in this chapter so far we have used words as the token of interest. We saw in Section \ref{casestudyngrams} how n-grams can be used in modeling as well.
One of the reasons why the Kickstarter data set is hard to work with is because the text is quite short so we don't have that many individual tokens to work with in a given blurb.
Another choice of token is \emph{subwords}, where we split the text into smaller units than words; longer words especially will be broken into multiple subword units. One way to tokenize text into subword units is \emph{byte pair encoding} \citep{Gage1994ANA}.
This algorithm has been repurposed to work on text by iteratively merging frequently occurring subword pairs.
Methods such as \href{https://github.com/google-research/bert}{BERT} and \href{https://openai.com/blog/better-language-models/}{GPT-2} use subword units for text with great success.
The byte pair encoding algorithm has a hyperparameter controlling the size of the vocabulary. Setting it to higher values allows the models to find more rarely used character sequences in the text.

Byte pair encoding offers a good trade-off between character level and word level information, and can also encode unknown words. For example, suppose that the model is aware of the word ``woman''. A simple tokenizer would have to put a word such as ``womanhood'' into an unknown bucket or ignore it completely, whereas byte pair encoding should be able to pick up on the subwords ``woman'' and ``hood'' (or ``woman'', ``h'', and ``ood'', depending on whether the model found ``hood'' as a common enough subword).
Using a subword tokenizer such as byte pair encoding should let us see the text with more granularity since we will have more and smaller tokens for each observation.

\begin{rmdnote}
Character level CNNs have also proven successful in some contexts. They have been explored by \citet{Zhang2015} and work quite well on some shorter texts such as headlines and tweets \citep{Vosoughi2016}.
\end{rmdnote}

We need to remind ourselves that these models don't contain any linguistic knowledge at all; they only ``learn'' the morphological\index{morphology} patterns of sequences of characters (Section \ref{morphology}) in the training set. This does not make the models useless, but it should set our expectations about what any given model is capable of.

Since we are using a completely different preprocessing approach, we need to specify a new feature engineering recipe.

\begin{rmdpackage}
The \textbf{textrecipes} package has a tokenization engine to perform
byte pair encoding, but we need to determine the vocabulary size and the
appropriate sequence length.
\end{rmdpackage}

Let's write a function that takes a character vector and a vocabulary size and returns a dataframe with the number of tokens in each observation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textrecipes)}

\NormalTok{get\_bpe\_token\_dist }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(vocab\_size, x) \{}
  \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{text, }\AttributeTok{data =} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{text =}\NormalTok{ x)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_mutate}\NormalTok{(}\AttributeTok{text =} \FunctionTok{tolower}\NormalTok{(text)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_tokenize}\NormalTok{(text,}
                  \AttributeTok{engine =} \StringTok{"tokenizers.bpe"}\NormalTok{,}
                  \AttributeTok{training\_options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{vocab\_size =}\NormalTok{ vocab\_size)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{transmute}\NormalTok{(}\AttributeTok{n\_tokens =} \FunctionTok{lengths}\NormalTok{(textrecipes}\SpecialCharTok{:::}\FunctionTok{get\_tokens}\NormalTok{(text)),}
              \AttributeTok{vocab\_size =}\NormalTok{ vocab\_size)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can use \texttt{map()} to try a handful of different vocabulary sizes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bpe\_token\_dist }\OtherTok{\textless{}{-}} \FunctionTok{map\_dfr}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\DecValTok{2500}\NormalTok{, }\DecValTok{5000}\NormalTok{, }\DecValTok{10000}\NormalTok{, }\DecValTok{20000}\NormalTok{),}
\NormalTok{  get\_bpe\_token\_dist,}
\NormalTok{  kickstarter\_train}\SpecialCharTok{$}\NormalTok{blurb}
\NormalTok{)}
\NormalTok{bpe\_token\_dist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 808,372 x 2
#>    n_tokens vocab_size
#>       <int>      <dbl>
#>  1        9       2500
#>  2       35       2500
#>  3       27       2500
#>  4       32       2500
#>  5       22       2500
#>  6       45       2500
#>  7       35       2500
#>  8       29       2500
#>  9       39       2500
#> 10       33       2500
#> # ... with 808,362 more rows
\end{verbatim}

If we compare with the word count distribution we saw in Figure \ref{fig:kickstarterwordlength}, then we see in Figure \ref{fig:kickstartersubwordlength} that any of these choices for vocabulary size will result in more tokens overall.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bpe\_token\_dist }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(n\_tokens)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{vocab\_size) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of subwords per campaign blurb"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of campaign blurbs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/kickstartersubwordlength-1} 

}

\caption{Distribution of subword count for Kickstarter campaign blurbs for different vocabulary sizes}\label{fig:kickstartersubwordlength}
\end{figure}

Let's pick a vocabulary size of 10,000 and a corresponding sequence length of 40. To use byte pair encoding as a tokenizer in textrecipes set \texttt{engine\ =\ "tokenizers.bpe"}; the vocabulary size can be denoted using the \texttt{training\_options} argument. Everything else in the recipe stays the same.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_subwords }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{bpe\_max\_length }\OtherTok{\textless{}{-}} \DecValTok{40}

\NormalTok{bpe\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_mutate}\NormalTok{(}\AttributeTok{blurb =} \FunctionTok{tolower}\NormalTok{(blurb)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb,}
                \AttributeTok{engine =} \StringTok{"tokenizers.bpe"}\NormalTok{,}
                \AttributeTok{training\_options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{vocab\_size =}\NormalTok{ max\_subwords)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(blurb, }\AttributeTok{sequence\_length =}\NormalTok{ bpe\_max\_length)}

\NormalTok{bpe\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(bpe\_rec)}

\NormalTok{bpe\_analysis }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(bpe\_prep, }\AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                     \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{bpe\_assess }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(bpe\_prep, }\AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(kick\_val}\SpecialCharTok{$}\NormalTok{splits[[}\DecValTok{1}\NormalTok{]]),}
                   \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Our model will be very similar to the baseline CNN model from Section \ref{firstcnn}; we'll use a larger kernel size of 7 to account for the finer detail in the tokens.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cnn\_bpe }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{16}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ bpe\_max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{32}\NormalTok{, }\AttributeTok{kernel\_size =} \DecValTok{7}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_global\_max\_pooling\_1d}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{cnn\_bpe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Model
#> Model: "sequential_3"
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> embedding_3 (Embedding)             (None, 40, 16)                  320016      
#> ________________________________________________________________________________
#> conv1d_4 (Conv1D)                   (None, 34, 32)                  3616        
#> ________________________________________________________________________________
#> global_max_pooling1d_3 (GlobalMaxPo (None, 32)                      0           
#> ________________________________________________________________________________
#> dense_8 (Dense)                     (None, 64)                      2112        
#> ________________________________________________________________________________
#> dense_7 (Dense)                     (None, 1)                       65          
#> ================================================================================
#> Total params: 325,809
#> Trainable params: 325,809
#> Non-trainable params: 0
#> ________________________________________________________________________________
\end{verbatim}

We can compile and train like we have done so many times now.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cnn\_bpe }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}

\NormalTok{bpe\_history }\OtherTok{\textless{}{-}}\NormalTok{ cnn\_bpe }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{fit}\NormalTok{(}
\NormalTok{  bpe\_analysis,}
\NormalTok{  state\_analysis,}
  \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(bpe\_assess, state\_assess),}
  \AttributeTok{batch\_size =} \DecValTok{512}
\NormalTok{)}

\NormalTok{bpe\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.03634
#>     accuracy: 0.9933
#>     val_loss: 0.9621
#> val_accuracy: 0.8092
\end{verbatim}

The performance is doing quite well, which is a pleasant surprise! This is what we hoped would happen if we switched to a higher detail tokenizer.

The \index{matrix!confusion}confusion matrix in Figure \ref{fig:bpeheatmap} also clearly shows that there isn't much bias between the two classes with this new tokenizer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val\_res\_bpe }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(cnn\_bpe, bpe\_assess, state\_assess)}

\NormalTok{val\_res\_bpe }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(state, .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/bpeheatmap-1} 

}

\caption{Confusion matrix for CNN model using byte pair encoding tokenization}\label{fig:bpeheatmap}
\end{figure}

What are the subwords being used in this model? We can extract them from \texttt{step\_sequence\_onehot()} using \texttt{tidy()} on the prepped recipe. All the tokens that start with an \texttt{"h"} are seen here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bpe\_rec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{(}\DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(token, }\StringTok{"\^{}h"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(token)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "h"     "ha"    "hab"   "ham"   "hand"  "he"    "head"  "heart" "heast"
#> [10] "hed"   "heim"  "hel"   "help"  "hem"   "hen"   "her"   "here"  "hern" 
#> [19] "hero"  "hes"   "hes,"  "hes."  "hest"  "het"   "hetic" "hett"  "hib"  
#> [28] "hic"   "hing"  "hing." "hip"   "hist"  "hn"    "ho"    "hol"   "hold" 
#> [37] "hood"  "hop"   "hor"   "hous"  "house" "how"   "hr"    "hs"    "hu"
\end{verbatim}

Notice how some of these subword tokens\index{tokenization!subword} are full words and some are parts of words. This is what allows the model to be able to ``read'' long unknown words by combining many smaller sub words.
We can also look at common long words.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bpe\_rec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{(}\DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(token))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(token)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>  [1] "▁singer-songwriter" "▁singer/songwriter" "▁post-apocalyptic" 
#>  [4] "▁environmentally"   "▁interchangeable"   "▁post-production"  
#>  [7] "▁singer/songwrit"   "▁entertainment."    "▁feature-length"   
#> [10] "▁groundbreaking"    "▁illustrations."    "▁professionally"   
#> [13] "▁relationships."    "▁self-published"    "▁sustainability"   
#> [16] "▁transformation"    "▁unconventional"    "▁architectural"    
#> [19] "▁automatically"     "▁award-winning"     "▁collaborating"    
#> [22] "▁collaboration"     "▁collaborative"     "▁coming-of-age"    
#> [25] "▁communication"
\end{verbatim}

These twenty-five words were common enough to get their own subword token, and helps us understand the nature of these Kickstarter crowdfunding campaigns.

\begin{rmdwarning}
Examining the longest subword tokens gives you a good sense of the data
you are working with!
\end{rmdwarning}

\hypertarget{lime}{%
\section{Case study: explainability with LIME}\label{lime}}

\index{models!explainability}
\index{models!interpretability|see {models, explainability}}
We noted in Section \ref{dllimitations} that one of the significant limitations of deep learning models is that they are hard to reason about. One of the ways to understand a predictive model, even a ``black box''\index{"black box"} one, is using an algorithm for observation-level variable importance like \emph{Local Interpretable Model-Agnostic Explanations} \citep{ribeiro2016why} algorithm, or \textbf{LIME} for short.

\begin{rmdnote}
As indicated by its name, LIME is an approach to compute local feature
importance, or explainability at the individual observation level. It
does not offer global feature importance, or explainability for the
model as a whole.
\end{rmdnote}

\begin{rmdpackage}
The \textbf{lime} package in R {[}@R-lime{]} implements the LIME
algorithm; it can take a prediction from a model and determine a small
set of features in the original data that drive the outcome of the
prediction.
\end{rmdpackage}

To use this package we need to write a helper function to get the data in the format we want. The \texttt{lime()} function takes two mandatory arguments, \texttt{x} and \texttt{model}. The \texttt{model} argument is the trained model we are trying to explain. The \texttt{lime()} function works out of the box with Keras models so we should be good to go there. The \texttt{x} argument is the training data used for training the model. This is where we need to to create a helper function; the lime package is expecting \texttt{x} to be a character vector so we'll need a function that takes a character vector as input and returns the matrix the Keras model is expecting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_prepped\_rec }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(kick\_rec)}

\NormalTok{text\_to\_matrix }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{bake}\NormalTok{(}
\NormalTok{    kick\_prepped\_rec,}
    \AttributeTok{new\_data =} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{blurb =}\NormalTok{ x),}
    \AttributeTok{composition =} \StringTok{"matrix"}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
Since the function needs to be able to work with just the \texttt{x}
parameter alone, we need to put \texttt{prepped\_recipe} inside the
function rather than passing it in as an argument. This will work with
R's scoping rules but does require you to create a new function for each
recipe.
\end{rmdnote}

Let's select a couple of training observations to explain.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentence\_to\_explain }\OtherTok{\textless{}{-}}\NormalTok{ kickstarter\_train }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(blurb)}

\NormalTok{sentence\_to\_explain}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Exploring paint and its place in a digital world."
#> [2] "Mike Fassio wants a side-by-side photo of me and Hazel eating cake with
our bare hands.  Let's make this a reality!"
\end{verbatim}

We now load the lime package and pass observations into \texttt{lime()} along with the model we are trying to explain and the preprocess function.

\begin{rmdwarning}
Be sure that the preprocessing function \emph{matches} the preprocessing
that was used to train the model.
\end{rmdwarning}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lime)}

\NormalTok{explainer }\OtherTok{\textless{}{-}} \FunctionTok{lime}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ sentence\_to\_explain,}
  \AttributeTok{model =}\NormalTok{ simple\_cnn\_model,}
  \AttributeTok{preprocess =}\NormalTok{ text\_to\_matrix}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This \texttt{explainer} object can now be used with \texttt{explain()} to generate explanations for the sentences. We set \texttt{n\_labels\ =\ 1} to only get explanations for the first label, since we are working with a binary classification model\footnote{The explanations of the second label would just be the inverse of the first label. If you have more than two labels, it makes sense to explore some or all of them.}. We set \texttt{n\_features\ =\ 12} to return the twelve most important features. If we were dealing with longer text, we might want to change \texttt{n\_features} to return more features (tokens).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{explanation }\OtherTok{\textless{}{-}} \FunctionTok{explain}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ sentence\_to\_explain,}
  \AttributeTok{explainer =}\NormalTok{ explainer,}
  \AttributeTok{n\_labels =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{n\_features =} \DecValTok{12}
\NormalTok{)}

\NormalTok{explanation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 21 x 13
#>    model_type    case label label_prob model_r2 model_intercept model_prediction
#>  * <chr>        <int> <chr>      <dbl>    <dbl>           <dbl>            <dbl>
#>  1 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  2 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  3 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  4 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  5 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  6 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  7 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  8 classificat~     1 1          0.997    0.288           0.751            1.01 
#>  9 classificat~     1 1          0.997    0.288           0.751            1.01 
#> 10 classificat~     2 1          1.00     0.524           0.379            0.913
#> # ... with 11 more rows, and 6 more variables: feature <chr>,
#> #   feature_value <chr>, feature_weight <dbl>, feature_desc <chr>, data <chr>,
#> #   prediction <list>
\end{verbatim}

The output comes in a tibble format where \texttt{feature} and \texttt{feature\_weight} are included, but fortunately lime contains some functions to visualize these weights. Figure \ref{fig:limeplotfeatures} shows the result of using \texttt{plot\_features()}, with each facet containing an observation-label pair and the bars showing the weight of the different tokens. Bars in the positive direction (darker) indicate that the weights \emph{support} the prediction and bars in the negative direction (lighter) indicate \emph{contradictions}. This chart is great for finding the most prominent features in an observation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_features}\NormalTok{(explanation)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/limeplotfeatures-1} 

}

\caption{Plot of most important features for a CNN model predicting two observations.}\label{fig:limeplotfeatures}
\end{figure}

\index{models!explainability}Figure \ref{fig:limeplottextexplanations} shows the weights by highlighting the words directly in the text. This gives us a way to see if any local patterns contain a lot of weight.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_text\_explanations}\NormalTok{(explanation)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/plot_text_explanations_1} 

}

\caption{Feature highlighting of words for two examples explained by a CNN model.}\label{fig:limeplottextexplanations}
\end{figure}

\begin{rmdnote}
The \texttt{interactive\_text\_explanations()} function can be used to
launch an interactive Shiny app where you can explore the model weights.
\end{rmdnote}

\index{models!explainability}One of the ways a deep learning model is hard to explain is that changes to a part of the input can affect how the input is being used as a whole. Remember that in bag-of-words models adding another token when predicting would just add another unit in the weight; this is not always the case when using deep learning models.
The following example shows this effect. We have created two very similar sentences in \texttt{fake\_sentences}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fake\_sentences }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \StringTok{"Fun and exciting dice game for the whole family"}\NormalTok{,}
  \StringTok{"Fun and exciting dice game for the family"}
\NormalTok{)}

\NormalTok{explainer }\OtherTok{\textless{}{-}} \FunctionTok{lime}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ fake\_sentences,}
  \AttributeTok{model =}\NormalTok{ simple\_cnn\_model,}
  \AttributeTok{preprocess =}\NormalTok{ text\_to\_matrix}
\NormalTok{)}

\NormalTok{explanation }\OtherTok{\textless{}{-}} \FunctionTok{explain}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ fake\_sentences,}
  \AttributeTok{explainer =}\NormalTok{ explainer,}
  \AttributeTok{n\_labels =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{n\_features =} \DecValTok{12}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Explanations based on these two sentences are fairly similar as we can see in Figure \ref{fig:robustlimeplottextexplanations}. However, notice how the removal of the word ``whole'' affects the weights of the other words in the examples, in some cases switching the sign from supporting to contradicting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_text\_explanations}\NormalTok{(explanation)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/plot_text_explanations_2} 

}

\caption{Feature highlighting of words in two examples explained by a CNN model.}\label{fig:robustlimeplottextexplanations}
\end{figure}

\index{models!explainability}It is these kinds of correlated patterns that can make deep learning models hard to reason about and can deliver surprising results.

\begin{rmdnote}
The LIME algorithm and \textbf{lime} R package are not limited to
explaining CNNs. This approach can be used with any of the models we
have used in this book, even the ones trained with \textbf{parsnip}.
\end{rmdnote}

\hypertarget{keras-hyperparameter}{%
\section{Case study: hyperparameter search}\label{keras-hyperparameter}}

\index{models!tuning}So far in all our deep learning models, we have only used one configuration of hyperparameters. Sometimes we want to try different hyperparameters out and find what works best for our model like we did in Sections \ref{mlregressionfull} and \ref{mlclassificationfull} using the \textbf{tune} package. We can use the \href{https://tensorflow.rstudio.com/tools/tfruns/overview/}{\textbf{tfruns}} package to run multiple Keras models and compare the results.

This workflow will be a little different than what we have seen in the book so far since we will have to create a \texttt{.R} file that contains the necessary modeling steps and then use that file to fit multiple models. Such an example file named \texttt{cnn-spec.R} used for the following models is available \href{https://raw.githubusercontent.com/EmilHvitfeldt/smltar/master/cnn-spec.R}{on GitHub}. The first thing we need to do is specify what hyperparameters we want to vary. By convention, this object is named \texttt{FLAGS} and it is created using the \texttt{flags()} function. For each parameter we want to tune, we add a corresponding \texttt{flag\_*()} function, which can be \texttt{flag\_integer()}, \texttt{flag\_boolean()}, \texttt{flag\_numeric()}, or \texttt{flag\_string()} depending on what we need to tune.

\begin{rmdwarning}
Be sure you are using the right type for each of these flags; Keras is
quite picky! If Keras is expecting an integer and gets a numeric then
you will get an error.
\end{rmdwarning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{FLAGS }\OtherTok{\textless{}{-}} \FunctionTok{flags}\NormalTok{(}
  \FunctionTok{flag\_integer}\NormalTok{(}\StringTok{"kernel\_size1"}\NormalTok{, }\DecValTok{5}\NormalTok{),}
  \FunctionTok{flag\_integer}\NormalTok{(}\StringTok{"strides1"}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Notice how we are giving each flag a name and a possible value. The value itself isn't important, as it is not used once we start running multiple models, but it needs to be the right type for the model we are using.

Next, we specify the Keras model we want to run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{16}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{32}\NormalTok{,}
                \AttributeTok{kernel\_size =}\NormalTok{ FLAGS}\SpecialCharTok{$}\NormalTok{kernel\_size1,}
                \AttributeTok{strides =}\NormalTok{ FLAGS}\SpecialCharTok{$}\NormalTok{strides1,}
                \AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_global\_max\_pooling\_1d}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
  \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We target the hyperparameters we want to change by marking them as \texttt{FLAGS\$name}. So in this model, we are tuning different values of \texttt{kernel\_size} and \texttt{strides}, which are denoted by the \texttt{kernel\_size1} and \texttt{strides1} flag respectively.

Lastly, we must specify how the model is trained and evaluated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{history }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ kick\_analysis,}
    \AttributeTok{y =}\NormalTok{ state\_analysis,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(kick\_assess, state\_assess)}
\NormalTok{  )}

\FunctionTok{plot}\NormalTok{(history)}

\NormalTok{score }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{evaluate}\NormalTok{(}
\NormalTok{  kick\_assess, state\_assess}
\NormalTok{)}

\FunctionTok{cat}\NormalTok{(}\StringTok{"Test accuracy:"}\NormalTok{, score[}\StringTok{"accuracy"}\NormalTok{], }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is mostly the same as what we have seen before. When we are running these different models, the scripts will be run in the environment they are initialized from, so the models will have access to objects like \texttt{prepped\_training} and \texttt{kickstarter\_train} and we don't have to create them inside the file.

Now that we have the file set up we need to specify the different hyperparameters we want to try. Three different values for the kernel size and two different values for the stride length give us \texttt{3\ *\ 2\ =\ 6} different runs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyperparams }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{kernel\_size1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \AttributeTok{strides1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdnote}
This is a small selection of hyperparameters and ranges. There is much
more room for experimentation.
\end{rmdnote}

Now we have everything we need for hyperparameter searching. Load up \textbf{tfruns} and pass the name of the file we just created along with \texttt{hyperparams} to the \texttt{tuning\_run()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tfruns)}
\NormalTok{runs }\OtherTok{\textless{}{-}} \FunctionTok{tuning\_run}\NormalTok{(}
  \AttributeTok{file =} \StringTok{"cnn{-}spec.R"}\NormalTok{,}
  \AttributeTok{runs\_dir =} \StringTok{"\_tuning"}\NormalTok{,}
  \AttributeTok{flags =}\NormalTok{ hyperparams}
\NormalTok{)}

\NormalTok{runs\_results }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(}\FunctionTok{ls\_runs}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

You don't have to, but we have manually specified the \texttt{runs\_dir} argument, which is where the results of the tuning will be saved.

A summary of all the runs in the folder can be retrieved with \texttt{ls\_runs()}; here we use \texttt{as\_tibble()} to get the results as a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 6 x 24
#>   run_dir     eval_ metric_loss metric_accuracy metric_val_loss metric_val_accu~
#>   <chr>       <dbl>       <dbl>           <dbl>           <dbl>            <dbl>
#> 1 _tuning/20~ 1.01       0.0334           0.992           1.01             0.806
#> 2 _tuning/20~ 0.983      0.0374           0.991           0.983            0.808
#> 3 _tuning/20~ 0.995      0.0434           0.989           0.995            0.805
#> 4 _tuning/20~ 0.929      0.0311           0.994           0.929            0.812
#> 5 _tuning/20~ 0.942      0.0342           0.993           0.942            0.811
#> 6 _tuning/20~ 0.942      0.0447           0.989           0.942            0.808
#> # ... with 18 more variables: flag_kernel_size1 <int>, flag_strides1 <int>,
#> #   epochs <int>, epochs_completed <int>, metrics <chr>, model <chr>,
#> #   loss_function <chr>, optimizer <chr>, learning_rate <dbl>, script <chr>,
#> #   start <dttm>, end <dttm>, completed <lgl>, output <chr>, source_code <chr>,
#> #   context <chr>, type <chr>, NA. <dbl>
\end{verbatim}

We can condense the results down a little bit by only pulling out the flags we are looking at and arranging them according to their performance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_runs }\OtherTok{\textless{}{-}}\NormalTok{ runs\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(metric\_val\_accuracy, flag\_kernel\_size1, flag\_strides1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(metric\_val\_accuracy))}

\NormalTok{best\_runs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 6 x 3
#>   metric_val_accuracy flag_kernel_size1 flag_strides1
#>                 <dbl>             <int>         <int>
#> 1               0.812                 7             1
#> 2               0.811                 5             1
#> 3               0.808                 5             2
#> 4               0.808                 3             1
#> 5               0.806                 7             2
#> 6               0.805                 3             2
\end{verbatim}

There isn't much performance difference between the different choices but using kernel size of 7 and stride length of 1 narrowly came out on top.

\hypertarget{cross-validation-for-evaluation}{%
\section{Cross-validation for evaluation}\label{cross-validation-for-evaluation}}

In Section \ref{dnncross}, we saw how we can use resampling to create cross-validation folds for evaluation. The Kickstarter data set we are using is big enough that we have ample data for a single training set, validation set, and testing set that all contain enough observations in them to give reliable performance metrics. However, it is important to understand how to implement other resampling strategies for situations when your data budget may not be as plentiful or when you need to compute performance metrics that are more precise.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{345}\NormalTok{)}
\NormalTok{kick\_folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(kickstarter\_train, }\AttributeTok{v =} \DecValTok{5}\NormalTok{)}
\NormalTok{kick\_folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> #  5-fold cross-validation 
#> # A tibble: 5 x 2
#>   splits                 id   
#>   <list>                 <chr>
#> 1 <split [161674/40419]> Fold1
#> 2 <split [161674/40419]> Fold2
#> 3 <split [161674/40419]> Fold3
#> 4 <split [161675/40418]> Fold4
#> 5 <split [161675/40418]> Fold5
\end{verbatim}

Each of these folds has an analysis or training set and an assessment or validation set. Instead of training our model one time and getting one measure of performance, we can train our model \texttt{v} times and get \texttt{v} measures (five, in this case), for more reliability.

Last time we saw how to create a custom function to handle preprocessing, fitting, and evaluation. We will use the same approach of creating the function, but this time use the model specification from Section \ref{firstcnn}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_split }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(split, prepped\_rec) \{}
  \DocumentationTok{\#\# preprocessing}
\NormalTok{  x\_train }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(prepped\_rec, }\AttributeTok{new\_data =} \FunctionTok{analysis}\NormalTok{(split),}
                  \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{  x\_val   }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(prepped\_rec, }\AttributeTok{new\_data =} \FunctionTok{assessment}\NormalTok{(split),}
                  \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
  
  \DocumentationTok{\#\# create model}
\NormalTok{  y\_train }\OtherTok{\textless{}{-}} \FunctionTok{analysis}\NormalTok{(split) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}
\NormalTok{  y\_val   }\OtherTok{\textless{}{-}} \FunctionTok{assessment}\NormalTok{(split) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(state)}
  
\NormalTok{  mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{16}\NormalTok{,}
                    \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{32}\NormalTok{, }\AttributeTok{kernel\_size =} \DecValTok{5}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_global\_max\_pooling\_1d}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{compile}\NormalTok{(}
      \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
      \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
      \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{    )}
  
  \DocumentationTok{\#\# fit model}
\NormalTok{  mod }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{fit}\NormalTok{(}
\NormalTok{      x\_train,}
\NormalTok{      y\_train,}
      \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
      \AttributeTok{validation\_data =} \FunctionTok{list}\NormalTok{(x\_val, y\_val),}
      \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
      \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{    )}
  
  \DocumentationTok{\#\# evaluate model}
  \FunctionTok{keras\_predict}\NormalTok{(mod, x\_val, y\_val) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can \texttt{map()} this function across all our cross-validation folds. This takes longer than our previous models to train, since we are training for 10 epochs each on five folds.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_fitted }\OtherTok{\textless{}{-}}\NormalTok{ kick\_folds }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{validation =} \FunctionTok{map}\NormalTok{(splits, fit\_split, kick\_prep))}

\NormalTok{cv\_fitted}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> #  5-fold cross-validation 
#> # A tibble: 5 x 3
#>   splits                 id    validation          
#>   <list>                 <chr> <list>              
#> 1 <split [161674/40419]> Fold1 <tibble[,3] [4 x 3]>
#> 2 <split [161674/40419]> Fold2 <tibble[,3] [4 x 3]>
#> 3 <split [161674/40419]> Fold3 <tibble[,3] [4 x 3]>
#> 4 <split [161675/40418]> Fold4 <tibble[,3] [4 x 3]>
#> 5 <split [161675/40418]> Fold5 <tibble[,3] [4 x 3]>
\end{verbatim}

Now we can use \texttt{unnest()} to find the metrics we computed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_fitted }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(validation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 20 x 5
#>    splits                 id    .metric     .estimator .estimate
#>    <list>                 <chr> <chr>       <chr>          <dbl>
#>  1 <split [161674/40419]> Fold1 accuracy    binary         0.823
#>  2 <split [161674/40419]> Fold1 kap         binary         0.646
#>  3 <split [161674/40419]> Fold1 mn_log_loss binary         0.909
#>  4 <split [161674/40419]> Fold1 roc_auc     binary         0.872
#>  5 <split [161674/40419]> Fold2 accuracy    binary         0.827
#>  6 <split [161674/40419]> Fold2 kap         binary         0.654
#>  7 <split [161674/40419]> Fold2 mn_log_loss binary         0.878
#>  8 <split [161674/40419]> Fold2 roc_auc     binary         0.873
#>  9 <split [161674/40419]> Fold3 accuracy    binary         0.826
#> 10 <split [161674/40419]> Fold3 kap         binary         0.651
#> 11 <split [161674/40419]> Fold3 mn_log_loss binary         0.908
#> 12 <split [161674/40419]> Fold3 roc_auc     binary         0.874
#> 13 <split [161675/40418]> Fold4 accuracy    binary         0.824
#> 14 <split [161675/40418]> Fold4 kap         binary         0.647
#> 15 <split [161675/40418]> Fold4 mn_log_loss binary         0.888
#> 16 <split [161675/40418]> Fold4 roc_auc     binary         0.871
#> 17 <split [161675/40418]> Fold5 accuracy    binary         0.825
#> 18 <split [161675/40418]> Fold5 kap         binary         0.649
#> 19 <split [161675/40418]> Fold5 mn_log_loss binary         0.895
#> 20 <split [161675/40418]> Fold5 roc_auc     binary         0.871
\end{verbatim}

We can summarize the unnested results to match what we normally would get from \texttt{collect\_metrics()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_fitted }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(validation) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(.metric) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(.estimate),}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{std\_err =} \FunctionTok{sd}\NormalTok{(.estimate) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(n)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 4
#>   .metric      mean     n  std_err
#>   <chr>       <dbl> <int>    <dbl>
#> 1 accuracy    0.825     5 0.000673
#> 2 kap         0.649     5 0.00136 
#> 3 mn_log_loss 0.896     5 0.00595 
#> 4 roc_auc     0.872     5 0.000517
\end{verbatim}

The metrics have little variance just like they did last time, which is reassuring; our model is robust with respect to the evaluation metrics.

\hypertarget{cnnfull}{%
\section{The full game: CNN}\label{cnnfull}}

We've come a long way in this chapter, and looked at the many different modifications to the simple CNN model we started with. Most of the alterations didn't add much so this final model is not going to be much different than what we have seen so far.

\index{models!challenges}

\begin{rmdwarning}
There are an incredible number of ways to change a deep learning network
architecture, but in most realistic situations, the benefit in model
performance from such changes is modest.
\end{rmdwarning}

\hypertarget{cnnfullpreprocess}{%
\subsection{Preprocess the data}\label{cnnfullpreprocess}}

For this final model, we are not going to use our separate validation data again, so we only need to \index{preprocess}preprocess the training data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_words }\OtherTok{\textless{}{-}} \FloatTok{2e4}
\NormalTok{max\_length }\OtherTok{\textless{}{-}} \DecValTok{30}

\NormalTok{kick\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =}\NormalTok{ max\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_sequence\_onehot}\NormalTok{(blurb, }\AttributeTok{sequence\_length =}\NormalTok{ max\_length)}

\NormalTok{kick\_prep }\OtherTok{\textless{}{-}} \FunctionTok{prep}\NormalTok{(kick\_rec)}
\NormalTok{kick\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}

\FunctionTok{dim}\NormalTok{(kick\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 202093     30
\end{verbatim}

\hypertarget{cnnfullmodel}{%
\subsection{Specify the model}\label{cnnfullmodel}}

Instead of using specific validation data that we can then compute performance metrics for, let's go back to specifying \texttt{validation\_split\ =\ 0.1} and let the Keras model choose the validation set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_mod }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_embedding}\NormalTok{(}\AttributeTok{input\_dim =}\NormalTok{ max\_words }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{output\_dim =} \DecValTok{16}\NormalTok{,}
                  \AttributeTok{input\_length =}\NormalTok{ max\_length) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_1d}\NormalTok{(}\AttributeTok{filter =} \DecValTok{32}\NormalTok{, }\AttributeTok{kernel\_size =} \DecValTok{7}\NormalTok{,}
                \AttributeTok{strides =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_global\_max\_pooling\_1d}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{, }\AttributeTok{activation =} \StringTok{"sigmoid"}\NormalTok{)}

\NormalTok{final\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{optimizer =} \StringTok{"adam"}\NormalTok{,}
    \AttributeTok{loss =} \StringTok{"binary\_crossentropy"}\NormalTok{,}
    \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{final\_history }\OtherTok{\textless{}{-}}\NormalTok{ final\_mod }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    kick\_matrix,}
\NormalTok{    kickstarter\_train}\SpecialCharTok{$}\NormalTok{state,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{  )}

\NormalTok{final\_history}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Final epoch (plot to see history):
#>         loss: 0.03343
#>     accuracy: 0.9927
#>     val_loss: 0.7211
#> val_accuracy: 0.8653
\end{verbatim}

This looks promising! Let's finally turn to the testing set, for the first time during this chapter, to evaluate this last model on data that has never been touched as part of the fitting process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kick\_matrix\_test }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(kick\_prep, }\AttributeTok{new\_data =}\NormalTok{ kickstarter\_test,}
                         \AttributeTok{composition =} \StringTok{"matrix"}\NormalTok{)}
\NormalTok{final\_res }\OtherTok{\textless{}{-}} \FunctionTok{keras\_predict}\NormalTok{(final\_mod, kick\_matrix\_test, kickstarter\_test}\SpecialCharTok{$}\NormalTok{state)}
\NormalTok{final\_res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{metrics}\NormalTok{(state, .pred\_class, .pred\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 3
#>   .metric     .estimator .estimate
#>   <chr>       <chr>          <dbl>
#> 1 accuracy    binary         0.852
#> 2 kap         binary         0.704
#> 3 mn_log_loss binary         0.778
#> 4 roc_auc     binary         0.894
\end{verbatim}

This is our best performing model in this chapter on CNN models, although not by much. We can again create an ROC curve, this time using the test data in Figure \ref{fig:cnnfinalroc}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(state, .pred\_1) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/cnnfinalroc-1} 

}

\caption{ROC curve for final CNN model predictions on testing set of Kickstarter campaign success}\label{fig:cnnfinalroc}
\end{figure}

We have been able to incrementally improve our model by adding to the structure and making good choices about \index{preprocessing}preprocessing. We can visualize this final CNN model's performance using a \index{matrix!confusion}confusion matrix as well, in Figure \ref{fig:cnnheatmapfinal}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{final\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(state, .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{10_dl_cnn_files/figure-latex/cnnheatmapfinal-1} 

}

\caption{Confusion matrix for final CNN model predictions on testing set of Kickstarter campaign success}\label{fig:cnnheatmapfinal}
\end{figure}

Notice that this final model performs better then any of the models we have tried so far in this chapter, Chapter \ref{dldnn}, and Chapter \ref{dllstm}.

\begin{rmdnote}
For this particular data set of short text blurbs, a CNN model able to
learn local features performed the best, better than either a densely
connected neural network or an LSTM.
\end{rmdnote}

\hypertarget{dlcnnsummary}{%
\section{Summary}\label{dlcnnsummary}}

CNNs are a type of neural network that can learn local spatial patterns. They essentially perform feature extraction\index{feature engineering}, which can then be used efficiently in later layers of a network. Their simplicity and fast running time, compared to models like LSTMs, makes them excellent candidates for supervised models for text.

\hypertarget{in-this-chapter-you-learned-9}{%
\subsection{In this chapter, you learned:}\label{in-this-chapter-you-learned-9}}

\begin{itemize}
\item
  how to preprocess text data for CNN models
\item
  about CNN network architectures
\item
  how CNN layers can be stacked to extract patterns of varying detail
\item
  how byte pair encoding can be used to tokenize for finer detail
\item
  how to do hyperparameter search in Keras with \textbf{tfruns}
\item
  how to evaluate CNN models for text
\end{itemize}

\cleardoublepage

\hypertarget{part-conclusion}{%
\part{Conclusion}\label{part-conclusion}}

\hypertarget{text-models-in-the-real-world}{%
\chapter*{Text models in the real world}\label{text-models-in-the-real-world}}


Models affect real people in real ways. As the school year of 2020 began with many schools in the United States operating online only because of the novel coronavirus pandemic, a parent of a junior high student \href{https://twitter.com/DanaJSimmons/status/1300639757165191170}{reported} that her son was deeply upset and filled with doubt because of the way the algorithm of an ed tech company automatically scored\index{automated grading} his text answers. The parent and child discovered \citep{Verge2020} how to ``game'' the ed tech system's scoring.

\begin{quote}
Algorithm update. He cracked it: Two full sentences, followed by a word salad of all possibly applicable keywords. 100\% on every assignment. Students on @EdgenuityInc, there's your ticket. He went from an F to an A+ without learning a thing.
\end{quote}

We can't know the details of the proprietary modeling and/or heuristics that make up the ed tech system's scoring algorithm, but there is enough detail in this student's experience to draw some conclusions. We surmise that this is a count-based method or model, perhaps a linear one but not necessarily so. The success of ``word salad'' submissions indicates that the model or heuristic being applied has not learned that complex, or even correct, language is important for the score.

What could a team building this kind of score do to avoid these problems? It seems like ``word salad'' type submissions were not included in the training data as negative examples (i.e., with low scores), indicating that the training data was \emph{biased}; it did not reflect\index{bias} the full spectrum of submissions that the system sees in real life. The system (code and data) is not auditable for teachers or students, and the ed tech company does not directly have a process in place to handle appeals or mistakes in the score itself.

The particular ed tech company in this example does claim that these scores are used only to provide scoring guidance to teachers and that teachers can either accept or overrule such scores, but it is not clear how often teachers overrule scores. This highlights the foundational question about whether such a model or system should even be built to start with; with its current performance, this system is failing at what educators and students understand its goals to be, and is doing harm to its users.

This situation is more urgent and important than only a single example from the pandemic-stressed United States educational system, because:

\begin{itemize}
\item
  these types of harms exacerbate existing inequalities, and
\item
  these systems are becoming more and more widely used.
\end{itemize}

\citet{Ramineni2018} report how GRE essays by African-American students receive lower scores from automatic grading algorithms\index{automated grading} than from expert human graders, and explore statistical differences in the two grading approaches. This is a stark reminder that machine learning systems learn patterns from training data and amplify those patterns. \citet{Feathers2019} reports that the kind of automatic essay grading described here is used in at least 21 states, and essay grading is not the only kind of predictive text model that has real impact on real individuals' lives\footnote{For more, see this discussion from Rachel Thomas: \url{https://youtu.be/bqCEUQq0z4o}}.

As you finish this book and take away ideas on how to transform language to features for modeling and how to build reliable text models, we want to end by reflecting on how our work as data practitioners plays out when applied. Language data is richly human, and what you and we do with it matters.

\cleardoublepage

\hypertarget{appendix-appendices}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{regexp}{%
\chapter{Regular expressions}\label{regexp}}

\begin{quote}
Some people, when confronted with a problem, think: ``I know, I'll use regular expressions.'' Now they have two problems.\\
\hfill --- \href{https://en.wikiquote.org/wiki/Jamie_Zawinski}{Jamie Zawinski}
\end{quote}

This section will give a brief overview on how to write and use a regular expression), often abbreviated \emph{regex}. Regular expressions are a way to specify or search for patterns of strings using a sequence of characters. By combining a selection of simple patterns, we can capture quite complicated strings.

Many functions in R take advantage of regular expressions. Some examples from base R include \texttt{grep}, \texttt{grepl}, \texttt{regexpr}, \texttt{gregexpr}, \texttt{sub}, \texttt{gsub}, and \texttt{strsplit}, as well as \texttt{ls} and \texttt{list.files}. The \textbf{stringr} package \citep{Wickham19} uses regular expressions extensively; the regular expressions are passed as the \texttt{pattern\ =} argument. Regular expressions can be used to detect, locate, or extract parts of a string.

\hypertarget{literal-characters}{%
\section{Literal characters}\label{literal-characters}}

The most basic regular expression consists of only a single character. Here let's detect if each of the following strings in the character vector \texttt{animals} contains the letter ``j''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stringr)}

\NormalTok{animals }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"jaguar"}\NormalTok{, }\StringTok{"jay"}\NormalTok{, }\StringTok{"bat"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(animals, }\StringTok{"j"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE  TRUE FALSE
\end{verbatim}

We are also able to \emph{extract} the match with \texttt{str\_extract}. This may not seem too useful right now, but it becomes very helpful once we use more advanced regular expressions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract}\NormalTok{(animals, }\StringTok{"j"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "j" "j" NA
\end{verbatim}

Lastly we are able to \emph{locate} the position of a match using \texttt{str\_locate}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_locate}\NormalTok{(animals, }\StringTok{"j"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      start end
#> [1,]     1   1
#> [2,]     1   1
#> [3,]    NA  NA
\end{verbatim}

\begin{rmdnote}
The functions \texttt{str\_detect}, \texttt{str\_extract}, and
\texttt{str\_locate} are some of the most simple and powerful main
functions in \textbf{stringr}, but the \textbf{stringr} package includes
many more functions. To see the remaining functions, run
\texttt{help(package\ =\ "stringr")} to open the documentation.
\end{rmdnote}

We can also match multiple characters in a row.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{animals }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"jaguar"}\NormalTok{, }\StringTok{"jay"}\NormalTok{, }\StringTok{"bat"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(animals, }\StringTok{"jag"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE FALSE FALSE
\end{verbatim}

Notice how these characters are case sensitive.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wows }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"wow"}\NormalTok{, }\StringTok{"WoW"}\NormalTok{, }\StringTok{"WOW"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(wows, }\StringTok{"wow"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE FALSE FALSE
\end{verbatim}

\hypertarget{meta-characters}{%
\subsection{Meta characters}\label{meta-characters}}

There are 14 meta characters that carry special meaning inside regular expressions. We need to ``escape'' them with a backslash if we want to match the literal character (and backslashes need to be doubled in R). Think of ``escaping'' as stripping the character of its special meaning.

The plus symbol \texttt{+} is one of the special meta characters for regular expressions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{math }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"1 + 2"}\NormalTok{, }\StringTok{"14 + 5"}\NormalTok{, }\StringTok{"3 {-} 5"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(math, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{+"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE  TRUE FALSE
\end{verbatim}

If we tried to use the plus sign without escaping it, like \texttt{"+"}, we would get an error and this line of code would not run.

The complete list of meta characters is displayed in Table \ref{tab:metacharacters} \citep{levithan2012regular}.

\begin{table}

\caption{\label{tab:metacharacters}All meta characters}
\centering
\begin{tabular}[t]{ll}
\toprule
Description & Character\\
\midrule
opening square bracket & [\\
closing square bracket & ]\\
backslash & \textbackslash{}\textbackslash{}\\
caret & \textasciicircum{}\\
dollar sign & \$\\
\addlinespace
period/dot & .\\
vertical bar & |\\
question mark & ?\\
asterisk & *\\
plus sign & +\\
\addlinespace
opening curly brackets & \{\\
closing curly brackets & \}\\
opening parentheses & (\\
closing parentheses & )\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{full-stop-the-wildcard}{%
\section{Full stop, the wildcard}\label{full-stop-the-wildcard}}

Let's start with the full stop/period/dot, which acts as a ``wildcard.'' This means that this character will match anything in place other then a newline character.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strings }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"cat"}\NormalTok{, }\StringTok{"cut"}\NormalTok{, }\StringTok{"cue"}\NormalTok{)}
\FunctionTok{str\_extract}\NormalTok{(strings, }\StringTok{"c."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "ca" "cu" "cu"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract}\NormalTok{(strings, }\StringTok{"c.t"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "cat" "cut" NA
\end{verbatim}

\hypertarget{character-classes}{%
\section{Character classes}\label{character-classes}}

So far we have only been able to match either exact characters or wildcards. \textbf{Character classes} (also called character sets) let us do more than that. A character class allows us to match a character specified inside the class. A character class is constructed with square brackets. The character class \texttt{{[}ac{]}} will match \emph{either} an ``a'' or a ``c''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strings }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(strings, }\StringTok{"[ac]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE FALSE  TRUE
\end{verbatim}

\begin{rmdnote}
Spaces inside character classes are meaningful as they are interpreted
as literal characters. Thus the character class \texttt{"{[}ac{]}"} will
match the letter ``a'' and ``c'', while the character class
\texttt{"{[}a\ c{]}"} will match the letters ``a'' and ``c'' but also a
space.
\end{rmdnote}

We can use a hyphen character to define a range of characters. Thus \texttt{{[}1-5{]}} is the same as \texttt{{[}12345{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numbers }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{, }\StringTok{"4"}\NormalTok{, }\StringTok{"5"}\NormalTok{, }\StringTok{"6"}\NormalTok{, }\StringTok{"7"}\NormalTok{, }\StringTok{"8"}\NormalTok{, }\StringTok{"9"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(numbers, }\StringTok{"[2{-}7]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentence }\OtherTok{\textless{}{-}} \StringTok{"This is a long sentence with 2 numbers with 1 digits."}
\FunctionTok{str\_locate\_all}\NormalTok{(sentence, }\StringTok{"[1{-}2a{-}b]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#>      start end
#> [1,]     9   9
#> [2,]    30  30
#> [3,]    35  35
#> [4,]    45  45
\end{verbatim}

We can also negate characters in a class with a caret \texttt{\^{}}. Placing a caret immediately inside the opening square bracket will make the regular expression match anything \emph{not} inside the class. Thus the regular expression \texttt{{[}\^{}ac{]}} will match anything that isn't the letter ``a'' or ``c''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strings }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(strings, }\StringTok{"[\^{}ac]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE  TRUE FALSE
\end{verbatim}

\hypertarget{shorthand-character-classes}{%
\subsection{Shorthand character classes}\label{shorthand-character-classes}}

Certain character classes are so commonly used that they have been predefined with names. A couple of these character classes have even shorter shorthands. The class \texttt{{[}:digit:{]}} denotes all the digits 0, 1, 2, 3, 4, 5, 6, 7, 8 and 9 but it can also be described by \texttt{\textbackslash{}\textbackslash{}d}. Table \ref{tab:characterclasses} presents these useful predefined character classes.

\begin{table}

\caption{\label{tab:characterclasses}All character classes}
\centering
\begin{tabular}[t]{ll}
\toprule
Description & Class\\
\midrule
Digits; [0-9] & [:digit:] or \textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}d\\
Alphabetic characters, uppercase and lowercase [A-z] & [:alpha:]\\
Alphanumeric characters, letters, and digits [A-z0-9] & [:alnum:]\\
Graphical characters [[:alnum:][:punct:]] & [:graph:]\\
Printable characters [[:alnum:][:punct:][:space:]] & [:print:]\\
\addlinespace
Lowercase letters [a-z] & [:lower:]\\
Uppercase letters [A-Z] & [:upper:]\\
Control characters such as newline, carriage return, etc. & [:cntrl:]\\
Punctuation characters: !"\#\$\%\&’()*+,-./:;<=>?@[]\textasciicircum{}\_`\{|\}\textasciitilde{} & [:punct:]\\
Space and tab & [:blank:]\\
\addlinespace
Space, tab, vertical tab, newline, form feed, carriage return & [:space:] or \textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}s\\
Hexadecimal digits [0-9A-Fa-f] & [:xdigit:]\\
Not space [\textasciicircum{}[:space:]] & \textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}S\\
Word characters:  letters, digits, and underscores [A-z0-9\_] & \textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}w\\
Non-word characters [\textasciicircum{}A-z0-9\_] & \textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}W\\
\addlinespace
Non-digits [\textasciicircum{}0-9] & \textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}D\\
\bottomrule
\end{tabular}
\end{table}

Notice that these short-hands are locale specific. This means that the danish character ø will be picked up in class \texttt{{[}:lower:{]}} but not in the class \texttt{{[}a-z{]}} as the character isn't located between a and z.

\hypertarget{quantifiers}{%
\section{Quantifiers}\label{quantifiers}}

We can specify how many times we expect something to occur using quantifiers. If we want to find a digit with four numerals, we don't have to write \texttt{{[}:digit:{]}{[}:digit:{]}{[}:digit:{]}{[}:digit:{]}}. Table \ref{tab:greedyquantifiers} shows how to specify repetitions. Notice that \texttt{?} is shorthand for \texttt{\{0,1\}}, \texttt{*} is shorthand for \texttt{\{0,\}} and \texttt{+} is shorthand for \texttt{\{1,\}} \citep{levithan2012regular}.

\begin{table}

\caption{\label{tab:greedyquantifiers}Regular expression quantifiers}
\centering
\begin{tabular}[t]{ll}
\toprule
Regex & Matches\\
\midrule
? & zero or one times\\
* & zero or more times\\
+ & one or more times\\
\{n\} & exactly n times\\
\{n,\} & at least n times\\
\addlinespace
\{n,m\} & between n and m times\\
\bottomrule
\end{tabular}
\end{table}

We can detect both color and colour by placing a quantifier after the ``u'' that detects 0 or 1 times used.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"colour"}\NormalTok{, }\StringTok{"color"}\NormalTok{, }\StringTok{"farver"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(col, }\StringTok{"colou?r"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE  TRUE FALSE
\end{verbatim}

And we can extract four-digit numbers using \texttt{\{4\}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentences }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"The year was 1776."}\NormalTok{, }\StringTok{"Alexander Hamilton died at 47."}\NormalTok{)}
\FunctionTok{str\_extract}\NormalTok{(sentences, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d\{4\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "1776" NA
\end{verbatim}

Sometimes we want the repetition to happen over multiple characters. This can be achieved by wrapping what we want repeated in parentheses. In the following example, we want to match all the instances of ``NA'' in the string. We put \texttt{"NA\ "} inside a set of parentheses and putting \texttt{+} after it to make sure we match at least once.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{batman }\OtherTok{\textless{}{-}} \StringTok{"NA NA NA NA NA NA NA NA NA NA NA NA NA NA BATMAN!!!"}
\FunctionTok{str\_extract}\NormalTok{(batman, }\StringTok{"(NA )+"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "NA NA NA NA NA NA NA NA NA NA NA NA NA NA "
\end{verbatim}

However, notice that this also matches the last space, which we don't want. We can fix this by matching zero or more ``NA'' followed by exactly 1 ``NA''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{batman }\OtherTok{\textless{}{-}} \StringTok{"NA NA NA NA NA NA NA NA NA NA NA NA NA NA BATMAN!!!"}
\FunctionTok{str\_extract}\NormalTok{(batman, }\StringTok{"(NA )*(NA)\{1\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "NA NA NA NA NA NA NA NA NA NA NA NA NA NA"
\end{verbatim}

By default these matches are ``greedy'', meaning that they will try to match the longest string possible. We can instead make them ``lazy'' by placing a \texttt{?} after, as shown in Table \ref{tab:lazyquantifiers}. This will make the regular expressions try to match the shortest string possible instead of the longest.

\begin{table}

\caption{\label{tab:lazyquantifiers}Lazy quantifiers}
\centering
\begin{tabular}[t]{ll}
\toprule
regex & matches\\
\midrule
?? & zero or one times, prefers 0\\
*? & zero or more times, match as few times as possible\\
+? & one or more times, match as few times as possible\\
\{n\}? & exactly n times, match as few times as possible\\
\{n,\}? & at least n times, match as few times as possible\\
\addlinespace
\{n,m\}? & between n and m times, match as few times as possible but at least n\\
\bottomrule
\end{tabular}
\end{table}

Comparing greedy and lazy matches gives us 3 and 7 ``NA'''s respectively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{batman }\OtherTok{\textless{}{-}} \StringTok{"NA NA NA NA NA NA NA NA NA NA NA NA NA NA BATMAN!!!"}
\FunctionTok{str\_extract}\NormalTok{(batman, }\StringTok{"(NA )\{3,7\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "NA NA NA NA NA NA NA "
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_extract}\NormalTok{(batman, }\StringTok{"(NA )\{3,7\}?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "NA NA NA "
\end{verbatim}

\hypertarget{anchors}{%
\section{Anchors}\label{anchors}}

The meta characters \texttt{\^{}} and \texttt{\$} have special meaning in regular expressions. They force the engine to check the beginning and end of the string respectively, hence the name \textbf{anchor}. A mnemonic device to remember this is ``First you get the power(\texttt{\^{}}) and the you get the money(\texttt{\textbackslash{}\$})''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seasons }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"The summer is hot this year"}\NormalTok{,}
             \StringTok{"The spring is a lovely time"}\NormalTok{,}
             \StringTok{"Winter is my favorite time of the year"}\NormalTok{,}
             \StringTok{"Fall is a time of peace"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(seasons, }\StringTok{"\^{}The"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE  TRUE FALSE FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str\_detect}\NormalTok{(seasons, }\StringTok{"year$"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1]  TRUE FALSE  TRUE FALSE
\end{verbatim}

We can also combine the two to match a string completely.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"analysis"}\NormalTok{, }\StringTok{"data{-}raw"}\NormalTok{, }\StringTok{"data"}\NormalTok{, }\StringTok{"R"}\NormalTok{)}
\FunctionTok{str\_detect}\NormalTok{(folder\_names, }\StringTok{"\^{}data$"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] FALSE FALSE  TRUE FALSE
\end{verbatim}

\hypertarget{additional-resources}{%
\section{Additional resources}\label{additional-resources}}

This appendix covered some of the basics of getting started with (or refreshed about) regular expressions. If you want to learn more:

\begin{itemize}
\item
  RStudio maintains \href{https://www.rstudio.com/resources/cheatsheets/}{an excellent collection of cheat sheets}, some of which are related to regular expressions.
\item
  www.rexegg.com has many pages of valuable information, including \href{https://www.rexegg.com/regex-quickstart.html}{this ``quick start'' page with helpful tables}.
\item
  \url{https://www.regular-expressions.info/} is another great general regular expression site.
\item
  The \href{https://r4ds.had.co.nz/strings.html}{strings chapter} in \emph{R for Data Science} \citep{Wickham2017} delves into examples written in R.
\item
  Lastly if you want to go down to the metal, check out \href{http://shop.oreilly.com/product/9780596528126.do}{\emph{Mastering Regular Expressions}}.
\end{itemize}

\hypertarget{appendixdata}{%
\chapter{Data}\label{appendixdata}}

There are four main text data sets we use throughout this book to demonstrate building features for machine learning and training models. These data sets include texts of different languages, different lengths (short to long), and from very recent time periods to a few hundred years ago.

These text data sets are not overly difficult to read into memory and prepare for analysis; by contrast, in many text modeling projects, the data itself may be in any of a number of formats from an API to literal paper. Practitioners may need to use skills such as web scraping or connecting to databases to even begin their work.

\hypertarget{hcandersen}{%
\section{Hans Christian Andersen fairy tales}\label{hcandersen}}

The \textbf{hcandersenr} \citep{R-hcandersenr} package includes the text of the 157 known fairy tales by the Danish author Hans Christian Andersen (1805 - 1875).
There are five different languages available, with:

\begin{itemize}
\item
  156 fairy tales in English,
\item
  154 in Spanish,
\item
  150 in German,
\item
  138 in Danish, and
\item
  58 in French.
\end{itemize}

The package contains a data set for each language with the naming convention \texttt{hcandersen\_**},
where \texttt{**} is a country code.
Each data set comes as a dataframe with two columns, \texttt{text} and \texttt{book} where the \texttt{book} variable has the text divided into strings of up to 80 characters.

The package also makes available a data set called \texttt{EK} which includes information about the publication date, language of origin, and names in the different languages.

This data set is used in Chapters \ref{tokenization}, \ref{stopwords}, and \ref{stemming}.

\hypertarget{scotus-opinions}{%
\section{Opinions of the Supreme Court of the United States}\label{scotus-opinions}}

The \textbf{scotus} \citep{R-scotus} package contains a sample of the Supreme Court of the United States' opinions.
The \texttt{scotus\_sample} dataframe includes one opinion per row along with the year, case name, docket number, and a unique ID number.

The text has had minimal preprocessing and includes header information in the text field, such as shown here:

\begin{verbatim}
#> No. 97-1992
#> VAUGHN L. MURPHY, Petitioner v. UNITED PARCEL SERVICE, INC.
#> ON WRIT OF CERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE TENTH
#> CIRCUIT
#> [June 22, 1999]
#> Justice O'Connor delivered the opinion of the Court.
#> Respondent United Parcel Service, Inc. (UPS), dismissed petitioner Vaughn
#> L. Murphy from his job as a UPS mechanic because of his high blood pressure.
#> Petitioner filed suit under Title I of the Americans with Disabilities Act of
#> 1990 (ADA or Act), 104 Stat. 328, 42 U.S.C. § 12101 et seq., in Federal District
#> Court. The District Court granted summary judgment to respondent, and the Court
#> of Appeals for the Tenth Circuit affirmed. We must decide whether the Court
#> of Appeals correctly considered petitioner in his medicated state when it held
#> that petitioner's impairment does not "substantially limi[t]" one or more of
#> his major life activities and whether it correctly determined that petitioner
#> is not "regarded as disabled." See §12102(2). In light of our decision in Sutton
#> v. United Air Lines, Inc., ante, p. ____, we conclude that the Court of Appeals'
#> resolution of both issues was correct.
\end{verbatim}

This data set is used in Chapters \ref{stemming}, \ref{mlregression}, and \ref{dllstm}.

\hypertarget{cfpb-complaints}{%
\section{Consumer Financial Protection Bureau (CFPB) complaints}\label{cfpb-complaints}}

Consumers can submit complaints to the \href{https://www.consumerfinance.gov/data-research/consumer-complaints/}{United States Consumer Financial Protection Bureau (CFPB)} about financial products and services; the CFPB sends the complaints to companies for response.

The data set of consumer complaints used in this book has been filtered to 117,214 complaints submitted to the CFPB after 1 January 2019 that include a consumer complaint narrative (i.e., some submitted text). Each observation has a \texttt{complaint\_id}, various categorical variables, and a text column \texttt{consumer\_complaint\_narrative} containing the written complaints, for a total of 18 columns.

This data set is used in Chapters \ref{embeddings} and \ref{mlclassification}.

\hypertarget{kickstarter-blurbs}{%
\section{Kickstarter campaign blurbs}\label{kickstarter-blurbs}}

The crowdfunding site \href{https://www.kickstarter.com/}{Kickstarter} provides people a platform to gather pledges to ``back'' their projects, such as films, music, comics, journalism, and more. When setting up a campaign, project owners submit a description or ``blurb'' for their campaign to tell potential backers what it is about. The data set of campaign blurbs used in this book \href{https://webrobots.io/kickstarter-datasets/}{was scraped from Kickstarter}; the blurbs used here for modeling are from 2009-04-21 to 2016-03-14, with a total of 269,790 campaigns in the sample. For each campaign, we know its \texttt{state}, whether it was successful in meeting its crowdfunding goal or not.

This data set is used in Chapters \ref{dldnn}, \ref{dllstm}, and \ref{dlcnn}.

\hypertarget{appendixbaseline}{%
\chapter{Baseline linear classifier}\label{appendixbaseline}}

In Chapters \ref{dldnn}, \ref{dllstm}, and \ref{dlcnn} we demonstrate in detail how to train and evaluate different kinds of deep learning classifiers for the Kickstarter data set of campaign blurbs and whether each campaign was successful or not. This Appendix shows a baseline linear classification model for this data set using machine learning techniques like those used in Chapters \ref{mlregression} and \ref{mlclassification}. It serves the purpose of comparison with the deep learning techniques, and also as a succinct summary of a basic supervised machine learning analysis for text.

This machine learning analysis is presented with only minimal narrative; see Chapters \ref{mlregression} and \ref{mlclassification} for more explanation and details.

\hypertarget{read-in-the-data}{%
\section{Read in the data}\label{read-in-the-data}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{kickstarter }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/kickstarter.csv.gz"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{state =} \FunctionTok{as.factor}\NormalTok{(state))}

\NormalTok{kickstarter}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 269,790 x 3
#>    blurb                                                        state created_at
#>    <chr>                                                        <fct> <date>    
#>  1 Exploring paint and its place in a digital world.            0     2015-03-17
#>  2 Mike Fassio wants a side-by-side photo of me and Hazel eati~ 0     2014-07-11
#>  3 I need your help to get a nice graphics tablet and Photosho~ 0     2014-07-30
#>  4 I want to create a Nature Photograph Series of photos of wi~ 0     2015-05-08
#>  5 I want to bring colour to the world in my own artistic skil~ 0     2015-02-01
#>  6 We start from some lovely pictures made by us and we decide~ 0     2015-11-18
#>  7 Help me raise money to get a drawing tablet                  0     2015-04-03
#>  8 I would like to share my art with the world and to do that ~ 0     2014-10-15
#>  9 Post Card don’t set out to simply decorate stories. Our goa~ 0     2015-06-25
#> 10 My name is Siu Lon Liu and I am an illustrator seeking fund~ 0     2014-07-19
#> # ... with 269,780 more rows
\end{verbatim}

\hypertarget{split-into-testtrain-and-create-resampling-folds}{%
\section{Split into test/train and create resampling folds}\label{split-into-testtrain-and-create-resampling-folds}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{kickstarter\_split }\OtherTok{\textless{}{-}}\NormalTok{ kickstarter }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{nchar}\NormalTok{(blurb) }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{initial\_split}\NormalTok{()}

\NormalTok{kickstarter\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(kickstarter\_split)}
\NormalTok{kickstarter\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(kickstarter\_split)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{kickstarter\_folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(kickstarter\_train)}
\NormalTok{kickstarter\_folds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> #  10-fold cross-validation 
#> # A tibble: 10 x 2
#>    splits                 id    
#>    <list>                 <chr> 
#>  1 <split [181883/20210]> Fold01
#>  2 <split [181883/20210]> Fold02
#>  3 <split [181883/20210]> Fold03
#>  4 <split [181884/20209]> Fold04
#>  5 <split [181884/20209]> Fold05
#>  6 <split [181884/20209]> Fold06
#>  7 <split [181884/20209]> Fold07
#>  8 <split [181884/20209]> Fold08
#>  9 <split [181884/20209]> Fold09
#> 10 <split [181884/20209]> Fold10
\end{verbatim}

\hypertarget{recipe-for-data-preprocessing}{%
\section{Recipe for data preprocessing}\label{recipe-for-data-preprocessing}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(textrecipes)}

\NormalTok{kickstarter\_rec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(state }\SpecialCharTok{\textasciitilde{}}\NormalTok{ blurb, }\AttributeTok{data =}\NormalTok{ kickstarter\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenize}\NormalTok{(blurb) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tokenfilter}\NormalTok{(blurb, }\AttributeTok{max\_tokens =} \FloatTok{5e3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_tfidf}\NormalTok{(blurb)}

\NormalTok{kickstarter\_rec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Data Recipe
#> 
#> Inputs:
#> 
#>       role #variables
#>    outcome          1
#>  predictor          1
#> 
#> Operations:
#> 
#> Tokenization for blurb
#> Text filtering for blurb
#> Term frequency-inverse document frequency with blurb
\end{verbatim}

\hypertarget{lasso-regularized-classification-model}{%
\section{Lasso regularized classification model}\label{lasso-regularized-classification-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso\_spec }\OtherTok{\textless{}{-}} \FunctionTok{logistic\_reg}\NormalTok{(}\AttributeTok{penalty =} \FunctionTok{tune}\NormalTok{(), }\AttributeTok{mixture =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glmnet"}\NormalTok{)}

\NormalTok{lasso\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

\hypertarget{a-model-workflow}{%
\section{A model workflow}\label{a-model-workflow}}

We need a few more components before we can tune our workflow. Let's use a sparse data encoding (Section \ref{casestudysparseencoding}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(hardhat)}
\NormalTok{sparse\_bp }\OtherTok{\textless{}{-}} \FunctionTok{default\_recipe\_blueprint}\NormalTok{(}\AttributeTok{composition =} \StringTok{"dgCMatrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's create a grid of possible regularization penalties to try.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda\_grid }\OtherTok{\textless{}{-}} \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{penalty}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{)), }\AttributeTok{levels =} \DecValTok{20}\NormalTok{)}
\NormalTok{lambda\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 20 x 1
#>      penalty
#>        <dbl>
#>  1 0.00001  
#>  2 0.0000183
#>  3 0.0000336
#>  4 0.0000616
#>  5 0.000113 
#>  6 0.000207 
#>  7 0.000379 
#>  8 0.000695 
#>  9 0.00127  
#> 10 0.00234  
#> 11 0.00428  
#> 12 0.00785  
#> 13 0.0144   
#> 14 0.0264   
#> 15 0.0483   
#> 16 0.0886   
#> 17 0.162    
#> 18 0.298    
#> 19 0.546    
#> 20 1
\end{verbatim}

Now these can be combined in a tuneable \texttt{workflow()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kickstarter\_wf }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(kickstarter\_rec, }\AttributeTok{blueprint =}\NormalTok{ sparse\_bp) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(lasso\_spec)}

\NormalTok{kickstarter\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> == Workflow ====================================================================
#> Preprocessor: Recipe
#> Model: logistic_reg()
#> 
#> -- Preprocessor ----------------------------------------------------------------
#> 3 Recipe Steps
#> 
#> * step_tokenize()
#> * step_tokenfilter()
#> * step_tfidf()
#> 
#> -- Model -----------------------------------------------------------------------
#> Logistic Regression Model Specification (classification)
#> 
#> Main Arguments:
#>   penalty = tune()
#>   mixture = 1
#> 
#> Computational engine: glmnet
\end{verbatim}

\hypertarget{tune-the-workflow}{%
\section{Tune the workflow}\label{tune-the-workflow}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2020}\NormalTok{)}
\NormalTok{lasso\_rs }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{  kickstarter\_wf,}
\NormalTok{  kickstarter\_folds,}
  \AttributeTok{grid =}\NormalTok{ lambda\_grid}
\NormalTok{)}

\NormalTok{lasso\_rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Tuning results
#> # 10-fold cross-validation 
#> # A tibble: 10 x 4
#>    splits                 id     .metrics              .notes              
#>    <list>                 <chr>  <list>                <list>              
#>  1 <split [181883/20210]> Fold01 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  2 <split [181883/20210]> Fold02 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  3 <split [181883/20210]> Fold03 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  4 <split [181884/20209]> Fold04 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  5 <split [181884/20209]> Fold05 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  6 <split [181884/20209]> Fold06 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  7 <split [181884/20209]> Fold07 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  8 <split [181884/20209]> Fold08 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#>  9 <split [181884/20209]> Fold09 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
#> 10 <split [181884/20209]> Fold10 <tibble[,5] [40 x 5]> <tibble[,1] [0 x 1]>
\end{verbatim}

What are the best models?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{show\_best}\NormalTok{(lasso\_rs, }\StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 5 x 7
#>     penalty .metric .estimator  mean     n  std_err .config              
#>       <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>                
#> 1 0.000695  roc_auc binary     0.752    10 0.000856 Preprocessor1_Model08
#> 2 0.000379  roc_auc binary     0.752    10 0.000889 Preprocessor1_Model07
#> 3 0.000207  roc_auc binary     0.751    10 0.000903 Preprocessor1_Model06
#> 4 0.000113  roc_auc binary     0.751    10 0.000914 Preprocessor1_Model05
#> 5 0.0000616 roc_auc binary     0.751    10 0.000920 Preprocessor1_Model04
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{show\_best}\NormalTok{(lasso\_rs, }\StringTok{"accuracy"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 5 x 7
#>     penalty .metric  .estimator  mean     n  std_err .config              
#>       <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>                
#> 1 0.000379  accuracy binary     0.684    10 0.000764 Preprocessor1_Model07
#> 2 0.000695  accuracy binary     0.684    10 0.000732 Preprocessor1_Model08
#> 3 0.000207  accuracy binary     0.684    10 0.000905 Preprocessor1_Model06
#> 4 0.0000616 accuracy binary     0.684    10 0.000909 Preprocessor1_Model04
#> 5 0.000113  accuracy binary     0.684    10 0.000879 Preprocessor1_Model05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autoplot}\NormalTok{(lasso\_rs)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{14_baseline_classifier_files/figure-latex/unnamed-chunk-10-1} \end{center}

  \bibliography{book.bib}

\backmatter
\printindex

\end{document}
