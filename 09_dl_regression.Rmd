# Regression {#dlregression}

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE, cache = FALSE, eval = TRUE,
               tidy = "styler", fig.width = 8, fig.height = 5)
suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_light())
options(crayon.enabled = FALSE)
doParallel::registerDoParallel()

## for GH actions
online <- TRUE
``` 

In this chapter, we will predict continuous values, much like we did in Chapter \@ref(mlregression), but we will use deep learning methods instead of methods such as naive Bayes or regularized linear regression. Let's consider a dataset of press releases from the United States Department of Justice (DOJ), which [they make available on their website](https://www.justice.gov/news).

```{r doj}
library(tidyverse)

doj_press <- read_csv("data/press_releases.csv.gz")
doj_press
```

We know the `date` that each of these press releases was published, and predicting this date from other characteristics of the press releases, such as the `contents` of the press release, is a regression problem.

```{r dojhist, dependson="doj", fig.cap="Distribution of Department of Justice press releases over time"}
library(lubridate)

doj_press %>%
  count(month = floor_date(date, unit = "months"), name = "releases") %>%
  ggplot(aes(month, releases)) +
  geom_area(alpha = 0.8) +
  geom_smooth() +
  labs(x = NULL, y = "Releases per month")
```

This dataset includes all press releases from the DOJ from the beginning of 2009 through July 2018. There is some month-to-month variation and an overall increase in releases, but there is good coverage over the time frame for which we would like to build a model.

There are `r n_distinct(doj_press$agency)` distinct main agencies associated with these releases, but some press releases have no agency associated with them. A few agencies, such as the Criminal Division, Civil Right Division, and Tax Division, account for many more press releases than other agencies.

```{r agencycounts, dependson="doj", fig.cap="Main agency associated with Department of Justice press releases"}
doj_press %>%
  count(agency) %>%
  slice_max(n, n = 10) %>%
  ggplot(aes(n, fct_reorder(agency, n))) +
  geom_col() +
  labs(x = "Number of press releases", y = NULL)
```

```{block, type = "rmdnote"}
The DOJ press releases are relatively _long_ documents; we will take this into consideration as we build neural network architectures for modeling.
```

```{r dojlength, dependson="doj", fig.cap="Distribution of word count for Department of Justice press releases"}
library(tidytext)
doj_press %>%
  unnest_tokens(word, contents) %>%
  count(title) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 25, alpha = 0.8) +
  scale_x_log10(labels = scales::comma_format()) +
  labs(x = "Number of words per press release",
       y = "Number of press releases")
```

Compared to the documents we built deep learning models for in Chapter \@ref(dlclassification), these press releases are long, with a median character count of `r scales::comma(median(nchar(doj_press$contents), na.rm = TRUE))` for the `contents` of the press releases. We can use deep learning models to model these longer sequences.

Some examples, such as this press release from the end of 2016, are quite short:

> Deputy Attorney General Sally Q. Yates released the following statement after President Obama granted commutation of sentence to 153 individuals: "Today, another 153 individuals were granted commutations by the President.  Over the last eight years, President Obama has given a second chance to over 1,100 inmates who have paid their debt to society.  Our work is ongoing and we look forward to additional announcements from the President before the end of his term."


## A first regression model {#firstdlregression}

```{block, type = "rmdnote"}
As we walk through building a deep learning model, notice which steps are different and which steps are the same now that we use a neural network architecture.
```

Much like all our previous modeling, our first step is to split our data into training and testing sets. We will still use our training set to build models and save the testing set for a final estimate of how our model will perform on new data. It is very easy to overfit deep learning models, so an unbiased estimate of future performance from a test set is more important than ever.

We use `initial_split()` to define the training/testing split, after removing examples that have a `title` but no `contents` in the press release. We will focus mainly on modeling the `contents` in this chapter, although the title is also text that could be handled in a deep learning model. Almost all of the press releases have character counts between 500 and 50,000, but let's exclude the ones that don't because they will represent a challenge for the preprocessing required for deep learning models.

```{r dojsplit, dependson="doj"}
library(tidymodels)
library(lubridate)
set.seed(1234)
doj_split <- doj_press %>%
  filter(!is.na(contents),
         nchar(contents) > 5e2, nchar(contents) < 5e4) %>%
  mutate(date = (as.numeric(date) - 1.6e4) / 1e3) %>%  ## can convert back with origin = "1970-01-01"
  initial_split(strata = date)

doj_train <- training(doj_split)
doj_test <- testing(doj_split)
```

```{block, type = "rmdnote"}
We converted the `date` variable to its underlying numeric representation so we can more easily train any kind of regression model we want. To go from an object that has R's date type to a numeric, use `as.numeric(date)`. To convert back from this numeric representation to a date, use `as.Date(date, origin = "1970-01-01")`. That special date is the "origin" (like zero) for the numbering system used by R's date types.
```

There are `r scales::comma(nrow(doj_train))` press releases in the training set and `r scales::comma(nrow(doj_test))` in the testing set.

```{block, type = "rmdwarning"}
Notice that we also shifted (subtracted) and scaled (divided) the `date` outcome by constant factors so all the values are close to one and centered around zero. Neural networks for regression problems typically behave better when dealing with outcomes that are roughly between -1 and 1.
```

### Preprocessing for deep learning

The preprocessing needed for deep learning network architectures is somewhat different than for the models we used in Chapters \@ref(mlclassification) and \@ref(mlregression). The first step is still to tokenize the text, as described in Chapter \@ref(tokenization). After we tokenize, we put a filter on how many words we'll keep in the analysis; `step_tokenfilter()` keeps the top tokens based on frequency in this dataset.

```{r dojrec, dependson="dojsplit"}
library(textrecipes)

max_words <- 5e3
max_length <- 500

doj_rec <- recipe(~ contents, data = doj_train) %>%
  step_tokenize(contents) %>%
  step_tokenfilter(contents, max_tokens = max_words) %>%
  step_sequence_onehot(contents, sequence_length = max_length)

doj_rec
```

After tokenizing, the preprocessing is different. We use `step_sequence_onehot()` to encode the sequences of words with integers representing each token in the vocabulary of `r scales::comma(max_words)` words. This is different than the representations we used in Chapters \@ref(mlclassification) and \@ref(mlregression), mainly because all the information about word sequence is encoded in this representation.

```{block, type = "rmdwarning"}
Using `step_sequence_onehot()` to preprocess text data records and encodes _sequence_ information, unlike the document-term matrix and/or bag-of-tokens approaches we used in Chapters \@ref(mlclassification) and \@ref(mlregression).
```

The DOJ press releases have a wide spread in document length, and we have to make a decision about how long of a sequence to include in our preprocessing. 

- If we choose the longest document, all the shorter documents will be "padded" with zeroes indicating no words or tokens in those empty spaces and our feature space will grow very large. 
- If we choose the shortest document as our sequence length, our feature space will be more manageable but all the longer documents will get cut off and we won't include any of that information in our model. 

In a situation like this, it can often work well to choose a medium sequence length, like `r max_length` words for this specific dataset, that involves truncating the longest documents and padding the shortest documents. This value corresponds to about the median document length in this collection of press releases.

In previous chapters, we used a preprocessing recipe like `doj_rec` in a tidymodels workflow but for our neural network models, we don't have that option. We need to be able to work with the keras modeling functions directly because of the flexible options needed to build many neural network architectures. We need to execute our preprocessing recipe, using first `prep()` and then `bake()`. 

```{block, type = "rmdwarning"}
When we `prep()` a recipe, we compute or estimate statistics from the training set; the output of `prep()` is a recipe. When we `bake()` a recipe, we apply the preprocessing to a dataset, either the training set that we started with or another set like the testing data or new data. The output of `bake()` is a dataset like a tibble or a matrix.
```

We could have applied these functions to any preprocessing recipes in previous chapters, but we didn't need to because our modeling workflows automated these steps.

```{r dojmatrix, dependson="dojrec"}
doj_prep <- prep(doj_rec)
doj_matrix <- bake(doj_prep, new_data = NULL, composition = "matrix")

dim(doj_matrix)
```

Here we use `composition = "matrix"` because the keras modeling functions operate on matrices, rather than a dataframe or tibble.


### Recurrent neural network

A recurrent neural network (RNN) is a specific kind of network architecture with feedback loops that allow information to persist through steps^[Vanilla neural networks do not have this ability for information to persist at all; they start learning from scratch at every step.]. RNNs are well-suited for text because of this ability to view text as a sequence of words or characters, and can model structures within text like word dependencies. RNNs are also used in domains like speech recognition. 

The keras library has convenient functions for broadly used architectures like RNNs so we don't have to build it from scratch from layers; we can instead use `layer_simple_rnn()`. This comes _after_ an embedding layer that makes dense vectors from our word sequences and _before_ a densely-connected layer for output.

```{r rnnmod}
library(keras)

rnn_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  layer_simple_rnn(units = 64) %>%
  layer_dense(units = 1)

rnn_mod
```

Because we are training a regression model, there is no activation function for the last layer; we want to fit and predict to arbitrary values for this numeric representation of date.

Next we `compile()` the model, which configures the model for training with a specific optimizer and set of metrics. 

```{block, type = "rmdnote"}
A good default optimizer for text regression problems is `"adam"`, and a good loss function for regression is mean squared error, `"mse"`.
```

```{r rnnmodcompile}
rnn_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )
```

```{block, type = "rmdwarning"}
As we noted in Chapter \@ref(dlclassification), the neural network model is modified **in place**; the object `rnn_mod` is different after we compile it, even those we didn't assign the object to anything. This is different from how most objects in R work, so pay special attention to the state of your model objects.
```

After the model is compiled, we can fit it. The `fit()` method for keras models has an argument `validation_split` that will set apart a fraction of the training data for evaluation and assessment. The performance metrics are evaluated on the validation set at the _end_ of each epoch.

```{r}
set.seed(123)

rnn_history <- rnn_mod %>% 
  fit(
    doj_matrix, 
    doj_train$date,
    epochs = 10,
    validation_split = 0.25,
    batch_size = 64,
    verbose = FALSE
  )

rnn_history
```

The loss on the training data (called `loss` here) is much better than the loss on the validation data (`val_loss`), indicating that we are overfitting pretty dramatically. We can see this by plotting the history as well.

```{r}
plot(rnn_history)
```

### Evaluation {#dlregevaluation}

We used some keras defaults for model evaluation in the previous section, but we can take more control if we want or need to. Instead of using the `validation_split` argument, we can instead use the `validation_data` argument and send in our own validation set.

```{r dojval}
set.seed(234)
doj_val <- validation_split(doj_train, strata = date)
doj_val
```

We can access the two datasets specified by this `split` via the functions `analysis()` (the analog to training) and `assessment()` (the analog to testing). We need to apply our prepped preprocessing recipe `doj_prep` to both to have this data in the appropriate format for our neural network architecture.

```{r dojanalysis, dependson=c("dojmatrix", "dojval")}
doj_analysis <- bake(doj_prep, new_data = analysis(doj_val$splits[[1]]), 
                     composition = "matrix")
dim(doj_analysis)

doj_assess <- bake(doj_prep, new_data = assessment(doj_val$splits[[1]]), 
                   composition = "matrix")
dim(doj_assess)
```

These are each matrices appropriate for a keras model.

We will also need the outcome variables for both sets.

```{r dateanalysis, dependson="dojval"}
date_analysis <- analysis(doj_val$splits[[1]]) %>% pull(date)
date_assess <- assessment(doj_val$splits[[1]]) %>% pull(date)
```

Let's also think about our model architecture. We saw evidence for significant overfitting with our first RNN, and we can counteract that by including dropout, both in the regular sense (`dropout`) and in the feedback loops (`recurrent_dropout`). 

```{block, type = "rmdwarning"}
When we include some dropout, we temporarily remove some units together with their connections from the network. The purpose of this is typically to reduce overfitting.
```

```{r}
rnn_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  layer_simple_rnn(units = 64, dropout = 0.3, recurrent_dropout = 0.3) %>%
  layer_dense(units = 1) 

rnn_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )

val_history <- rnn_mod %>% 
  fit(
    doj_analysis, 
    date_analysis,
    epochs = 10,
    validation_data = list(doj_assess, date_assess),
    batch_size = 64,
    verbose = FALSE
  )

val_history
```

Instead of overfitting, now we are underfitting, with higher training loss than validation loss. This is too much dropout for this model architecture and this dataset.

```{r}
plot(val_history)
```

Remember that this is specific validation data that we have chosen ahead of time, so we can evaluate metrics flexibly in any way we need to, for example, using yardstick functions. We can create a tibble with the true and predicted values for the validation set.

```{r}
val_res <- tibble(date = date_assess,
                  .pred = predict(rnn_mod, doj_assess)[,1])

val_res %>% metrics(date, .pred)
```

These results are pretty disappointing overall! Simple RNNs like the ones in this section can be challenging to train well, and just cranking up the number of embedding dimensions or units usually does not fix the problem. Often, RNNs just don't work well compared to simpler deep learning architectures like the dense network introduced in Section tktk [@Minaee2020], or other machine learning approaches like regularized linear models with good preprocessing. For example, a regularized linear model for this dataset of DOJ press releases results in an RMSE of 0.517 and an $R^2$ of 0.73. 

Fortunately, we can build on the ideas of a simple RNN to build better performing models.

## Compare to an LSTM

Another network architecture used with text is the long short-term memory neural network (LSTM). This architecture is a special kind of RNN which solves problems with misbehaving gradients and can do a better job of "remembering" and "forgetting" information through sequences via a memory cell. 

```{block, type = "rmdwarning"}
Simple RNNs can only connect very recent information and structure in sequences, but LSTMS can learn long-range dependencies and broader context. 
```

LSTMs are useful in text modeling because of this memory through long sequences; they are also used for time series, machine translation, and similar problems. We can use the keras function `layer_lstm()` and keep the rest of our model specification the same as for our simple RNN. We `compile()` the model in the same way as well, and can use the same validation strategy.

```{r lstmmod}
lstm_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  layer_lstm(units = 64, dropout = 0.3, recurrent_dropout = 0.3) %>%
  layer_dense(units = 1) 

lstm_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )

lstm_history <- lstm_mod %>% 
  fit(
    doj_analysis, 
    date_analysis,
    epochs = 10,
    validation_data = list(doj_assess, date_assess),
    batch_size = 64,
    verbose = FALSE
  )

lstm_history
```

The loss, both for the training and validation data, is lower than in the previous sections, indicating that the LSTM architecture is better for modeling these DOJ press releases than the RNN architecture. This is generally true for basically all text data.

```{r}
plot(lstm_history)
```

The performance of this model on the validation data stops improving after about five or six epochs, indicating that we don't need to keep training beyond then. This is typical for neural networks; the longer you train, the more overfit your model will be. We can compute any metrics for our validation data that we need to, because we have them available in `doj_assess` and `date_assess`.

```{r}
lstm_res <- tibble(date = date_assess,
                   .pred = predict(lstm_mod, doj_assess)[,1])

lstm_res %>% metrics(date, .pred)
```

```{r echo=FALSE}
lstm_rsq <- lstm_res %>% 
  metrics(date, .pred) %>% 
  filter(.metric == "rsq") %>% 
  pull(.estimate) %>% 
  round(3)

lstm_rmse <- lstm_res %>% 
  metrics(date, .pred) %>% 
  filter(.metric == "rmse") %>% 
  pull(.estimate) %>% 
  round(4)
```

These results are much better than the RNNs, although not close to the regularized linear results of RMSE of 0.517 and an $R^2$ of 0.73. We can plot these predictions for the validation set to evaluate the performance across the range of dates. To make an interpretable plot, we need to convert our numeric representation for date back to R's date type, as we described in Section \@ref(firstdlregression).

```{r lstmpreds, fig.cap="Predicted and true dates for Department of Justice press releases using an LSTM model"}
lstm_res %>% 
  mutate(date = as.Date(date * 1e3 + 1.6e4, origin = "1970-01-01"),
         .pred = as.Date(.pred * 1e3 + 1.6e4, origin = "1970-01-01")) %>%
  ggplot(aes(date, .pred)) + 
  geom_abline(lty = 2, color = "gray20", size = 1.5, alpha = 0.8) + 
  geom_point(alpha = 0.2, color = "#4070a0") +
  labs(x = "Truth", y = "Predicted date")
```

This first LSTM model is still not working very well, especially at the more recent dates.

```{r plotpreds, echo=FALSE}
plot_preds <- function(res) {
  res %>% 
    mutate(date = as.Date(date * 1e3 + 1.6e4, origin = "1970-01-01"),
           .pred = as.Date(.pred * 1e3 + 1.6e4, origin = "1970-01-01")) %>%
    ggplot(aes(date, .pred)) + 
    geom_abline(lty = 2, color = "gray20", size = 1.5, alpha = 0.9) + 
    geom_point(alpha = 0.2, color = "#4070a0") +
    labs(x = "Truth", y = "Predicted date") +
    coord_fixed()
}
```



## Case study: bidirectional LSTM {#mlregbilstm}

The RNNs and LSTMs that we have fit so far have modeled text as sequences, specifically sequences where information and memory persists moving forward. These kinds of models can learn structures and dependencies moving forward _only_. In language, the structures move both directions, though; the words that come _after_ a given structure or word can be just as important for understanding it as the ones that come before it.

We can build this into our neural network architecture with a **bidirectional** wrapper for RNNs or LSTMs. 

```{block, type = "rmdnote"}
A bidirectional LSTM allows the network to have both the forward and backward information about the sequences at each step.
```

The input sequences are passed through the network in two directions, both forward and backward, allowing the network to learn more context, structures, and dependencies.


```{r}
bilstm_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  bidirectional(layer_lstm(units = 64, dropout = 0.3, recurrent_dropout = 0.3)) %>%
  layer_dense(units = 1) 

bilstm_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )

bilstm_history <- bilstm_mod %>% 
  fit(
    doj_analysis, 
    date_analysis,
    epochs = 10,
    validation_data = list(doj_assess, date_assess),
    batch_size = 64,
    verbose = FALSE
  )

bilstm_history
```

The bidirectional LSTM is theoretically more able to represent the data well, but with the same amount of dropout, the result is more dramatic overfitting; there is not much improvement on the validation set. When moving to a more complex network, we will need to take more steps to reduce overfitting.


```{r}
bilstm_res <- tibble(date = date_assess,
                     .pred = predict(bilstm_mod, doj_assess)[,1])

bilstm_res %>% metrics(date, .pred)
```

This bidirectional LSTM, able to learn both forward and backward text structures, is not much of an improvement over the regular LSTM on the validation set (which had an RMSE of `r lstm_rmse`) because of the overfitting. 


## Case study: stacking LSTM layers

Deep learning architectures can be built up to create extremely complex networks. For example, RNN and LSTM layers can be stacked on top of each other. The idea of this stacking is to increase the ability of a network to represent the data well. 

```{block, type = "rmdwarning"}
Intermediate layers must be set up to return sequences (with `return_sequences = TRUE`) instead of the last output for each sequence.
```

Let's start by adding one single additional layer.

```{r stackmod}
stacked_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  layer_lstm(units = 64, dropout = 0.3, recurrent_dropout = 0.3, 
             return_sequences = TRUE) %>%
  layer_lstm(units = 64, dropout = 0.3, recurrent_dropout = 0.3) %>%
  layer_dense(units = 1) 

stacked_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )

stacked_history <- stacked_mod %>% 
  fit(
    doj_analysis, 
    date_analysis,
    epochs = 10,
    validation_data = list(doj_assess, date_assess),
    batch_size = 64,
    verbose = FALSE
  )

stacked_history
```

Adding another separate layer in the forward direction appears to have improved the network, more than extending the LSTM layer to handle information in the backward direction via the bidirectional LSTM.

```{r}
stacked_res <- tibble(date = date_assess,
                      .pred = predict(stacked_mod, doj_assess)[,1])

stacked_res %>% metrics(date, .pred)
```

This model is still not performing even as well as a well-trained regularized linear model, unfortunately.

## Case study: cross-validation for deep learning

So far, we have relied on the keras' `fit()` method and its internal processes for finding and using a single validation test. We can use other resampling approaches with deep learning, just as we did with the machine learning approaches as first described in Section \@ref(firstregressionevaluation).

```{block, type = "rmdnote"}
Think of a single validation set as conceptually doing the same thing as multiple iterations of resampling like cross-validation or bootstrap. A validation set provides **one** opportunity to estimate performance, and repeated iterations of resampling provide **multiple** opportunities.
```

A single validation set works well when the original data set is quite large. If that original pool of data isn't quite so large, like in our dataset of DOJ press releases, then the set allocated for validation ends up too small to be statistically representative or the set allocated for training isn't adequate for fitting more complex architectures. In such cases, instead of a single validation set, we can use a resampling strategy like cross-validation.

```{r dojfolds}
set.seed(234)

doj_folds <- vfold_cv(doj_train, v = 5, strata = date)
doj_folds
```

The dataset of DOJ press releases we are using has `r scales::comma(nrow(doj_press))` total examples, and we allocated 3/4 of the press releases to training and 1/4 to testing. When we use 5-fold cross-validation, the `r scales::comma(nrow(doj_train))` observations in the training set are divided into five folds; we use four folds for fitting and one fold for evaluation/assessment during each iteration, iterating through the folds.

```{block, type = "rmdwarning"}
What does it mean to conduct stratified resampling with a continuous variable like `date`? The `strata` are determined from quantiles of the distribution of `date`.
```

Let's use the same LSTM from Section tktk but give it a different name for reading clarity. 

```{r cvbilstmmod}
cv_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  layer_lstm(units = 64, dropout = 0.3, recurrent_dropout = 0.3) %>%
  layer_dense(units = 1) 

cv_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )
```


However, we aren't going to just `fit()` this model; we are going to write a function that will take an rsample `split` and fit on the analysis set, then evaluate on the assessment set. 


```{block, type = "rmdwarning"}
Remember that a model like this is **modified in place**, so this process will fit the model for one epoch each on the five cross-validation folds, for a total of five epochs. The function returns the transformed validation data for each split, but the model is being updated.
```


```{r}
fit_split <- function(split, prepped_rec, mod) {
  
  x_train <- bake(prepped_rec, new_data = analysis(split), 
                  composition = "matrix")
  x_val   <- bake(prepped_rec, new_data = assessment(split), 
                  composition = "matrix")
  
  y_train <- analysis(split) %>% pull(date)
  y_val   <- assessment(split) %>% pull(date)
  
  mod %>%
    fit(
      x_train, 
      y_train,
      epochs = 1,
      validation_data = list(x_val, y_val),
      batch_size = 64,
      verbose = FALSE
    )
  
  x_val
}
```

We can `map()` this function across all our cross-validation folds. The model will be trained for five epochs (1 each for the 5 folds).

```{r}
cv_fitted <- doj_folds %>%
  mutate(validation = map(splits, fit_split, doj_prep, cv_mod))

cv_fitted
```

Now we can compute model performance metrics using our trained model `cv_mod`.

```{r}
cv_metrics <- cv_fitted %>%
  mutate(truth = map(splits, ~ assessment(.) %>% pull(date)),
         .pred = map(validation, ~ predict(cv_mod, .)[,1]),
         rmse = map2_dbl(truth, .pred, rmse_vec),
         rsq  = map2_dbl(truth, .pred, rsq_vec))

cv_metrics %>%
  select(splits, id, rmse, rsq)
```

These are metrics computed on the assessment set of each resample. They look very promising!

```{block, type = "rmdnote"}
The functions `rmse_vec()` and `rsq_vec()` from yardstick are for situations when the true and predicted values are stored in a vector rather than dataframe or tibble, like we have here. All yardstick metrics have vector versions of functions.
```

We can compute metrics on our single validation set like we have in previous sections, for comparison.

```{r}
cv_res <- tibble(date = date_assess,
                 .pred = predict(cv_mod, doj_assess)[,1])

cv_res %>% metrics(date, .pred)
```

Finally we have trained a deep learning model that performs better than a straightforward linear model. Remember that we reported in Section \@ref(dlregevaluation) that such a model has an RMSE of 0.517 and an $R^2$ of 0.73. Using cross-validation results in much better model performance compared to using a single validation set alone. The amount of data we have in these DOJ press releases requires a more complex resampling strategy for fitting neural networks, rather than a single validation set.


```{r cvpreds, fig.cap="Predicted and true dates for Department of Justice press releases using an LSTM and cross-validation"}
plot_preds(cv_res)
```


```{block, type = "rmdnote"}
Being able to flexibly use a resampling strategy like cross-validation is an important piece of your machine learning toolkit. 
```



## Case study: padding

```{r padrec, dependson="dojsplit"}
padding_rec <- recipe(~ contents, data = doj_train) %>%
  step_tokenize(contents) %>%
  step_tokenfilter(contents, max_tokens = max_words) %>%
  step_sequence_onehot(contents, sequence_length = max_length, padding = "post")

padding_prep <- prep(padding_rec)
padding_matrix <- bake(padding_prep, new_data = NULL, composition = "matrix")
dim(padding_matrix)
```

This matrix has the same dimensions as `doj_matrix` but instead of padding with zeroes at the beginning of these DOJ press releases, this matrix is padded with zeroes at the end. (This preprocessing strategy still truncates longer sequences in the same way.)

```{r padanalysis, dependson=c("padrec", "dojval")}
pad_analysis <- bake(padding_prep, new_data = analysis(doj_val$splits[[1]]), 
                     composition = "matrix")
pad_assess <- bake(padding_prep, new_data = assessment(doj_val$splits[[1]]), 
                   composition = "matrix")
```


```{r}
padding_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  layer_lstm(units = 64, dropout = 0.3, recurrent_dropout = 0.3) %>%
  layer_dense(units = 1) 

padding_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )

padding_history <- padding_mod %>% 
  fit(
    pad_analysis, 
    date_analysis,
    epochs = 10,
    validation_data = list(pad_assess, date_assess),
    batch_size = 64,
    verbose = FALSE
  )

padding_history
```
This padding strategy results in much worse performance than the default option!

```{r}
padding_res <- tibble(date = date_assess,
                      .pred = predict(padding_mod, pad_assess)[,1])

padding_res %>% metrics(date, .pred)
```

The same model architecture with default padding preprocessing resulted in an RMSE of `r lstm_rmse` and an $R^2$ of `r lstm_rsq`; changing to `padding = "post"` has resulted in a remarkable degrading of predictive capacity.

```{block, type = "rmdwarning"}
Different preprocessing strategies have a huge impact on deep learning results. 
```


## Case study: max_length, number of words, batch size???


## The full game: regression {#dlregfull}

We've learned a lot about how to model this dataset over the course of this chapter.

- We can use `dropout` and cross-validation to reduce overfitting.
- Let's stack several layers together, and in fact increase the number of LSTM layers to three.
- We have still been overfitting significantly with our model architectures, so let's try increasing the dropout more.

```{r dlregfinalmod}
final_mod <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1, output_dim = 128) %>%
  layer_lstm(
    units = 64, dropout = 0.4, recurrent_dropout = 0.4, 
    return_sequences = TRUE
  ) %>%
  layer_lstm(
    units = 64, dropout = 0.4, recurrent_dropout = 0.4, 
    return_sequences = TRUE
  ) %>%
  layer_lstm(
    units = 64, dropout = 0.4, recurrent_dropout = 0.4
  ) %>%
  layer_dense(units = 1) 

final_mod %>% 
  compile(
    optimizer = "adam",
    loss = "mse",
    metrics = c("mean_squared_error")
  )
```

Like we did in Section tktk, we can `map()` our function `fit_split()` across our five cross-validation folds, training for one epoch on each fold.

```{r}
final_fitted <- doj_folds %>%
  mutate(validation = map(splits, fit_split, doj_prep, final_mod))

final_fitted
```

Remember that `final_mod` is modified in place so as this process fits the model for one epoch each on the five cross-validation folds, the model is being updated. Once finished, we can compute model performance metrics using our now-trained model `final_mod`.

```{r}
final_metrics <- final_fitted %>%
  mutate(truth = map(splits, ~ assessment(.) %>% pull(date)),
         .pred = map(validation, ~ predict(final_mod, .)[,1]),
         rmse  = map2_dbl(truth, .pred, rmse_vec),
         rsq   = map2_dbl(truth, .pred, rsq_vec))

final_metrics %>%
  select(splits, id, rmse, rsq)
```

These are metrics computed on the assessment set of each resample, created from the training set. Let's finally turn to the testing set to evaluate this last model on data that has never been touched as part of the fitting process. First, the testing data needs to be preprocessed using the prepared recipe `doj_prep`.

```{r}
doj_testing <- bake(doj_prep, new_data = doj_test, composition = "matrix")
dim(doj_testing)
```

Now we can predict on the preprocessed training data `doj_testing` used the fitted model `final_mod` that was trained using cross-validation.

```{r}
final_res <- tibble(date = doj_test$date,
                      .pred = predict(final_mod, doj_testing)[,1])

final_res %>% metrics(date, .pred)
```


```{r}
plot_preds(final_res)
```

## Summary {#dlregressionsummary}

Neural networks don't always perform better than more straightforward models...

### In this chapter, you learned:

- how to preprocess text data for deep learning models
- about the importance of centering and scaling regression outcomes for neural networks
- about RNN, LSTM, and bidirectional LSTM network architectures
- that network layers like RNNs and LSTMs can be stacked for greater network capacity
- about resampling strategies for deep learning models
- how to evaluate deep learning regression models for text


```{r echo=FALSE}
knitr::knit_exit()
```


```{r eval = FALSE}
library(hardhat)
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")

## baseline lasso model
set.seed(123)
doj_folds <- vfold_cv(doj_train)

doj_rec <- recipe(date ~ contents, data = doj_train) %>%
  step_tokenize(contents) %>%
  step_tokenfilter(contents, max_tokens = 5e3) %>%
  step_tfidf(contents)

doj_rec

lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")
lasso_spec

lambda_grid <- grid_regular(penalty(), levels = 20)
lambda_grid

doj_wf <- workflow() %>%
  add_recipe(doj_rec, blueprint = sparse_bp) %>%
  add_model(lasso_spec)

doj_wf

doParallel::registerDoParallel()
set.seed(2020)
lasso_rs <- tune_grid(
  doj_wf,
  doj_folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE)
)

autoplot(lasso_rs)

show_best(lasso_rs, "rmse")   ## rmse ~ .517 for date transformed to numeric
show_best(lasso_rs, "rsq")    ## rsq ~ 0.73

lasso_rs %>%
  collect_predictions() %>%
  inner_join(select_best(lasso_rs, "rmse")) %>%
  mutate(date = as.Date(date * 1e3 + 1.6e4, origin = "1970-01-01"),
         .pred = as.Date(.pred * 1e3 + 1.6e4, origin = "1970-01-01")) %>%
  ggplot(aes(x = date, y = .pred, color = id)) + 
  geom_abline(lty = 2, color = "gray50", size = 1.5, alpha = 0.8) + 
  geom_point(alpha = 0.3) +
  ylim(min(doj_press$date), max(doj_press$date))

```



