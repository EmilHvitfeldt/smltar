<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Stemming | Supervised Machine Learning for Text Analysis in R</title>
  <meta name="description" content="Chapter 4 Stemming | Supervised Machine Learning for Text Analysis in R" />
  <meta name="generator" content="bookdown 0.21.7 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Stemming | Supervised Machine Learning for Text Analysis in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 4 Stemming | Supervised Machine Learning for Text Analysis in R" />
  <meta name="github-repo" content="EmilHvitfeldt/smltar" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Stemming | Supervised Machine Learning for Text Analysis in R" />
  
  <meta name="twitter:description" content="Chapter 4 Stemming | Supervised Machine Learning for Text Analysis in R" />
  

<meta name="author" content="Emil Hvitfeldt and Julia Silge" />


<meta name="date" content="2021-03-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="stopwords.html"/>
<link rel="next" href="embeddings.html"/>
<script src="libs/header-attrs-2.7.4/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="libs/plot_text_explanations-0.1.0/plot_text_explanations.css" rel="stylesheet" />
<script src="libs/plot_text_explanations-binding-0.5.2/plot_text_explanations.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="smltar.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning for Text Analysis in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Supervised Machine Learning for Text Analysis in R</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#topics-this-book-will-not-cover"><i class="fa fa-check"></i>Topics this book will not cover</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#colophon"><i class="fa fa-check"></i>Colophon</a></li>
</ul></li>
<li class="part"><span><b>I Natural Language Features</b></span></li>
<li class="chapter" data-level="1" data-path="language.html"><a href="language.html"><i class="fa fa-check"></i><b>1</b> Language and modeling</a>
<ul>
<li class="chapter" data-level="1.1" data-path="language.html"><a href="language.html#linguistics-for-text-analysis"><i class="fa fa-check"></i><b>1.1</b> Linguistics for text analysis</a></li>
<li class="chapter" data-level="1.2" data-path="language.html"><a href="language.html#morphology"><i class="fa fa-check"></i><b>1.2</b> A glimpse into one area: morphology</a></li>
<li class="chapter" data-level="1.3" data-path="language.html"><a href="language.html#different-languages"><i class="fa fa-check"></i><b>1.3</b> Different languages</a></li>
<li class="chapter" data-level="1.4" data-path="language.html"><a href="language.html#other-ways-text-can-vary"><i class="fa fa-check"></i><b>1.4</b> Other ways text can vary</a></li>
<li class="chapter" data-level="1.5" data-path="language.html"><a href="language.html#languagesummary"><i class="fa fa-check"></i><b>1.5</b> Summary</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="language.html"><a href="language.html#in-this-chapter-you-learned"><i class="fa fa-check"></i><b>1.5.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tokenization.html"><a href="tokenization.html"><i class="fa fa-check"></i><b>2</b> Tokenization</a>
<ul>
<li class="chapter" data-level="2.1" data-path="tokenization.html"><a href="tokenization.html#what-is-a-token"><i class="fa fa-check"></i><b>2.1</b> What is a token?</a></li>
<li class="chapter" data-level="2.2" data-path="tokenization.html"><a href="tokenization.html#types-of-tokens"><i class="fa fa-check"></i><b>2.2</b> Types of tokens</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="tokenization.html"><a href="tokenization.html#character-tokens"><i class="fa fa-check"></i><b>2.2.1</b> Character tokens</a></li>
<li class="chapter" data-level="2.2.2" data-path="tokenization.html"><a href="tokenization.html#word-tokens"><i class="fa fa-check"></i><b>2.2.2</b> Word tokens</a></li>
<li class="chapter" data-level="2.2.3" data-path="tokenization.html"><a href="tokenization.html#tokenizingngrams"><i class="fa fa-check"></i><b>2.2.3</b> Tokenizing by n-grams</a></li>
<li class="chapter" data-level="2.2.4" data-path="tokenization.html"><a href="tokenization.html#lines-sentence-and-paragraph-tokens"><i class="fa fa-check"></i><b>2.2.4</b> Lines, sentence, and paragraph tokens</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="tokenization.html"><a href="tokenization.html#where-does-tokenization-break-down"><i class="fa fa-check"></i><b>2.3</b> Where does tokenization break down?</a></li>
<li class="chapter" data-level="2.4" data-path="tokenization.html"><a href="tokenization.html#building-your-own-tokenizer"><i class="fa fa-check"></i><b>2.4</b> Building your own tokenizer</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="tokenization.html"><a href="tokenization.html#tokenize-to-characters-only-keeping-letters"><i class="fa fa-check"></i><b>2.4.1</b> Tokenize to characters, only keeping letters</a></li>
<li class="chapter" data-level="2.4.2" data-path="tokenization.html"><a href="tokenization.html#allow-for-hyphenated-words"><i class="fa fa-check"></i><b>2.4.2</b> Allow for hyphenated words</a></li>
<li class="chapter" data-level="2.4.3" data-path="tokenization.html"><a href="tokenization.html#wrapping-it-in-a-function"><i class="fa fa-check"></i><b>2.4.3</b> Wrapping it in a function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="tokenization.html"><a href="tokenization.html#tokenization-for-non-latin-alphabets"><i class="fa fa-check"></i><b>2.5</b> Tokenization for non-Latin alphabets</a></li>
<li class="chapter" data-level="2.6" data-path="tokenization.html"><a href="tokenization.html#tokenization-benchmark"><i class="fa fa-check"></i><b>2.6</b> Tokenization benchmark</a></li>
<li class="chapter" data-level="2.7" data-path="tokenization.html"><a href="tokenization.html#tokensummary"><i class="fa fa-check"></i><b>2.7</b> Summary</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="tokenization.html"><a href="tokenization.html#in-this-chapter-you-learned-1"><i class="fa fa-check"></i><b>2.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stopwords.html"><a href="stopwords.html"><i class="fa fa-check"></i><b>3</b> Stop words</a>
<ul>
<li class="chapter" data-level="3.1" data-path="stopwords.html"><a href="stopwords.html#premadestopwords"><i class="fa fa-check"></i><b>3.1</b> Using premade stop word lists</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="stopwords.html"><a href="stopwords.html#stop-word-removal-in-r"><i class="fa fa-check"></i><b>3.1.1</b> Stop word removal in R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="stopwords.html"><a href="stopwords.html#homemadestopwords"><i class="fa fa-check"></i><b>3.2</b> Creating your own stop words list</a></li>
<li class="chapter" data-level="3.3" data-path="stopwords.html"><a href="stopwords.html#all-stop-word-lists-are-context-specific"><i class="fa fa-check"></i><b>3.3</b> All stop word lists are context-specific</a></li>
<li class="chapter" data-level="3.4" data-path="stopwords.html"><a href="stopwords.html#what-happens-when-you-remove-stop-words"><i class="fa fa-check"></i><b>3.4</b> What happens when you remove stop words</a></li>
<li class="chapter" data-level="3.5" data-path="stopwords.html"><a href="stopwords.html#stop-words-in-languages-other-than-english"><i class="fa fa-check"></i><b>3.5</b> Stop words in languages other than English</a></li>
<li class="chapter" data-level="3.6" data-path="stopwords.html"><a href="stopwords.html#stopwordssummary"><i class="fa fa-check"></i><b>3.6</b> Summary</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="stopwords.html"><a href="stopwords.html#in-this-chapter-you-learned-2"><i class="fa fa-check"></i><b>3.6.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stemming.html"><a href="stemming.html"><i class="fa fa-check"></i><b>4</b> Stemming</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stemming.html"><a href="stemming.html#how-to-stem-text-in-r"><i class="fa fa-check"></i><b>4.1</b> How to stem text in R</a></li>
<li class="chapter" data-level="4.2" data-path="stemming.html"><a href="stemming.html#should-you-use-stemming-at-all"><i class="fa fa-check"></i><b>4.2</b> Should you use stemming at all?</a></li>
<li class="chapter" data-level="4.3" data-path="stemming.html"><a href="stemming.html#understand-a-stemming-algorithm"><i class="fa fa-check"></i><b>4.3</b> Understand a stemming algorithm</a></li>
<li class="chapter" data-level="4.4" data-path="stemming.html"><a href="stemming.html#handling-punctuation-when-stemming"><i class="fa fa-check"></i><b>4.4</b> Handling punctuation when stemming</a></li>
<li class="chapter" data-level="4.5" data-path="stemming.html"><a href="stemming.html#compare-some-stemming-options"><i class="fa fa-check"></i><b>4.5</b> Compare some stemming options</a></li>
<li class="chapter" data-level="4.6" data-path="stemming.html"><a href="stemming.html#lemmatization"><i class="fa fa-check"></i><b>4.6</b> Lemmatization and stemming</a></li>
<li class="chapter" data-level="4.7" data-path="stemming.html"><a href="stemming.html#stemming-and-stop-words"><i class="fa fa-check"></i><b>4.7</b> Stemming and stop words</a></li>
<li class="chapter" data-level="4.8" data-path="stemming.html"><a href="stemming.html#stemmingsummary"><i class="fa fa-check"></i><b>4.8</b> Summary</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="stemming.html"><a href="stemming.html#in-this-chapter-you-learned-3"><i class="fa fa-check"></i><b>4.8.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="embeddings.html"><a href="embeddings.html"><i class="fa fa-check"></i><b>5</b> Word Embeddings</a>
<ul>
<li class="chapter" data-level="5.1" data-path="embeddings.html"><a href="embeddings.html#motivatingsparse"><i class="fa fa-check"></i><b>5.1</b> Motivating embeddings for sparse, high-dimensional data</a></li>
<li class="chapter" data-level="5.2" data-path="embeddings.html"><a href="embeddings.html#understand-word-embeddings-by-finding-them-yourself"><i class="fa fa-check"></i><b>5.2</b> Understand word embeddings by finding them yourself</a></li>
<li class="chapter" data-level="5.3" data-path="embeddings.html"><a href="embeddings.html#exploring-cfpb-word-embeddings"><i class="fa fa-check"></i><b>5.3</b> Exploring CFPB word embeddings</a></li>
<li class="chapter" data-level="5.4" data-path="embeddings.html"><a href="embeddings.html#glove"><i class="fa fa-check"></i><b>5.4</b> Use pre-trained word embeddings</a></li>
<li class="chapter" data-level="5.5" data-path="embeddings.html"><a href="embeddings.html#fairnessembeddings"><i class="fa fa-check"></i><b>5.5</b> Fairness and word embeddings</a></li>
<li class="chapter" data-level="5.6" data-path="embeddings.html"><a href="embeddings.html#using-word-embeddings-in-the-real-world"><i class="fa fa-check"></i><b>5.6</b> Using word embeddings in the real world</a></li>
<li class="chapter" data-level="5.7" data-path="embeddings.html"><a href="embeddings.html#embeddingssummary"><i class="fa fa-check"></i><b>5.7</b> Summary</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="embeddings.html"><a href="embeddings.html#in-this-chapter-you-learned-4"><i class="fa fa-check"></i><b>5.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Machine Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html"><i class="fa fa-check"></i>Foreword</a>
<ul>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#should-we-even-be-doing-this"><i class="fa fa-check"></i>Should we even be doing this?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-bias-is-already-in-the-data"><i class="fa fa-check"></i>What bias is already in the data?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#can-the-code-and-data-be-audited"><i class="fa fa-check"></i>Can the code and data be audited?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-are-the-error-rates-for-sub-groups"><i class="fa fa-check"></i>What are the error rates for sub-groups?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-is-the-accuracy-of-a-simple-rule-based-alternative"><i class="fa fa-check"></i>What is the accuracy of a simple rule-based alternative?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-processes-are-in-place-to-handle-appeals-or-mistakes"><i class="fa fa-check"></i>What processes are in place to handle appeals or mistakes?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#how-diverse-is-the-team-that-built-it"><i class="fa fa-check"></i>How diverse is the team that built it?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlregression.html"><a href="mlregression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mlregression.html"><a href="mlregression.html#firstmlregression"><i class="fa fa-check"></i><b>6.1</b> A first regression model</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="mlregression.html"><a href="mlregression.html#firstregression"><i class="fa fa-check"></i><b>6.1.1</b> Building our first regression model</a></li>
<li class="chapter" data-level="6.1.2" data-path="mlregression.html"><a href="mlregression.html#firstregressionevaluation"><i class="fa fa-check"></i><b>6.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="mlregression.html"><a href="mlregression.html#regnull"><i class="fa fa-check"></i><b>6.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="6.3" data-path="mlregression.html"><a href="mlregression.html#comparerf"><i class="fa fa-check"></i><b>6.3</b> Compare to a random forest model</a></li>
<li class="chapter" data-level="6.4" data-path="mlregression.html"><a href="mlregression.html#casestudystopwords"><i class="fa fa-check"></i><b>6.4</b> Case study: removing stop words</a></li>
<li class="chapter" data-level="6.5" data-path="mlregression.html"><a href="mlregression.html#casestudyngrams"><i class="fa fa-check"></i><b>6.5</b> Case study: varying n-grams</a></li>
<li class="chapter" data-level="6.6" data-path="mlregression.html"><a href="mlregression.html#mlregressionlemmatization"><i class="fa fa-check"></i><b>6.6</b> Case study: lemmatization</a></li>
<li class="chapter" data-level="6.7" data-path="mlregression.html"><a href="mlregression.html#case-study-feature-hashing"><i class="fa fa-check"></i><b>6.7</b> Case study: feature hashing</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="mlregression.html"><a href="mlregression.html#text-normalization"><i class="fa fa-check"></i><b>6.7.1</b> Text normalization</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="mlregression.html"><a href="mlregression.html#what-evaluation-metrics-are-appropriate"><i class="fa fa-check"></i><b>6.8</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="6.9" data-path="mlregression.html"><a href="mlregression.html#mlregressionfull"><i class="fa fa-check"></i><b>6.9</b> The full game: regression</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="mlregression.html"><a href="mlregression.html#preprocess-the-data"><i class="fa fa-check"></i><b>6.9.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="6.9.2" data-path="mlregression.html"><a href="mlregression.html#specify-the-model"><i class="fa fa-check"></i><b>6.9.2</b> Specify the model</a></li>
<li class="chapter" data-level="6.9.3" data-path="mlregression.html"><a href="mlregression.html#tune-the-model"><i class="fa fa-check"></i><b>6.9.3</b> Tune the model</a></li>
<li class="chapter" data-level="6.9.4" data-path="mlregression.html"><a href="mlregression.html#regression-final-evaluation"><i class="fa fa-check"></i><b>6.9.4</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="mlregression.html"><a href="mlregression.html#mlregressionsummary"><i class="fa fa-check"></i><b>6.10</b> Summary</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="mlregression.html"><a href="mlregression.html#in-this-chapter-you-learned-5"><i class="fa fa-check"></i><b>6.10.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mlclassification.html"><a href="mlclassification.html"><i class="fa fa-check"></i><b>7</b> Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mlclassification.html"><a href="mlclassification.html#classfirstattemptlookatdata"><i class="fa fa-check"></i><b>7.1</b> A first classification model</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="mlclassification.html"><a href="mlclassification.html#classfirstmodel"><i class="fa fa-check"></i><b>7.1.1</b> Building our first classification model</a></li>
<li class="chapter" data-level="7.1.2" data-path="mlclassification.html"><a href="mlclassification.html#evaluation"><i class="fa fa-check"></i><b>7.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mlclassification.html"><a href="mlclassification.html#classnull"><i class="fa fa-check"></i><b>7.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="7.3" data-path="mlclassification.html"><a href="mlclassification.html#comparetolasso"><i class="fa fa-check"></i><b>7.3</b> Compare to a lasso classification model</a></li>
<li class="chapter" data-level="7.4" data-path="mlclassification.html"><a href="mlclassification.html#tunelasso"><i class="fa fa-check"></i><b>7.4</b> Tuning lasso hyperparameters</a></li>
<li class="chapter" data-level="7.5" data-path="mlclassification.html"><a href="mlclassification.html#casestudysparseencoding"><i class="fa fa-check"></i><b>7.5</b> Case study: sparse encoding</a></li>
<li class="chapter" data-level="7.6" data-path="mlclassification.html"><a href="mlclassification.html#mlmulticlass"><i class="fa fa-check"></i><b>7.6</b> Two class or multiclass?</a></li>
<li class="chapter" data-level="7.7" data-path="mlclassification.html"><a href="mlclassification.html#case-study-including-non-text-data"><i class="fa fa-check"></i><b>7.7</b> Case study: including non-text data</a></li>
<li class="chapter" data-level="7.8" data-path="mlclassification.html"><a href="mlclassification.html#case-study-data-censoring"><i class="fa fa-check"></i><b>7.8</b> Case study: data censoring</a></li>
<li class="chapter" data-level="7.9" data-path="mlclassification.html"><a href="mlclassification.html#customfeatures"><i class="fa fa-check"></i><b>7.9</b> Case study: custom features</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="mlclassification.html"><a href="mlclassification.html#detect-credit-cards"><i class="fa fa-check"></i><b>7.9.1</b> Detect credit cards</a></li>
<li class="chapter" data-level="7.9.2" data-path="mlclassification.html"><a href="mlclassification.html#calculate-percentage-censoring"><i class="fa fa-check"></i><b>7.9.2</b> Calculate percentage censoring</a></li>
<li class="chapter" data-level="7.9.3" data-path="mlclassification.html"><a href="mlclassification.html#detect-monetary-amounts"><i class="fa fa-check"></i><b>7.9.3</b> Detect monetary amounts</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="mlclassification.html"><a href="mlclassification.html#what-evaluation-metrics-are-appropriate-1"><i class="fa fa-check"></i><b>7.10</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="7.11" data-path="mlclassification.html"><a href="mlclassification.html#mlclassificationfull"><i class="fa fa-check"></i><b>7.11</b> The full game: classification</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="mlclassification.html"><a href="mlclassification.html#feature-selection"><i class="fa fa-check"></i><b>7.11.1</b> Feature selection</a></li>
<li class="chapter" data-level="7.11.2" data-path="mlclassification.html"><a href="mlclassification.html#specify-the-model-1"><i class="fa fa-check"></i><b>7.11.2</b> Specify the model</a></li>
<li class="chapter" data-level="7.11.3" data-path="mlclassification.html"><a href="mlclassification.html#classification-final-evaluation"><i class="fa fa-check"></i><b>7.11.3</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="mlclassification.html"><a href="mlclassification.html#mlclassificationsummary"><i class="fa fa-check"></i><b>7.12</b> Summary</a>
<ul>
<li class="chapter" data-level="7.12.1" data-path="mlclassification.html"><a href="mlclassification.html#in-this-chapter-you-learned-6"><i class="fa fa-check"></i><b>7.12.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Deep Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html"><i class="fa fa-check"></i>Foreword</a>
<ul>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#spending-your-data-budget"><i class="fa fa-check"></i>Spending your data budget</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#feature-engineering"><i class="fa fa-check"></i>Feature engineering</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#fitting-and-tuning"><i class="fa fa-check"></i>Fitting and tuning</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#model-evaluation"><i class="fa fa-check"></i>Model evaluation</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#putting-the-model-process-in-context"><i class="fa fa-check"></i>Putting the model process in context</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="dldnn.html"><a href="dldnn.html"><i class="fa fa-check"></i><b>8</b> Dense neural networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dldnn.html"><a href="dldnn.html#kickstarter"><i class="fa fa-check"></i><b>8.1</b> Kickstarter data</a></li>
<li class="chapter" data-level="8.2" data-path="dldnn.html"><a href="dldnn.html#firstdlclassification"><i class="fa fa-check"></i><b>8.2</b> A first deep learning model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="dldnn.html"><a href="dldnn.html#dnnrecipe"><i class="fa fa-check"></i><b>8.2.1</b> Preprocessing for deep learning</a></li>
<li class="chapter" data-level="8.2.2" data-path="dldnn.html"><a href="dldnn.html#onehotsequence"><i class="fa fa-check"></i><b>8.2.2</b> One-hot sequence embedding of text</a></li>
<li class="chapter" data-level="8.2.3" data-path="dldnn.html"><a href="dldnn.html#simple-flattened-dense-network"><i class="fa fa-check"></i><b>8.2.3</b> Simple flattened dense network</a></li>
<li class="chapter" data-level="8.2.4" data-path="dldnn.html"><a href="dldnn.html#evaluate-dnn"><i class="fa fa-check"></i><b>8.2.4</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="dldnn.html"><a href="dldnn.html#using-pre-trained-word-embeddings"><i class="fa fa-check"></i><b>8.3</b> Using pre-trained word embeddings</a></li>
<li class="chapter" data-level="8.4" data-path="dldnn.html"><a href="dldnn.html#dnncross"><i class="fa fa-check"></i><b>8.4</b> Cross-validation for deep learning models</a></li>
<li class="chapter" data-level="8.5" data-path="dldnn.html"><a href="dldnn.html#compare-and-evaluate-dnn-models"><i class="fa fa-check"></i><b>8.5</b> Compare and evaluate DNN models</a></li>
<li class="chapter" data-level="8.6" data-path="dldnn.html"><a href="dldnn.html#dllimitations"><i class="fa fa-check"></i><b>8.6</b> Limitations of deep learning</a></li>
<li class="chapter" data-level="8.7" data-path="dldnn.html"><a href="dldnn.html#dldnnsummary"><i class="fa fa-check"></i><b>8.7</b> Summary</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="dldnn.html"><a href="dldnn.html#in-this-chapter-you-learned-7"><i class="fa fa-check"></i><b>8.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dllstm.html"><a href="dllstm.html"><i class="fa fa-check"></i><b>9</b> Long short-term memory (LSTM) networks</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dllstm.html"><a href="dllstm.html#firstlstm"><i class="fa fa-check"></i><b>9.1</b> A first LSTM model</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="dllstm.html"><a href="dllstm.html#building-an-lstm"><i class="fa fa-check"></i><b>9.1.1</b> Building an LSTM</a></li>
<li class="chapter" data-level="9.1.2" data-path="dllstm.html"><a href="dllstm.html#lstmevaluation"><i class="fa fa-check"></i><b>9.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dllstm.html"><a href="dllstm.html#compare-to-a-recurrent-neural-network"><i class="fa fa-check"></i><b>9.2</b> Compare to a recurrent neural network</a></li>
<li class="chapter" data-level="9.3" data-path="dllstm.html"><a href="dllstm.html#bilstm"><i class="fa fa-check"></i><b>9.3</b> Case study: bidirectional LSTM</a></li>
<li class="chapter" data-level="9.4" data-path="dllstm.html"><a href="dllstm.html#case-study-stacking-lstm-layers"><i class="fa fa-check"></i><b>9.4</b> Case study: stacking LSTM layers</a></li>
<li class="chapter" data-level="9.5" data-path="dllstm.html"><a href="dllstm.html#lstmpadding"><i class="fa fa-check"></i><b>9.5</b> Case study: padding</a></li>
<li class="chapter" data-level="9.6" data-path="dllstm.html"><a href="dllstm.html#case-study-training-a-regression-model"><i class="fa fa-check"></i><b>9.6</b> Case study: training a regression model</a></li>
<li class="chapter" data-level="9.7" data-path="dllstm.html"><a href="dllstm.html#case-study-vocabulary-size"><i class="fa fa-check"></i><b>9.7</b> Case study: vocabulary size</a></li>
<li class="chapter" data-level="9.8" data-path="dllstm.html"><a href="dllstm.html#lstmfull"><i class="fa fa-check"></i><b>9.8</b> The full game: LSTM</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="dllstm.html"><a href="dllstm.html#lstmfullpreprocess"><i class="fa fa-check"></i><b>9.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="9.8.2" data-path="dllstm.html"><a href="dllstm.html#lstmfullmodel"><i class="fa fa-check"></i><b>9.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="dllstm.html"><a href="dllstm.html#dllstmsummary"><i class="fa fa-check"></i><b>9.9</b> Summary</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="dllstm.html"><a href="dllstm.html#in-this-chapter-you-learned-8"><i class="fa fa-check"></i><b>9.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dlcnn.html"><a href="dlcnn.html"><i class="fa fa-check"></i><b>10</b> Convolutional neural networks</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dlcnn.html"><a href="dlcnn.html#what-are-cnns"><i class="fa fa-check"></i><b>10.1</b> What are CNNs?</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="dlcnn.html"><a href="dlcnn.html#filters"><i class="fa fa-check"></i><b>10.1.1</b> Filters</a></li>
<li class="chapter" data-level="10.1.2" data-path="dlcnn.html"><a href="dlcnn.html#kernel-size"><i class="fa fa-check"></i><b>10.1.2</b> Kernel size</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="dlcnn.html"><a href="dlcnn.html#firstcnn"><i class="fa fa-check"></i><b>10.2</b> A first CNN model</a></li>
<li class="chapter" data-level="10.3" data-path="dlcnn.html"><a href="dlcnn.html#case-study-adding-more-layers"><i class="fa fa-check"></i><b>10.3</b> Case study: adding more layers</a></li>
<li class="chapter" data-level="10.4" data-path="dlcnn.html"><a href="dlcnn.html#case-study-byte-pair-encoding"><i class="fa fa-check"></i><b>10.4</b> Case study: byte pair encoding</a></li>
<li class="chapter" data-level="10.5" data-path="dlcnn.html"><a href="dlcnn.html#lime"><i class="fa fa-check"></i><b>10.5</b> Case study: explainability with LIME</a></li>
<li class="chapter" data-level="10.6" data-path="dlcnn.html"><a href="dlcnn.html#case-study-hyperparameter-search"><i class="fa fa-check"></i><b>10.6</b> Case study: hyperparameter search</a></li>
<li class="chapter" data-level="10.7" data-path="dlcnn.html"><a href="dlcnn.html#cross-validation-for-evaluation"><i class="fa fa-check"></i><b>10.7</b> Cross-validation for evaluation</a></li>
<li class="chapter" data-level="10.8" data-path="dlcnn.html"><a href="dlcnn.html#cnnfull"><i class="fa fa-check"></i><b>10.8</b> The full game: CNN</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="dlcnn.html"><a href="dlcnn.html#cnnfullpreprocess"><i class="fa fa-check"></i><b>10.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="10.8.2" data-path="dlcnn.html"><a href="dlcnn.html#cnnfullmodel"><i class="fa fa-check"></i><b>10.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="dlcnn.html"><a href="dlcnn.html#dlcnnsummary"><i class="fa fa-check"></i><b>10.9</b> Summary</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="dlcnn.html"><a href="dlcnn.html#in-this-chapter-you-learned-9"><i class="fa fa-check"></i><b>10.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Conclusion</b></span></li>
<li class="chapter" data-level="" data-path="text-models-in-the-real-world.html"><a href="text-models-in-the-real-world.html"><i class="fa fa-check"></i>Text models in the real world</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="regexp.html"><a href="regexp.html"><i class="fa fa-check"></i><b>A</b> Regular expressions</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regexp.html"><a href="regexp.html#literal-characters"><i class="fa fa-check"></i><b>A.1</b> Literal characters</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="regexp.html"><a href="regexp.html#meta-characters"><i class="fa fa-check"></i><b>A.1.1</b> Meta characters</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="regexp.html"><a href="regexp.html#full-stop-the-wildcard"><i class="fa fa-check"></i><b>A.2</b> Full stop, the wildcard</a></li>
<li class="chapter" data-level="A.3" data-path="regexp.html"><a href="regexp.html#character-classes"><i class="fa fa-check"></i><b>A.3</b> Character classes</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="regexp.html"><a href="regexp.html#shorthand-character-classes"><i class="fa fa-check"></i><b>A.3.1</b> Shorthand character classes</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="regexp.html"><a href="regexp.html#quantifiers"><i class="fa fa-check"></i><b>A.4</b> Quantifiers</a></li>
<li class="chapter" data-level="A.5" data-path="regexp.html"><a href="regexp.html#anchors"><i class="fa fa-check"></i><b>A.5</b> Anchors</a></li>
<li class="chapter" data-level="A.6" data-path="regexp.html"><a href="regexp.html#additional-resources"><i class="fa fa-check"></i><b>A.6</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixdata.html"><a href="appendixdata.html"><i class="fa fa-check"></i><b>B</b> Data</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixdata.html"><a href="appendixdata.html#hcandersen"><i class="fa fa-check"></i><b>B.1</b> Hans Christian Andersen fairy tales</a></li>
<li class="chapter" data-level="B.2" data-path="appendixdata.html"><a href="appendixdata.html#scotus-opinions"><i class="fa fa-check"></i><b>B.2</b> Opinions of the Supreme Court of the United States</a></li>
<li class="chapter" data-level="B.3" data-path="appendixdata.html"><a href="appendixdata.html#cfpb-complaints"><i class="fa fa-check"></i><b>B.3</b> Consumer Financial Protection Bureau (CFPB) complaints</a></li>
<li class="chapter" data-level="B.4" data-path="appendixdata.html"><a href="appendixdata.html#kickstarter-blurbs"><i class="fa fa-check"></i><b>B.4</b> Kickstarter campaign blurbs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendixbaseline.html"><a href="appendixbaseline.html"><i class="fa fa-check"></i><b>C</b> Baseline linear classifier</a>
<ul>
<li class="chapter" data-level="C.1" data-path="appendixbaseline.html"><a href="appendixbaseline.html#read-in-the-data"><i class="fa fa-check"></i><b>C.1</b> Read in the data</a></li>
<li class="chapter" data-level="C.2" data-path="appendixbaseline.html"><a href="appendixbaseline.html#split-into-testtrain-and-create-resampling-folds"><i class="fa fa-check"></i><b>C.2</b> Split into test/train and create resampling folds</a></li>
<li class="chapter" data-level="C.3" data-path="appendixbaseline.html"><a href="appendixbaseline.html#recipe-for-data-preprocessing"><i class="fa fa-check"></i><b>C.3</b> Recipe for data preprocessing</a></li>
<li class="chapter" data-level="C.4" data-path="appendixbaseline.html"><a href="appendixbaseline.html#lasso-regularized-classification-model"><i class="fa fa-check"></i><b>C.4</b> Lasso regularized classification model</a></li>
<li class="chapter" data-level="C.5" data-path="appendixbaseline.html"><a href="appendixbaseline.html#a-model-workflow"><i class="fa fa-check"></i><b>C.5</b> A model workflow</a></li>
<li class="chapter" data-level="C.6" data-path="appendixbaseline.html"><a href="appendixbaseline.html#tune-the-workflow"><i class="fa fa-check"></i><b>C.6</b> Tune the workflow</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning for Text Analysis in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stemming" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Stemming</h1>
<p>When we deal with text, often documents contain different versions of one base word, often called a <strong>stem</strong>. “The Fir-Tree,” for example, contains more than one version (i.e., inflected form) of the word <code>"tree"</code>.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="stemming.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hcandersenr)</span>
<span id="cb81-2"><a href="stemming.html#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb81-3"><a href="stemming.html#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidytext)</span>
<span id="cb81-4"><a href="stemming.html#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="stemming.html#cb81-5" aria-hidden="true" tabindex="-1"></a>fir_tree <span class="ot">&lt;-</span> <span class="fu">hca_fairytales</span>() <span class="sc">%&gt;%</span></span>
<span id="cb81-6"><a href="stemming.html#cb81-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(book <span class="sc">==</span> <span class="st">&quot;The fir tree&quot;</span>,</span>
<span id="cb81-7"><a href="stemming.html#cb81-7" aria-hidden="true" tabindex="-1"></a>         language <span class="sc">==</span> <span class="st">&quot;English&quot;</span>)</span>
<span id="cb81-8"><a href="stemming.html#cb81-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-9"><a href="stemming.html#cb81-9" aria-hidden="true" tabindex="-1"></a>tidy_fir_tree <span class="ot">&lt;-</span> fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb81-10"><a href="stemming.html#cb81-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text) <span class="sc">%&gt;%</span></span>
<span id="cb81-11"><a href="stemming.html#cb81-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(<span class="fu">get_stopwords</span>())</span>
<span id="cb81-12"><a href="stemming.html#cb81-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-13"><a href="stemming.html#cb81-13" aria-hidden="true" tabindex="-1"></a>tidy_fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb81-14"><a href="stemming.html#cb81-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(word, <span class="at">sort =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb81-15"><a href="stemming.html#cb81-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(word, <span class="st">&quot;^tree&quot;</span>))</span></code></pre></div>
<pre><code>#&gt; # A tibble: 3 x 2
#&gt;   word       n
#&gt;   &lt;chr&gt;  &lt;int&gt;
#&gt; 1 tree      76
#&gt; 2 trees     12
#&gt; 3 tree&#39;s     1</code></pre>
<p>Trees, we see once again, are important in this story; the singular form appears 76 times and the plural form appears twelve times. (We’ll come back to how we might handle the apostrophe in <code>"tree's"</code> later in this chapter.)</p>
<p>What if we aren’t interested in the difference between <code>"trees"</code> and <code>"tree"</code> and we want to treat both together? That idea is at the heart of <strong>stemming</strong>, the process of identifying the base word (or stem) for a data set of words. Stemming is concerned with the linguistics subfield of morphology, how words are formed. In this example, <code>"trees"</code> would lose its letter <code>"s"</code> while <code>"tree"</code> stays the same. If we counted word frequencies again after stemming, we would find that there are 88 occurrences of the stem <code>"tree"</code> (89, if we also find the stem for <code>"tree's"</code>).</p>
<div id="how-to-stem-text-in-r" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> How to stem text in R</h2>
<p>There have been many algorithms built for stemming words over the past half century or so; we’ll focus on two approaches. The first is the stemming algorithm of <span class="citation"><a href="#ref-Porter80" role="doc-biblioref">Martin F. Porter</a> (<a href="#ref-Porter80" role="doc-biblioref">1980</a>)</span>, probably the most widely used stemmer for English. Porter himself released the algorithm implemented in the framework <a href="https://snowballstem.org/">Snowball</a> with an open-source license; you can use it from R via the <a href="https://cran.r-project.org/package=SnowballC">SnowballC</a> package. (It has been extended to languages other than English as well.)</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="stemming.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SnowballC)</span>
<span id="cb83-2"><a href="stemming.html#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="stemming.html#cb83-3" aria-hidden="true" tabindex="-1"></a>tidy_fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb83-4"><a href="stemming.html#cb83-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb83-5"><a href="stemming.html#cb83-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(stem, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 570 x 2
#&gt;    stem        n
#&gt;    &lt;chr&gt;   &lt;int&gt;
#&gt;  1 tree       88
#&gt;  2 fir        34
#&gt;  3 littl      23
#&gt;  4 said       22
#&gt;  5 stori      16
#&gt;  6 thought    16
#&gt;  7 branch     15
#&gt;  8 on         15
#&gt;  9 came       14
#&gt; 10 know       14
#&gt; # … with 560 more rows</code></pre>
<p>Take a look at those stems. Notice that we do now have 88 incidences of <code>"tree"</code>. Also notice that some words don’t look like they are spelled as real words; this is normal and expected with this stemming algorithm. The Porter algorithm identifies the stem of both <code>"story"</code> and <code>"stories"</code> as <code>"stori"</code>, not a regular English word but instead a special stem object.</p>

<div class="rmdnote">
<p>If you want to tokenize <em>and</em> stem your text data, you can try out the function <code>tokenize_word_stems()</code> from the tokenizers package, which implements Porter stemming just like what we demonstrated here. For more on tokenization, see Chapter <a href="tokenization.html#tokenization">2</a>.</p>
</div>
<p>Does Porter stemming only work for English? Far from it! We can use the <code>language</code> argument to implement Porter stemming in multiple languages. First we can tokenize the text and <code>nest()</code> into list-columns.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="stemming.html#cb85-1" aria-hidden="true" tabindex="-1"></a>stopword_df <span class="ot">&lt;-</span> <span class="fu">tribble</span>(<span class="sc">~</span>language, <span class="sc">~</span>two_letter,</span>
<span id="cb85-2"><a href="stemming.html#cb85-2" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;danish&quot;</span>,  <span class="st">&quot;da&quot;</span>,</span>
<span id="cb85-3"><a href="stemming.html#cb85-3" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;english&quot;</span>, <span class="st">&quot;en&quot;</span>,</span>
<span id="cb85-4"><a href="stemming.html#cb85-4" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;french&quot;</span>,  <span class="st">&quot;fr&quot;</span>,</span>
<span id="cb85-5"><a href="stemming.html#cb85-5" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;german&quot;</span>,  <span class="st">&quot;de&quot;</span>,</span>
<span id="cb85-6"><a href="stemming.html#cb85-6" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;spanish&quot;</span>, <span class="st">&quot;es&quot;</span>)</span>
<span id="cb85-7"><a href="stemming.html#cb85-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-8"><a href="stemming.html#cb85-8" aria-hidden="true" tabindex="-1"></a>tidy_by_lang <span class="ot">&lt;-</span> <span class="fu">hca_fairytales</span>() <span class="sc">%&gt;%</span></span>
<span id="cb85-9"><a href="stemming.html#cb85-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(book <span class="sc">==</span> <span class="st">&quot;The fir tree&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb85-10"><a href="stemming.html#cb85-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(text, language) <span class="sc">%&gt;%</span></span>
<span id="cb85-11"><a href="stemming.html#cb85-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">language =</span> <span class="fu">str_to_lower</span>(language)) <span class="sc">%&gt;%</span></span>
<span id="cb85-12"><a href="stemming.html#cb85-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text) <span class="sc">%&gt;%</span></span>
<span id="cb85-13"><a href="stemming.html#cb85-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">data =</span> word)</span></code></pre></div>
<p>Then we can remove stop words (using <code>get_stopwords(language = "da")</code> and similar for each language) and stem with the language-specific Porter algorithm. What are the top 20 stems for “The Fir-Tree” in each of these five languages, after removing the Snowball stop words for that language?</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="stemming.html#cb86-1" aria-hidden="true" tabindex="-1"></a>tidy_by_lang <span class="sc">%&gt;%</span></span>
<span id="cb86-2"><a href="stemming.html#cb86-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(stopword_df) <span class="sc">%&gt;%</span></span>
<span id="cb86-3"><a href="stemming.html#cb86-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">data =</span> <span class="fu">map2</span>(</span>
<span id="cb86-4"><a href="stemming.html#cb86-4" aria-hidden="true" tabindex="-1"></a>    data, two_letter, <span class="sc">~</span> <span class="fu">anti_join</span>(.x, <span class="fu">get_stopwords</span>(<span class="at">language =</span> .y)))</span>
<span id="cb86-5"><a href="stemming.html#cb86-5" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb86-6"><a href="stemming.html#cb86-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(data) <span class="sc">%&gt;%</span></span>
<span id="cb86-7"><a href="stemming.html#cb86-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">wordStem</span>(word, <span class="at">language =</span> language)) <span class="sc">%&gt;%</span></span>
<span id="cb86-8"><a href="stemming.html#cb86-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(language) <span class="sc">%&gt;%</span></span>
<span id="cb86-9"><a href="stemming.html#cb86-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(stem) <span class="sc">%&gt;%</span></span>
<span id="cb86-10"><a href="stemming.html#cb86-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">top_n</span>(<span class="dv">20</span>, n) <span class="sc">%&gt;%</span></span>
<span id="cb86-11"><a href="stemming.html#cb86-11" aria-hidden="true" tabindex="-1"></a>  ungroup <span class="sc">%&gt;%</span></span>
<span id="cb86-12"><a href="stemming.html#cb86-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, <span class="fu">fct_reorder</span>(stem, n), <span class="at">fill =</span> language)) <span class="sc">+</span></span>
<span id="cb86-13"><a href="stemming.html#cb86-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb86-14"><a href="stemming.html#cb86-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>language, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>, <span class="at">ncol =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb86-15"><a href="stemming.html#cb86-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Frequency&quot;</span>, <span class="at">y =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:porterlanguages"></span>
<img src="04_stemming_files/figure-html/porterlanguages-1.png" alt="Porter stemming results in five languages" width="672" />
<p class="caption">
FIGURE 4.1: Porter stemming results in five languages
</p>
</div>
<p>Figure <a href="stemming.html#fig:porterlanguages">4.1</a> demonstrates some of the challenges in working with languages other English; the stop word lists may not be even from language to language, and tokenization strategies that work for a language like English may struggle for a language like French with more stop word contractions. Given that, we see here words about little fir trees at the top for all languages, in their stemmed forms.</p>
<p>The Porter stemmer is an algorithm that starts with a word and ends up with a single stem, but that’s not the only kind of stemmer out there. Another class of stemmer are dictionary-based stemmers. One such stemmer is the stemming algorithm of the <a href="http://hunspell.github.io/">Hunspell</a> library. The “Hun” in Hunspell stands for Hungarian; this set of NLP algorithms was originally written to handle Hungarian but has since been extended to handle many languages with compound words and complicated morphology. The Hunspell library is used mostly as a spell checker, but as part of identifying correct spellings, this library identifies word stems as well. You can use the Hunspell library from R via the <a href="https://cran.r-project.org/package=hunspell">hunspell</a> package.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="stemming.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(hunspell)</span>
<span id="cb87-2"><a href="stemming.html#cb87-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-3"><a href="stemming.html#cb87-3" aria-hidden="true" tabindex="-1"></a>tidy_fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb87-4"><a href="stemming.html#cb87-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">hunspell_stem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb87-5"><a href="stemming.html#cb87-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(stem) <span class="sc">%&gt;%</span></span>
<span id="cb87-6"><a href="stemming.html#cb87-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(stem, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 595 x 2
#&gt;    stem       n
#&gt;    &lt;chr&gt;  &lt;int&gt;
#&gt;  1 tree      89
#&gt;  2 fir       34
#&gt;  3 little    23
#&gt;  4 said      22
#&gt;  5 story     16
#&gt;  6 branch    15
#&gt;  7 one       15
#&gt;  8 came      14
#&gt;  9 know      14
#&gt; 10 now       14
#&gt; # … with 585 more rows</code></pre>
<p>Notice that the code here is a little different (we had to use <code>unnest()</code>) and that the results are a little different. We have only real English words, and we have more total rows in the result. What happened?</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="stemming.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hunspell_stem</span>(<span class="st">&quot;discontented&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; [[1]]
#&gt; [1] &quot;contented&quot; &quot;content&quot;</code></pre>
<p>We have <em>two</em> stems! This stemmer works differently; it uses both morphological analysis of a word and existing dictionaries to find possible stems. It’s possible to end up with more than one, and it’s possible for a stem to be a word that is not related by meaning to the original word. For example, one of the stems of “number” is “numb” with this library. The Hunspell library was built to be a spell checker, so depending on your analytical purposes, it may not be an appropriate choice.</p>
</div>
<div id="should-you-use-stemming-at-all" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Should you use stemming at all?</h2>
<p>You will often see stemming as part of NLP pipelines, sometimes without much comment about when it is helpful or not. We encourage you to think of stemming as a preprocessing step in text modeling, one that must be thought through and chosen (or not) with good judgment.</p>
<p>Why does stemming often help, if you are training a machine learning model for text? Stemming <em>reduces the feature space</em> of text data. Let’s see this in action, with a data set of United States Supreme Court opinions available in the <a href="https://github.com/EmilHvitfeldt/scotus"><strong>scotus</strong></a> package. How many words are there, after removing a standard data set of stopwords?</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="stemming.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(scotus)</span>
<span id="cb91-2"><a href="stemming.html#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="stemming.html#cb91-3" aria-hidden="true" tabindex="-1"></a>tidy_scotus <span class="ot">&lt;-</span> scotus_filtered <span class="sc">%&gt;%</span></span>
<span id="cb91-4"><a href="stemming.html#cb91-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text) <span class="sc">%&gt;%</span></span>
<span id="cb91-5"><a href="stemming.html#cb91-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(<span class="fu">get_stopwords</span>())</span>
<span id="cb91-6"><a href="stemming.html#cb91-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-7"><a href="stemming.html#cb91-7" aria-hidden="true" tabindex="-1"></a>tidy_scotus <span class="sc">%&gt;%</span></span>
<span id="cb91-8"><a href="stemming.html#cb91-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(word, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 167,879 x 2
#&gt;    word        n
#&gt;    &lt;chr&gt;   &lt;int&gt;
#&gt;  1 court  286448
#&gt;  2 v      204176
#&gt;  3 state  148320
#&gt;  4 states 128160
#&gt;  5 case   121439
#&gt;  6 act    111033
#&gt;  7 s.ct   108168
#&gt;  8 u.s    106413
#&gt;  9 upon   105069
#&gt; 10 united 103267
#&gt; # … with 167,869 more rows</code></pre>
<p>There are 167,879 distinct words in this data set we have created (after removing stopwords) but notice that even in the most common words we see a pair like <code>"state"</code> and <code>"states"</code>. A common data structure for modeling, and a helpful mental model for thinking about the sparsity of text data, is a matrix. Let’s <code>cast()</code> this tidy data to a sparse matrix (technically, a document-feature matrix object from the <a href="https://cran.r-project.org/package=quanteda">quanteda</a> package).</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="stemming.html#cb93-1" aria-hidden="true" tabindex="-1"></a>tidy_scotus <span class="sc">%&gt;%</span></span>
<span id="cb93-2"><a href="stemming.html#cb93-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(case_name, word) <span class="sc">%&gt;%</span></span>
<span id="cb93-3"><a href="stemming.html#cb93-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_dfm</span>(case_name, word, n)</span></code></pre></div>
<pre><code>#&gt; Document-feature matrix of: 9,642 documents, 167,879 features (99.5% sparse).</code></pre>
<p>Look at the sparsity of this matrix. It’s high! Think of this sparsity as the sparsity of data that we will want to use to build a supervised machine learning model.</p>
<p>What if instead we use stemming as a preprocessing step here?</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="stemming.html#cb95-1" aria-hidden="true" tabindex="-1"></a>tidy_scotus <span class="sc">%&gt;%</span></span>
<span id="cb95-2"><a href="stemming.html#cb95-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb95-3"><a href="stemming.html#cb95-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(case_name, stem) <span class="sc">%&gt;%</span></span>
<span id="cb95-4"><a href="stemming.html#cb95-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_dfm</span>(case_name, stem, n)</span></code></pre></div>
<pre><code>#&gt; Document-feature matrix of: 9,642 documents, 135,570 features (99.5% sparse).</code></pre>
<p>We reduced the number of word features by many thousands, although the sparsity did not change much. Why is it possibly helpful to reduce the number of features? Common sense says that reducing the number of word features in our data set so dramatically will improve the performance of any machine learning model we train with it, <em>assuming that we haven’t lost any important information by stemming</em>.</p>
<p>There is a growing body of academic research demonstrating that stemming can be counterproductive for text modeling. For example, <span class="citation"><a href="#ref-Schofield16" role="doc-biblioref">Schofield and Mimno</a> (<a href="#ref-Schofield16" role="doc-biblioref">2016</a>)</span> and related work explore how choices around stemming and other preprocessing steps don’t help and can actually hurt performance when training topic models for text. From <span class="citation"><a href="#ref-Schofield16" role="doc-biblioref">Schofield and Mimno</a> (<a href="#ref-Schofield16" role="doc-biblioref">2016</a>)</span> specifically,</p>
<blockquote>
<p>Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability.</p>
</blockquote>
<p>Topic modeling is an example of unsupervised machine learning for text and is not the same as the predictive modeling approaches we’ll be focusing on in this book, but the lesson remains that stemming may or may not be beneficial for any specific context. As we work through the rest of this chapter and learn more about stemming, consider what information we lose when we stem text in exchange for reducing the number of word features. Stemming can be helpful in some contexts, but typical stemming algorithms are somewhat aggressive and have been built to favor sensitivity (or recall, or the true positive rate) at the expense of specificity (or precision, or the true negative rate).</p>
<p>Most common stemming algorithms you are likely to encounter will successfully reduce words to stems (i.e., not leave extraneous word endings on the words) but at the expense of collapsing some words with dramatic differences in meaning, semantics, use, etc. to the same stems. Examples of the latter are numerous, but some include:</p>
<ul>
<li>meaning and mean</li>
<li>likely, like, liking</li>
<li>university and universe</li>
</ul>
<p>In a supervised machine learning context, this affects a model’s positive predictive value (precision), or ability to not incorrectly label true negatives as positive. In Chapter <a href="mlclassification.html#mlclassification">7</a>, we will train models to predict whether a complaint to the United States Consumer Financial Protection Bureau was about a mortgage or not. Stemming can increase a model’s ability to find the positive examples, i.e., the complaints about mortgages. However, if the complaint text is over-stemmed, the resulting model loses its ability to label the negative examples, the complaints <em>not</em> about mortgages, correctly.</p>
</div>
<div id="understand-a-stemming-algorithm" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Understand a stemming algorithm</h2>
<p>If stemming is going to be in our NLP toolbox, it’s worth sitting down with one approach in detail to understand how it works under the hood. The Porter stemming algorithm is so approachable that we can walk through its outline in less than a page or so. It involves five steps, and the idea of a word <strong>measure</strong>.</p>
<p>Think of any word as made up alternating groups of vowels <span class="math inline">\(V\)</span> and consonants <span class="math inline">\(C\)</span>. One or more vowels together are one instance of <span class="math inline">\(V\)</span>, and one or more consonants togther are one instance of <span class="math inline">\(C\)</span>. We can write any word as</p>
<p><span class="math display">\[[C](VC)^m[V]\]</span>
where <span class="math inline">\(m\)</span> is called the “measure” of the word. The first <span class="math inline">\(C\)</span> and the last <span class="math inline">\(V\)</span> in brackets are optional. In this framework, we could write out the word <code>"tree"</code> as</p>
<p><span class="math display">\[CV\]</span></p>
<p>with <span class="math inline">\(C\)</span> being “tr” and <span class="math inline">\(V\)</span> being “ee”; it’s an <code>m = 0</code> word. We would write out the word <code>"algorithms"</code> as</p>
<p><span class="math display">\[VCVCVC\]</span>
and it is an <code>m = 3</code> word.</p>
<ul>
<li>The first step of the Porter stemmer is (perhaps this seems like cheating) actually made of three substeps working with plural and past participle word endings. In the first substep (1a), “sses” is replaced with “ss,” “ies” is replaced with “i,” and final single “s” letters are removed. The second substep (1b) depends on the measure of the word <code>m</code> but works with endings like “eed,” “ed,” “ing,” adding “e” back on to make endings like “ate,” “ble,” and “ize” when appropriate. The third substep (1c) replaces “y” with “i” for words of a certain <code>m</code>.</li>
<li>The second step of the Porter stemmer takes the output of the first step and regularizes a set of 20 endings. In this step, “ization” goes to “ize,” “alism” goes to “al,” “aliti” goes to “al” (notice that the ending “i” there came from the first step), and so on for the other 17 endings.</li>
<li>The third step again processes the output, using a list of seven endings. Here, “ical” and “iciti” both go to “ic,” “ful” and “ness” are both removed, and so forth for the three other endings in this step.</li>
<li>The fourth step involves a longer list of endings to deal with again (19), and they are all removed. Endings like “ent,” “ism,” “ment,” and more are removed in this step.</li>
<li>The fifth and final step has two substeps, both which depend on the measure <code>m</code> of the word. In this step, depending on <code>m</code>, final “e” letters are sometimes removed and final double letters are sometimes removed.</li>
</ul>
<div class="rmdnote">
<p>
How would this work for a few example words? The word “supervised” loses its “ed” in step 1b and is not touched by the rest of the algorithm, ending at “supervis.” The word “relational” changes “ational” to “ate” in step 2 and loses its final “e” in step 5, ending at “relat.” Notice that neither of these results are regular English words, but instead special stem objects. This is expected.
</p>
</div>
<p>This algorithm was first published in <span class="citation"><a href="#ref-Porter80" role="doc-biblioref">Martin F. Porter</a> (<a href="#ref-Porter80" role="doc-biblioref">1980</a>)</span> and is still broadly used; read <span class="citation"><a href="#ref-Willett06" role="doc-biblioref">Willett</a> (<a href="#ref-Willett06" role="doc-biblioref">2006</a>)</span> for background on how and why it has become a stemming standard. We can reach even <em>further</em> back and examine what is considered the first ever published stemming algorithm in <span class="citation"><a href="#ref-Lovins68" role="doc-biblioref">Lovins</a> (<a href="#ref-Lovins68" role="doc-biblioref">1968</a>)</span>. The domain Lovins worked in was engineering, so her approach was particularly suited to technical terms. This algorithm uses much larger lists of word endings, conditions, and rules than the Porter algorithm and, although considered old-fashioned, is actually faster!</p>
<div class="rmdwarning">
<p>
Check out the <a href="https://snowballstem.org/algorithms/german/stemmer.html">steps of a Snowball stemming algorithm for German</a>.
</p>
</div>
</div>
<div id="handling-punctuation-when-stemming" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Handling punctuation when stemming</h2>
<p>Punctuation contains information that can be used in text analysis. Punctuation <em>is</em> typically less information-dense than the words themselves and thus it is often removed early in a text mining analysis project, but it’s worth thinking through the impact of punctuation specifically on stemming. Think about words like <code>"they're"</code> and <code>"child's"</code>.</p>
<p>We’ve already seen how punctuation and stemming can interact with our small example of “The Fir-Tree”; none of the stemming strategies we’ve discussed so far have recognized <code>"tree's"</code> as belonging to the same stem as <code>"trees"</code> and <code>"tree"</code>.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="stemming.html#cb97-1" aria-hidden="true" tabindex="-1"></a>tidy_fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb97-2"><a href="stemming.html#cb97-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(word, <span class="at">sort =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb97-3"><a href="stemming.html#cb97-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(word, <span class="st">&quot;^tree&quot;</span>))</span></code></pre></div>
<pre><code>#&gt; # A tibble: 3 x 2
#&gt;   word       n
#&gt;   &lt;chr&gt;  &lt;int&gt;
#&gt; 1 tree      76
#&gt; 2 trees     12
#&gt; 3 tree&#39;s     1</code></pre>
<p>It is possible to split tokens not only on white space but <strong>also</strong> on punctuation, using a regular expression (see Appendix <a href="regexp.html#regexp">A</a>).</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="stemming.html#cb99-1" aria-hidden="true" tabindex="-1"></a>fir_tree_counts <span class="ot">&lt;-</span> fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb99-2"><a href="stemming.html#cb99-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text, <span class="at">token =</span> <span class="st">&quot;regex&quot;</span>, <span class="at">pattern =</span> <span class="st">&quot;</span><span class="sc">\\</span><span class="st">s+|[[:punct:]]+&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb99-3"><a href="stemming.html#cb99-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(<span class="fu">get_stopwords</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb99-4"><a href="stemming.html#cb99-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb99-5"><a href="stemming.html#cb99-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(stem, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span>
<span id="cb99-6"><a href="stemming.html#cb99-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-7"><a href="stemming.html#cb99-7" aria-hidden="true" tabindex="-1"></a>fir_tree_counts</span></code></pre></div>
<pre><code>#&gt; # A tibble: 572 x 2
#&gt;    stem        n
#&gt;    &lt;chr&gt;   &lt;int&gt;
#&gt;  1 tree       89
#&gt;  2 fir        34
#&gt;  3 littl      23
#&gt;  4 said       22
#&gt;  5 stori      16
#&gt;  6 thought    16
#&gt;  7 branch     15
#&gt;  8 on         15
#&gt;  9 came       14
#&gt; 10 know       14
#&gt; # … with 562 more rows</code></pre>
<p>Now we are able to put all these related words together, having identified them with the same stem.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="stemming.html#cb101-1" aria-hidden="true" tabindex="-1"></a>fir_tree_counts <span class="sc">%&gt;%</span></span>
<span id="cb101-2"><a href="stemming.html#cb101-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(stem, <span class="st">&quot;^tree&quot;</span>))</span></code></pre></div>
<pre><code>#&gt; # A tibble: 1 x 2
#&gt;   stem      n
#&gt;   &lt;chr&gt; &lt;int&gt;
#&gt; 1 tree     89</code></pre>
<p>Handling punctuation in this way further reduces sparsity in word features. Whether this kind of tokenization and stemming strategy is a good choice in any particular data analysis situation depends on the particulars of the text characteristics.</p>
</div>
<div id="compare-some-stemming-options" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Compare some stemming options</h2>
<p>Let’s compare a few simple stemming algorithms and see what results we end with. Let’s look at “The Fir-Tree,” specifically the tidied data set from which we have removed stop words. Let’s compare three very straightforward stemming approaches.</p>
<ul>
<li><strong>Only remove final instances of the letter “s.”</strong> This probably strikes you as not a great idea after our discussion in this chapter, but it is something that people try in real life, so let’s see what the impact is.</li>
<li><strong>Handle plural endings with slightly more complex rules in the “S” stemmer.</strong> The S-removal stemmer or “S” stemmer of <span class="citation"><a href="#ref-Harman91" role="doc-biblioref">Harman</a> (<a href="#ref-Harman91" role="doc-biblioref">1991</a>)</span> is a simple algorithm with only three rules.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></li>
<li><strong>Implement actual Porter stemming.</strong> We can now compare to the most commonly used stemming algorithm in English.</li>
</ul>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="stemming.html#cb103-1" aria-hidden="true" tabindex="-1"></a>stemming <span class="ot">&lt;-</span> tidy_fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb103-2"><a href="stemming.html#cb103-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>book, <span class="sc">-</span>language) <span class="sc">%&gt;%</span></span>
<span id="cb103-3"><a href="stemming.html#cb103-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="st">`</span><span class="at">Remove S</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">str_remove</span>(word, <span class="st">&quot;s$&quot;</span>),</span>
<span id="cb103-4"><a href="stemming.html#cb103-4" aria-hidden="true" tabindex="-1"></a>         <span class="st">`</span><span class="at">Plural endings</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">case_when</span>(<span class="fu">str_detect</span>(word, <span class="st">&quot;[^e|aies$]ies$&quot;</span>) <span class="sc">~</span></span>
<span id="cb103-5"><a href="stemming.html#cb103-5" aria-hidden="true" tabindex="-1"></a>                                        <span class="fu">str_replace</span>(word, <span class="st">&quot;ies$&quot;</span>, <span class="st">&quot;y&quot;</span>),</span>
<span id="cb103-6"><a href="stemming.html#cb103-6" aria-hidden="true" tabindex="-1"></a>                                      <span class="fu">str_detect</span>(word, <span class="st">&quot;[^e|a|oes$]es$&quot;</span>) <span class="sc">~</span></span>
<span id="cb103-7"><a href="stemming.html#cb103-7" aria-hidden="true" tabindex="-1"></a>                                        <span class="fu">str_replace</span>(word, <span class="st">&quot;es$&quot;</span>, <span class="st">&quot;e&quot;</span>),</span>
<span id="cb103-8"><a href="stemming.html#cb103-8" aria-hidden="true" tabindex="-1"></a>                                      <span class="fu">str_detect</span>(word, <span class="st">&quot;[^ss$|us$]s$&quot;</span>) <span class="sc">~</span></span>
<span id="cb103-9"><a href="stemming.html#cb103-9" aria-hidden="true" tabindex="-1"></a>                                        <span class="fu">str_remove</span>(word, <span class="st">&quot;s$&quot;</span>),</span>
<span id="cb103-10"><a href="stemming.html#cb103-10" aria-hidden="true" tabindex="-1"></a>                                      <span class="cn">TRUE</span> <span class="sc">~</span> word),</span>
<span id="cb103-11"><a href="stemming.html#cb103-11" aria-hidden="true" tabindex="-1"></a>         <span class="st">`</span><span class="at">Porter stemming</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb103-12"><a href="stemming.html#cb103-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="st">`</span><span class="at">Original word</span><span class="st">`</span> <span class="ot">=</span> word)</span></code></pre></div>
<p>Figure <a href="stemming.html#fig:stemmingresults">4.2</a> shows the results of these stemming strategies. All successfully handled the transition from <code>"trees"</code> to <code>"tree"</code> in the same way, but we have different results for <code>"stories"</code> to <code>"story"</code> or <code>"stori"</code>, different handling of <code>"branches"</code>, and more. There are subtle differences in the output of even these straightforward stemming approaches that can effect the transformation of text features for modeling.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="stemming.html#cb104-1" aria-hidden="true" tabindex="-1"></a>stemming <span class="sc">%&gt;%</span></span>
<span id="cb104-2"><a href="stemming.html#cb104-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(Type, Result, <span class="st">`</span><span class="at">Remove S</span><span class="st">`</span><span class="sc">:</span><span class="st">`</span><span class="at">Porter stemming</span><span class="st">`</span>) <span class="sc">%&gt;%</span></span>
<span id="cb104-3"><a href="stemming.html#cb104-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Type =</span> <span class="fu">fct_inorder</span>(Type)) <span class="sc">%&gt;%</span></span>
<span id="cb104-4"><a href="stemming.html#cb104-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(Type, Result) <span class="sc">%&gt;%</span></span>
<span id="cb104-5"><a href="stemming.html#cb104-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Type) <span class="sc">%&gt;%</span></span>
<span id="cb104-6"><a href="stemming.html#cb104-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">top_n</span>(<span class="dv">20</span>, n) <span class="sc">%&gt;%</span></span>
<span id="cb104-7"><a href="stemming.html#cb104-7" aria-hidden="true" tabindex="-1"></a>  ungroup <span class="sc">%&gt;%</span></span>
<span id="cb104-8"><a href="stemming.html#cb104-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="fu">fct_reorder</span>(Result, n),</span>
<span id="cb104-9"><a href="stemming.html#cb104-9" aria-hidden="true" tabindex="-1"></a>             n, <span class="at">fill =</span> Type)) <span class="sc">+</span></span>
<span id="cb104-10"><a href="stemming.html#cb104-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb104-11"><a href="stemming.html#cb104-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>Type, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb104-12"><a href="stemming.html#cb104-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb104-13"><a href="stemming.html#cb104-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="cn">NULL</span>, <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:stemmingresults"></span>
<img src="04_stemming_files/figure-html/stemmingresults-1.png" alt="Results for three different stemming strategies" width="672" />
<p class="caption">
FIGURE 4.2: Results for three different stemming strategies
</p>
</div>
<p>Porter stemming is the most different from the other two approaches. In the top twenty words here, we don’t see a difference between removing only the letter “s” and taking the slightly more sophisticated “S” stemmer approach to plural endings. In what situations <em>do</em> we see a difference?</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="stemming.html#cb105-1" aria-hidden="true" tabindex="-1"></a>stemming <span class="sc">%&gt;%</span></span>
<span id="cb105-2"><a href="stemming.html#cb105-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="st">`</span><span class="at">Remove S</span><span class="st">`</span> <span class="sc">!=</span> <span class="st">`</span><span class="at">Plural endings</span><span class="st">`</span>) <span class="sc">%&gt;%</span></span>
<span id="cb105-3"><a href="stemming.html#cb105-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>(<span class="st">`</span><span class="at">Remove S</span><span class="st">`</span>, <span class="st">`</span><span class="at">Plural endings</span><span class="st">`</span>, <span class="at">.keep_all =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 13 x 4
#&gt;    `Original word` `Remove S`  `Plural endings` `Porter stemming`
#&gt;    &lt;chr&gt;           &lt;chr&gt;       &lt;chr&gt;            &lt;chr&gt;            
#&gt;  1 raspberries     raspberrie  raspberry        raspberri        
#&gt;  2 strawberries    strawberrie strawberry       strawberri       
#&gt;  3 less            les         less             less             
#&gt;  4 us              u           us               u                
#&gt;  5 brightness      brightnes   brightness       bright           
#&gt;  6 conscious       consciou    conscious        consciou         
#&gt;  7 faintness       faintnes    faintness        faint            
#&gt;  8 happiness       happines    happiness        happi            
#&gt;  9 ladies          ladie       lady             ladi             
#&gt; 10 babies          babie       baby             babi             
#&gt; 11 anxious         anxiou      anxious          anxiou           
#&gt; 12 princess        princes     princess         princess         
#&gt; 13 stories         storie      story            stori</code></pre>
<p>We also see situations where the same sets of original words are bucketed differently (not just with different stem labels) under different stemming strategies. In the following very small example, two of the strategies bucket these words into two stems while one strategy buckets them into one stem.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="stemming.html#cb107-1" aria-hidden="true" tabindex="-1"></a>stemming <span class="sc">%&gt;%</span></span>
<span id="cb107-2"><a href="stemming.html#cb107-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gather</span>(Type, Result, <span class="st">`</span><span class="at">Remove S</span><span class="st">`</span><span class="sc">:</span><span class="st">`</span><span class="at">Porter stemming</span><span class="st">`</span>) <span class="sc">%&gt;%</span></span>
<span id="cb107-3"><a href="stemming.html#cb107-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Result <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;come&quot;</span>, <span class="st">&quot;coming&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb107-4"><a href="stemming.html#cb107-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">distinct</span>(<span class="st">`</span><span class="at">Original word</span><span class="st">`</span>, Type, Result)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 9 x 3
#&gt;   `Original word` Type            Result
#&gt;   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt; 
#&gt; 1 come            Remove S        come  
#&gt; 2 comes           Remove S        come  
#&gt; 3 coming          Remove S        coming
#&gt; 4 come            Plural endings  come  
#&gt; 5 comes           Plural endings  come  
#&gt; 6 coming          Plural endings  coming
#&gt; 7 come            Porter stemming come  
#&gt; 8 comes           Porter stemming come  
#&gt; 9 coming          Porter stemming come</code></pre>
<p>These different characteristics can either be positive or negative, depending on the nature of the text being modeled and the analytical question being pursued.</p>
<div class="rmdwarning">
<p>
Language use is connected to culture and identity. How might the results of stemming strategies be different for text created with the same language (like English) but in different social or cultural contexts, or by people with different identities? With what kind of text do you think stemming algorithms behave most consistently, or most as expected? What impact might that have on text modeling?
</p>
</div>
</div>
<div id="lemmatization" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Lemmatization and stemming</h2>
<p>When people use the word “stemming” in natural language processing, they typically mean a system like the one we’ve been describing in this chapter, with rules, conditions, heuristics, and lists of word endings. Think of stemming as typically implemented in NLP as <strong>rule-based</strong>, operating on the word by itself. There is another option for normalizing words to a root that takes a different approach. Instead of using rules to cut words down to their stems, lemmatization uses knowledge about a language’s structure to reduce words down to their lemmas, the canonical or dictionary forms of words. Think of lemmatization as typically implemented in NLP as <strong>linguistics-based</strong>, operating on the word in its context.</p>
<p>Lemmatization requires more information than the rule-based stemmers we’ve discussed so far. We need to know what part of speech a word is to correctly identify its lemma,<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> and we also need more information about what words mean in their contexts. Often lemmatizers use a rich lexical database like <a href="https://wordnet.princeton.edu/">WordNet</a> as a way to look up word meanings for a given part-of-speech use <span class="citation">(<a href="#ref-Miller95" role="doc-biblioref">Miller 1995</a>)</span>. Notice that lemmatization involves more linguistic knowledge of a language than stemming.</p>
<div class="rmdnote">
<p>
How does lemmatization work in languages other than English? Lookup dictionaries connecting words, lemmas, and parts of speech for languages other than English have been developed as well.
</p>
</div>
<p>A modern, efficient implementation for lemmatization is available in the excellent <a href="https://spacy.io/">spaCy</a> library <span class="citation">(<a href="#ref-spacy2" role="doc-biblioref">Honnibal and Montani 2017</a>)</span>, which is written in Python.</p>

<div class="rmdpackage">
NLP practitioners who work with R can use this library via the <a href="http://spacyr.quanteda.io/"><strong>spacyr</strong></a> package <span class="citation">(<a href="#ref-Benoit19" role="doc-biblioref">Benoit and Matsuo 2019</a>)</span>, the <a href="https://statsmaths.github.io/cleanNLP/"><strong>cleanNLP</strong></a> package <span class="citation">(<a href="#ref-Arnold17" role="doc-biblioref">Arnold 2017</a>)</span>, or as an “engine” in the <a href="https://textrecipes.tidymodels.org/"><strong>textrecipes</strong></a> package <span class="citation">(<a href="#ref-textrecipes" role="doc-biblioref">Hvitfeldt 2020b</a>)</span>.
</div>
<p>Section <a href="mlregression.html#mlregressionlemmatization">6.6</a> demonstrates how to use textrecipes with spaCy as an engine and include lemmas as features for modeling. You might also consider using spaCy directly in R Markdown <a href="https://rstudio.github.io/reticulate/articles/r_markdown.html">via its Python engine</a>.</p>
<p>Let’s briefly walk through how to use spacyr.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="stemming.html#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(spacyr)</span>
<span id="cb109-2"><a href="stemming.html#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="fu">spacy_initialize</span>(<span class="at">entity =</span> <span class="cn">FALSE</span>)</span>
<span id="cb109-3"><a href="stemming.html#cb109-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-4"><a href="stemming.html#cb109-4" aria-hidden="true" tabindex="-1"></a>fir_tree <span class="sc">%&gt;%</span></span>
<span id="cb109-5"><a href="stemming.html#cb109-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">doc_id =</span> <span class="fu">paste0</span>(<span class="st">&quot;doc&quot;</span>, <span class="fu">row_number</span>())) <span class="sc">%&gt;%</span></span>
<span id="cb109-6"><a href="stemming.html#cb109-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(doc_id, <span class="fu">everything</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb109-7"><a href="stemming.html#cb109-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">spacy_parse</span>() <span class="sc">%&gt;%</span></span>
<span id="cb109-8"><a href="stemming.html#cb109-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(<span class="fu">get_stopwords</span>(), <span class="at">by =</span> <span class="fu">c</span>(<span class="st">&quot;lemma&quot;</span> <span class="ot">=</span> <span class="st">&quot;word&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb109-9"><a href="stemming.html#cb109-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(lemma, <span class="at">sort =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb109-10"><a href="stemming.html#cb109-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">top_n</span>(<span class="dv">20</span>, n) <span class="sc">%&gt;%</span></span>
<span id="cb109-11"><a href="stemming.html#cb109-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, <span class="fu">fct_reorder</span>(lemma, n))) <span class="sc">+</span></span>
<span id="cb109-12"><a href="stemming.html#cb109-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb109-13"><a href="stemming.html#cb109-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Frequency&quot;</span>, <span class="at">y =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:lemmafirtree"></span>
<img src="04_stemming_files/figure-html/lemmafirtree-1.png" alt="Results for lemmatization, rather than stemming" width="672" />
<p class="caption">
FIGURE 4.3: Results for lemmatization, rather than stemming
</p>
</div>
<p>Figure <a href="stemming.html#fig:lemmafirtree">4.3</a> demonstrates how different lemmatization is from stemming, especially is we compare to Figure <a href="stemming.html#fig:stemmingresults">4.2</a>. Punctuation characters are treated as tokens (these punctuation tokens can have predictive power for some modeling questions!) and all pronouns are lemmatized to <code>-PRON-</code>. We see our familiar friends “tree” and “fir,” but notice that we see the normalized version “say” instead of “said,” “come” instead of “came,” and similar. This transformation to the canonical or dictionary form of words is the goal of lemmatization.</p>
<div class="rmdnote">
<p>
Why did we need to initialize the spaCy library? You may not need to, but spaCy is a full-featured NLP pipeline that not only tokenizes and identifies lemmas but also performs entity recognition. We will not use entity recognition in modeling or analysis in this book and it takes a lot of computational power. Initializing with <code>entity = FALSE</code> will allow lemmatization to run much faster.
</p>
</div>
<p>Implementing lemmatization is slower and more complex than stemming. Just like with stemming, lemmatization often improves the true positive rate (or recall) but at the expense of the true negative rate (or precision) compared to not using lemmatization, but typically less so than stemming.</p>
</div>
<div id="stemming-and-stop-words" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Stemming and stop words</h2>
<p>Our deep dive into stemming came <em>after</em> our chapters on tokenization (Chapter <a href="tokenization.html#tokenization">2</a>) and stop words (Chapter <a href="stopwords.html#stopwords">3</a>) because this is typically when you will want to implement stemming, if appropriate to your analytical question. Stop word lists are usually unstemmed, so you need to remove stop words before stemming text data. For example, the Porter stemming algorithm transforms words like <code>"themselves"</code> to <code>"themselv"</code>, so stemming first would leave you without the ability to match up to the commonly used stop word lexicons.</p>
<p>A handy trick is to use the following function on your stop word list to return the words that don’t have a stemmed version in the list. If the function returns a length 0 vector then you can stem and remove stop words in any order.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="stemming.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stopwords)</span>
<span id="cb110-2"><a href="stemming.html#cb110-2" aria-hidden="true" tabindex="-1"></a>not_stemmed_in <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb110-3"><a href="stemming.html#cb110-3" aria-hidden="true" tabindex="-1"></a>  x[<span class="sc">!</span>SnowballC<span class="sc">::</span><span class="fu">wordStem</span>(x) <span class="sc">%in%</span> x]</span>
<span id="cb110-4"><a href="stemming.html#cb110-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-5"><a href="stemming.html#cb110-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-6"><a href="stemming.html#cb110-6" aria-hidden="true" tabindex="-1"></a><span class="fu">not_stemmed_in</span>(<span class="fu">stopwords</span>(<span class="at">source =</span> <span class="st">&quot;snowball&quot;</span>))</span></code></pre></div>
<pre><code>#&gt;  [1] &quot;ourselves&quot;  &quot;yourselves&quot; &quot;his&quot;        &quot;they&quot;       &quot;themselves&quot;
#&gt;  [6] &quot;this&quot;       &quot;are&quot;        &quot;was&quot;        &quot;has&quot;        &quot;does&quot;      
#&gt; [11] &quot;you&#39;re&quot;     &quot;he&#39;s&quot;       &quot;she&#39;s&quot;      &quot;it&#39;s&quot;       &quot;we&#39;re&quot;     
#&gt; [16] &quot;they&#39;re&quot;    &quot;i&#39;ve&quot;       &quot;you&#39;ve&quot;     &quot;we&#39;ve&quot;      &quot;they&#39;ve&quot;   
#&gt; [21] &quot;let&#39;s&quot;      &quot;that&#39;s&quot;     &quot;who&#39;s&quot;      &quot;what&#39;s&quot;     &quot;here&#39;s&quot;    
#&gt; [26] &quot;there&#39;s&quot;    &quot;when&#39;s&quot;     &quot;where&#39;s&quot;    &quot;why&#39;s&quot;      &quot;how&#39;s&quot;     
#&gt; [31] &quot;because&quot;    &quot;during&quot;     &quot;before&quot;     &quot;above&quot;      &quot;once&quot;      
#&gt; [36] &quot;any&quot;        &quot;only&quot;       &quot;very&quot;</code></pre>
<p>Here we see that many of the words that are lost are the contractions.</p>

<div class="rmdwarning">
In Section <a href="stopwords.html#homemadestopwords">3.2</a>, we explored whether to include “tree” as a stop word for “The Fir-Tree.” Now we can understand that this is more complicated than we first discussed, because there are different versions of the base word (“trees,” “tree’s”) in our data set. Interactions between preprocessing steps can have a major impact on your analysis.
</div>
</div>
<div id="stemmingsummary" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> Summary</h2>
<p>In this chapter, we explored stemming, the practice of identifying and extracting the base or stem for a word using rules and heuristics. Stemming reduces the sparsity of text data which can be helpful when training models, but at the cost of throwing information away. Lemmatization is another way to normalize words to a root, based on language structure and how words are used in their context.</p>
<div id="in-this-chapter-you-learned-3" class="section level3" number="4.8.1">
<h3><span class="header-section-number">4.8.1</span> In this chapter, you learned:</h3>
<ul>
<li>about the most broadly used stemming algorithms</li>
<li>how to implement stemming</li>
<li>that stemming changes the sparsity or feature space of text data</li>
<li>the differences between stemming and lemmatization</li>
</ul>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Arnold17" class="csl-entry">
Arnold, Taylor. 2017. <span>“A Tidy Data Model for Natural Language Processing Using cleanNLP.”</span> <em>The R Journal</em> 9 (2): 1–20. <a href="https://journal.r-project.org/archive/2017/RJ-2017-035/index.html">https://journal.r-project.org/archive/2017/RJ-2017-035/index.html</a>.
</div>
<div id="ref-Benoit19" class="csl-entry">
Benoit, Kenneth, and Akitaka Matsuo. 2019. <em>Spacyr: Wrapper to the ’spaCy’ ’NLP’ Library</em>. <a href="https://CRAN.R-project.org/package=spacyr">https://CRAN.R-project.org/package=spacyr</a>.
</div>
<div id="ref-Harman91" class="csl-entry">
Harman, Donna. 1991. <span>“How Effective Is Suffixing?”</span> <em>Journal of the American Society for Information Science</em> 42 (1): 7–15.
</div>
<div id="ref-spacy2" class="csl-entry">
Honnibal, Matthew, and Ines Montani. 2017. <span>“<span class="nocase">spaCy 2</span>: Natural Language Understanding with <span>B</span>loom Embeddings, Convolutional Neural Networks and Incremental Parsing.”</span>
</div>
<div id="ref-textrecipes" class="csl-entry">
———. 2020b. <em>Textrecipes: Extra ’Recipes’ for Text Processing</em>. <a href="https://CRAN.R-project.org/package=textrecipes">https://CRAN.R-project.org/package=textrecipes</a>.
</div>
<div id="ref-Lovins68" class="csl-entry">
Lovins, Julie B. 1968. <span>“Development of a Stemming Algorithm.”</span> <em>Mechanical Translation and Computational Linguistics</em> 11: 22–31.
</div>
<div id="ref-Miller95" class="csl-entry">
Miller, George A. 1995. <span>“WordNet: A Lexical Database for English.”</span> <em>Commun. ACM</em> 38 (11): 39–41. <a href="https://doi.org/10.1145/219717.219748">https://doi.org/10.1145/219717.219748</a>.
</div>
<div id="ref-Porter80" class="csl-entry">
Porter, Martin F. 1980. <span>“An Algorithm for Suffix Stripping.”</span> <em>Program</em> 14 (3): 130–37. <a href="https://doi.org/10.1108/eb046814">https://doi.org/10.1108/eb046814</a>.
</div>
<div id="ref-Schofield16" class="csl-entry">
Schofield, Alexandra, and David Mimno. 2016. <span>“Comparing Apples to Apple: The Effects of Stemmers on Topic Models.”</span> <em>Transactions of the Association for Computational Linguistics</em> 4: 287–300. <a href="https://doi.org/10.1162/tacl_a_00099">https://doi.org/10.1162/tacl_a_00099</a>.
</div>
<div id="ref-Willett06" class="csl-entry">
Willett, P. 2006. <span>“The Porter Stemming Algorithm: Then and Now.”</span> <em>Program: Electronic Library and Information Systems</em> 40 (3): 219–23. <a href="http://eprints.whiterose.ac.uk/1434/">http://eprints.whiterose.ac.uk/1434/</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>This simple, “weak” stemmer is handy to have in your toolkit for many applications. Notice how we implement it here using <code>dplyr::case_when()</code>.<a href="stemming.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Part-of-speech information is also sometimes used directly in machine learning<a href="stemming.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="stopwords.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="embeddings.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/EmilHvitfeldt/smltar/edit/master/04_stemming.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
