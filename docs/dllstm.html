<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Long short-term memory (LSTM) networks | Supervised Machine Learning for Text Analysis in R</title>
  <meta name="description" content="Chapter 9 Long short-term memory (LSTM) networks | Supervised Machine Learning for Text Analysis in R" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Long short-term memory (LSTM) networks | Supervised Machine Learning for Text Analysis in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 9 Long short-term memory (LSTM) networks | Supervised Machine Learning for Text Analysis in R" />
  <meta name="github-repo" content="EmilHvitfeldt/smltar" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Long short-term memory (LSTM) networks | Supervised Machine Learning for Text Analysis in R" />
  
  <meta name="twitter:description" content="Chapter 9 Long short-term memory (LSTM) networks | Supervised Machine Learning for Text Analysis in R" />
  

<meta name="author" content="Emil Hvitfeldt and Julia Silge" />


<meta name="date" content="2021-04-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dldnn.html"/>
<link rel="next" href="dlcnn.html"/>
<script src="libs/header-attrs-2.7.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="libs/plot_text_explanations-0.1.0/plot_text_explanations.css" rel="stylesheet" />
<script src="libs/plot_text_explanations-binding-0.5.2/plot_text_explanations.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="smltar.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning for Text Analysis in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Supervised Machine Learning for Text Analysis in R</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#topics-this-book-will-not-cover"><i class="fa fa-check"></i>Topics this book will not cover</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#colophon"><i class="fa fa-check"></i>Colophon</a></li>
</ul></li>
<li class="part"><span><b>I Natural Language Features</b></span></li>
<li class="chapter" data-level="1" data-path="language.html"><a href="language.html"><i class="fa fa-check"></i><b>1</b> Language and modeling</a>
<ul>
<li class="chapter" data-level="1.1" data-path="language.html"><a href="language.html#linguistics-for-text-analysis"><i class="fa fa-check"></i><b>1.1</b> Linguistics for text analysis</a></li>
<li class="chapter" data-level="1.2" data-path="language.html"><a href="language.html#morphology"><i class="fa fa-check"></i><b>1.2</b> A glimpse into one area: morphology</a></li>
<li class="chapter" data-level="1.3" data-path="language.html"><a href="language.html#different-languages"><i class="fa fa-check"></i><b>1.3</b> Different languages</a></li>
<li class="chapter" data-level="1.4" data-path="language.html"><a href="language.html#other-ways-text-can-vary"><i class="fa fa-check"></i><b>1.4</b> Other ways text can vary</a></li>
<li class="chapter" data-level="1.5" data-path="language.html"><a href="language.html#languagesummary"><i class="fa fa-check"></i><b>1.5</b> Summary</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="language.html"><a href="language.html#in-this-chapter-you-learned"><i class="fa fa-check"></i><b>1.5.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tokenization.html"><a href="tokenization.html"><i class="fa fa-check"></i><b>2</b> Tokenization</a>
<ul>
<li class="chapter" data-level="2.1" data-path="tokenization.html"><a href="tokenization.html#what-is-a-token"><i class="fa fa-check"></i><b>2.1</b> What is a token?</a></li>
<li class="chapter" data-level="2.2" data-path="tokenization.html"><a href="tokenization.html#types-of-tokens"><i class="fa fa-check"></i><b>2.2</b> Types of tokens</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="tokenization.html"><a href="tokenization.html#character-tokens"><i class="fa fa-check"></i><b>2.2.1</b> Character tokens</a></li>
<li class="chapter" data-level="2.2.2" data-path="tokenization.html"><a href="tokenization.html#word-tokens"><i class="fa fa-check"></i><b>2.2.2</b> Word tokens</a></li>
<li class="chapter" data-level="2.2.3" data-path="tokenization.html"><a href="tokenization.html#tokenizingngrams"><i class="fa fa-check"></i><b>2.2.3</b> Tokenizing by n-grams</a></li>
<li class="chapter" data-level="2.2.4" data-path="tokenization.html"><a href="tokenization.html#lines-sentence-and-paragraph-tokens"><i class="fa fa-check"></i><b>2.2.4</b> Lines, sentence, and paragraph tokens</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="tokenization.html"><a href="tokenization.html#where-does-tokenization-break-down"><i class="fa fa-check"></i><b>2.3</b> Where does tokenization break down?</a></li>
<li class="chapter" data-level="2.4" data-path="tokenization.html"><a href="tokenization.html#building-your-own-tokenizer"><i class="fa fa-check"></i><b>2.4</b> Building your own tokenizer</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="tokenization.html"><a href="tokenization.html#tokenize-to-characters-only-keeping-letters"><i class="fa fa-check"></i><b>2.4.1</b> Tokenize to characters, only keeping letters</a></li>
<li class="chapter" data-level="2.4.2" data-path="tokenization.html"><a href="tokenization.html#allow-for-hyphenated-words"><i class="fa fa-check"></i><b>2.4.2</b> Allow for hyphenated words</a></li>
<li class="chapter" data-level="2.4.3" data-path="tokenization.html"><a href="tokenization.html#wrapping-it-in-a-function"><i class="fa fa-check"></i><b>2.4.3</b> Wrapping it in a function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="tokenization.html"><a href="tokenization.html#tokenization-for-non-latin-alphabets"><i class="fa fa-check"></i><b>2.5</b> Tokenization for non-Latin alphabets</a></li>
<li class="chapter" data-level="2.6" data-path="tokenization.html"><a href="tokenization.html#tokenization-benchmark"><i class="fa fa-check"></i><b>2.6</b> Tokenization benchmark</a></li>
<li class="chapter" data-level="2.7" data-path="tokenization.html"><a href="tokenization.html#tokensummary"><i class="fa fa-check"></i><b>2.7</b> Summary</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="tokenization.html"><a href="tokenization.html#in-this-chapter-you-learned-1"><i class="fa fa-check"></i><b>2.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stopwords.html"><a href="stopwords.html"><i class="fa fa-check"></i><b>3</b> Stop words</a>
<ul>
<li class="chapter" data-level="3.1" data-path="stopwords.html"><a href="stopwords.html#premadestopwords"><i class="fa fa-check"></i><b>3.1</b> Using premade stop word lists</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="stopwords.html"><a href="stopwords.html#stop-word-removal-in-r"><i class="fa fa-check"></i><b>3.1.1</b> Stop word removal in R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="stopwords.html"><a href="stopwords.html#homemadestopwords"><i class="fa fa-check"></i><b>3.2</b> Creating your own stop words list</a></li>
<li class="chapter" data-level="3.3" data-path="stopwords.html"><a href="stopwords.html#all-stop-word-lists-are-context-specific"><i class="fa fa-check"></i><b>3.3</b> All stop word lists are context-specific</a></li>
<li class="chapter" data-level="3.4" data-path="stopwords.html"><a href="stopwords.html#what-happens-when-you-remove-stop-words"><i class="fa fa-check"></i><b>3.4</b> What happens when you remove stop words</a></li>
<li class="chapter" data-level="3.5" data-path="stopwords.html"><a href="stopwords.html#stop-words-in-languages-other-than-english"><i class="fa fa-check"></i><b>3.5</b> Stop words in languages other than English</a></li>
<li class="chapter" data-level="3.6" data-path="stopwords.html"><a href="stopwords.html#stopwordssummary"><i class="fa fa-check"></i><b>3.6</b> Summary</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="stopwords.html"><a href="stopwords.html#in-this-chapter-you-learned-2"><i class="fa fa-check"></i><b>3.6.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stemming.html"><a href="stemming.html"><i class="fa fa-check"></i><b>4</b> Stemming</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stemming.html"><a href="stemming.html#how-to-stem-text-in-r"><i class="fa fa-check"></i><b>4.1</b> How to stem text in R</a></li>
<li class="chapter" data-level="4.2" data-path="stemming.html"><a href="stemming.html#should-you-use-stemming-at-all"><i class="fa fa-check"></i><b>4.2</b> Should you use stemming at all?</a></li>
<li class="chapter" data-level="4.3" data-path="stemming.html"><a href="stemming.html#understand-a-stemming-algorithm"><i class="fa fa-check"></i><b>4.3</b> Understand a stemming algorithm</a></li>
<li class="chapter" data-level="4.4" data-path="stemming.html"><a href="stemming.html#handling-punctuation-when-stemming"><i class="fa fa-check"></i><b>4.4</b> Handling punctuation when stemming</a></li>
<li class="chapter" data-level="4.5" data-path="stemming.html"><a href="stemming.html#compare-some-stemming-options"><i class="fa fa-check"></i><b>4.5</b> Compare some stemming options</a></li>
<li class="chapter" data-level="4.6" data-path="stemming.html"><a href="stemming.html#lemmatization"><i class="fa fa-check"></i><b>4.6</b> Lemmatization and stemming</a></li>
<li class="chapter" data-level="4.7" data-path="stemming.html"><a href="stemming.html#stemming-and-stop-words"><i class="fa fa-check"></i><b>4.7</b> Stemming and stop words</a></li>
<li class="chapter" data-level="4.8" data-path="stemming.html"><a href="stemming.html#stemmingsummary"><i class="fa fa-check"></i><b>4.8</b> Summary</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="stemming.html"><a href="stemming.html#in-this-chapter-you-learned-3"><i class="fa fa-check"></i><b>4.8.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="embeddings.html"><a href="embeddings.html"><i class="fa fa-check"></i><b>5</b> Word Embeddings</a>
<ul>
<li class="chapter" data-level="5.1" data-path="embeddings.html"><a href="embeddings.html#motivatingsparse"><i class="fa fa-check"></i><b>5.1</b> Motivating embeddings for sparse, high-dimensional data</a></li>
<li class="chapter" data-level="5.2" data-path="embeddings.html"><a href="embeddings.html#understand-word-embeddings-by-finding-them-yourself"><i class="fa fa-check"></i><b>5.2</b> Understand word embeddings by finding them yourself</a></li>
<li class="chapter" data-level="5.3" data-path="embeddings.html"><a href="embeddings.html#exploring-cfpb-word-embeddings"><i class="fa fa-check"></i><b>5.3</b> Exploring CFPB word embeddings</a></li>
<li class="chapter" data-level="5.4" data-path="embeddings.html"><a href="embeddings.html#glove"><i class="fa fa-check"></i><b>5.4</b> Use pre-trained word embeddings</a></li>
<li class="chapter" data-level="5.5" data-path="embeddings.html"><a href="embeddings.html#fairnessembeddings"><i class="fa fa-check"></i><b>5.5</b> Fairness and word embeddings</a></li>
<li class="chapter" data-level="5.6" data-path="embeddings.html"><a href="embeddings.html#using-word-embeddings-in-the-real-world"><i class="fa fa-check"></i><b>5.6</b> Using word embeddings in the real world</a></li>
<li class="chapter" data-level="5.7" data-path="embeddings.html"><a href="embeddings.html#embeddingssummary"><i class="fa fa-check"></i><b>5.7</b> Summary</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="embeddings.html"><a href="embeddings.html#in-this-chapter-you-learned-4"><i class="fa fa-check"></i><b>5.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Machine Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html"><i class="fa fa-check"></i>Foreword</a>
<ul>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#should-we-even-be-doing-this"><i class="fa fa-check"></i>Should we even be doing this?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-bias-is-already-in-the-data"><i class="fa fa-check"></i>What bias is already in the data?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#can-the-code-and-data-be-audited"><i class="fa fa-check"></i>Can the code and data be audited?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-are-the-error-rates-for-sub-groups"><i class="fa fa-check"></i>What are the error rates for sub-groups?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-is-the-accuracy-of-a-simple-rule-based-alternative"><i class="fa fa-check"></i>What is the accuracy of a simple rule-based alternative?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#what-processes-are-in-place-to-handle-appeals-or-mistakes"><i class="fa fa-check"></i>What processes are in place to handle appeals or mistakes?</a></li>
<li class="chapter" data-level="" data-path="mlforeword.html"><a href="mlforeword.html#how-diverse-is-the-team-that-built-it"><i class="fa fa-check"></i>How diverse is the team that built it?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlregression.html"><a href="mlregression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mlregression.html"><a href="mlregression.html#firstmlregression"><i class="fa fa-check"></i><b>6.1</b> A first regression model</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="mlregression.html"><a href="mlregression.html#firstregression"><i class="fa fa-check"></i><b>6.1.1</b> Building our first regression model</a></li>
<li class="chapter" data-level="6.1.2" data-path="mlregression.html"><a href="mlregression.html#firstregressionevaluation"><i class="fa fa-check"></i><b>6.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="mlregression.html"><a href="mlregression.html#regnull"><i class="fa fa-check"></i><b>6.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="6.3" data-path="mlregression.html"><a href="mlregression.html#comparerf"><i class="fa fa-check"></i><b>6.3</b> Compare to a random forest model</a></li>
<li class="chapter" data-level="6.4" data-path="mlregression.html"><a href="mlregression.html#casestudystopwords"><i class="fa fa-check"></i><b>6.4</b> Case study: removing stop words</a></li>
<li class="chapter" data-level="6.5" data-path="mlregression.html"><a href="mlregression.html#casestudyngrams"><i class="fa fa-check"></i><b>6.5</b> Case study: varying n-grams</a></li>
<li class="chapter" data-level="6.6" data-path="mlregression.html"><a href="mlregression.html#mlregressionlemmatization"><i class="fa fa-check"></i><b>6.6</b> Case study: lemmatization</a></li>
<li class="chapter" data-level="6.7" data-path="mlregression.html"><a href="mlregression.html#case-study-feature-hashing"><i class="fa fa-check"></i><b>6.7</b> Case study: feature hashing</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="mlregression.html"><a href="mlregression.html#text-normalization"><i class="fa fa-check"></i><b>6.7.1</b> Text normalization</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="mlregression.html"><a href="mlregression.html#what-evaluation-metrics-are-appropriate"><i class="fa fa-check"></i><b>6.8</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="6.9" data-path="mlregression.html"><a href="mlregression.html#mlregressionfull"><i class="fa fa-check"></i><b>6.9</b> The full game: regression</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="mlregression.html"><a href="mlregression.html#preprocess-the-data"><i class="fa fa-check"></i><b>6.9.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="6.9.2" data-path="mlregression.html"><a href="mlregression.html#specify-the-model"><i class="fa fa-check"></i><b>6.9.2</b> Specify the model</a></li>
<li class="chapter" data-level="6.9.3" data-path="mlregression.html"><a href="mlregression.html#tune-the-model"><i class="fa fa-check"></i><b>6.9.3</b> Tune the model</a></li>
<li class="chapter" data-level="6.9.4" data-path="mlregression.html"><a href="mlregression.html#regression-final-evaluation"><i class="fa fa-check"></i><b>6.9.4</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="mlregression.html"><a href="mlregression.html#mlregressionsummary"><i class="fa fa-check"></i><b>6.10</b> Summary</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="mlregression.html"><a href="mlregression.html#in-this-chapter-you-learned-5"><i class="fa fa-check"></i><b>6.10.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mlclassification.html"><a href="mlclassification.html"><i class="fa fa-check"></i><b>7</b> Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mlclassification.html"><a href="mlclassification.html#classfirstattemptlookatdata"><i class="fa fa-check"></i><b>7.1</b> A first classification model</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="mlclassification.html"><a href="mlclassification.html#classfirstmodel"><i class="fa fa-check"></i><b>7.1.1</b> Building our first classification model</a></li>
<li class="chapter" data-level="7.1.2" data-path="mlclassification.html"><a href="mlclassification.html#evaluation"><i class="fa fa-check"></i><b>7.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mlclassification.html"><a href="mlclassification.html#classnull"><i class="fa fa-check"></i><b>7.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="7.3" data-path="mlclassification.html"><a href="mlclassification.html#comparetolasso"><i class="fa fa-check"></i><b>7.3</b> Compare to a lasso classification model</a></li>
<li class="chapter" data-level="7.4" data-path="mlclassification.html"><a href="mlclassification.html#tunelasso"><i class="fa fa-check"></i><b>7.4</b> Tuning lasso hyperparameters</a></li>
<li class="chapter" data-level="7.5" data-path="mlclassification.html"><a href="mlclassification.html#casestudysparseencoding"><i class="fa fa-check"></i><b>7.5</b> Case study: sparse encoding</a></li>
<li class="chapter" data-level="7.6" data-path="mlclassification.html"><a href="mlclassification.html#mlmulticlass"><i class="fa fa-check"></i><b>7.6</b> Two class or multiclass?</a></li>
<li class="chapter" data-level="7.7" data-path="mlclassification.html"><a href="mlclassification.html#case-study-including-non-text-data"><i class="fa fa-check"></i><b>7.7</b> Case study: including non-text data</a></li>
<li class="chapter" data-level="7.8" data-path="mlclassification.html"><a href="mlclassification.html#case-study-data-censoring"><i class="fa fa-check"></i><b>7.8</b> Case study: data censoring</a></li>
<li class="chapter" data-level="7.9" data-path="mlclassification.html"><a href="mlclassification.html#customfeatures"><i class="fa fa-check"></i><b>7.9</b> Case study: custom features</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="mlclassification.html"><a href="mlclassification.html#detect-credit-cards"><i class="fa fa-check"></i><b>7.9.1</b> Detect credit cards</a></li>
<li class="chapter" data-level="7.9.2" data-path="mlclassification.html"><a href="mlclassification.html#calculate-percentage-censoring"><i class="fa fa-check"></i><b>7.9.2</b> Calculate percentage censoring</a></li>
<li class="chapter" data-level="7.9.3" data-path="mlclassification.html"><a href="mlclassification.html#detect-monetary-amounts"><i class="fa fa-check"></i><b>7.9.3</b> Detect monetary amounts</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="mlclassification.html"><a href="mlclassification.html#what-evaluation-metrics-are-appropriate-1"><i class="fa fa-check"></i><b>7.10</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="7.11" data-path="mlclassification.html"><a href="mlclassification.html#mlclassificationfull"><i class="fa fa-check"></i><b>7.11</b> The full game: classification</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="mlclassification.html"><a href="mlclassification.html#feature-selection"><i class="fa fa-check"></i><b>7.11.1</b> Feature selection</a></li>
<li class="chapter" data-level="7.11.2" data-path="mlclassification.html"><a href="mlclassification.html#specify-the-model-1"><i class="fa fa-check"></i><b>7.11.2</b> Specify the model</a></li>
<li class="chapter" data-level="7.11.3" data-path="mlclassification.html"><a href="mlclassification.html#classification-final-evaluation"><i class="fa fa-check"></i><b>7.11.3</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="mlclassification.html"><a href="mlclassification.html#mlclassificationsummary"><i class="fa fa-check"></i><b>7.12</b> Summary</a>
<ul>
<li class="chapter" data-level="7.12.1" data-path="mlclassification.html"><a href="mlclassification.html#in-this-chapter-you-learned-6"><i class="fa fa-check"></i><b>7.12.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Deep Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html"><i class="fa fa-check"></i>Foreword</a>
<ul>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#spending-your-data-budget"><i class="fa fa-check"></i>Spending your data budget</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#feature-engineering"><i class="fa fa-check"></i>Feature engineering</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#fitting-and-tuning"><i class="fa fa-check"></i>Fitting and tuning</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#model-evaluation"><i class="fa fa-check"></i>Model evaluation</a></li>
<li class="chapter" data-level="" data-path="dlforeword.html"><a href="dlforeword.html#putting-the-model-process-in-context"><i class="fa fa-check"></i>Putting the model process in context</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="dldnn.html"><a href="dldnn.html"><i class="fa fa-check"></i><b>8</b> Dense neural networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dldnn.html"><a href="dldnn.html#kickstarter"><i class="fa fa-check"></i><b>8.1</b> Kickstarter data</a></li>
<li class="chapter" data-level="8.2" data-path="dldnn.html"><a href="dldnn.html#firstdlclassification"><i class="fa fa-check"></i><b>8.2</b> A first deep learning model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="dldnn.html"><a href="dldnn.html#dnnrecipe"><i class="fa fa-check"></i><b>8.2.1</b> Preprocessing for deep learning</a></li>
<li class="chapter" data-level="8.2.2" data-path="dldnn.html"><a href="dldnn.html#onehotsequence"><i class="fa fa-check"></i><b>8.2.2</b> One-hot sequence embedding of text</a></li>
<li class="chapter" data-level="8.2.3" data-path="dldnn.html"><a href="dldnn.html#simple-flattened-dense-network"><i class="fa fa-check"></i><b>8.2.3</b> Simple flattened dense network</a></li>
<li class="chapter" data-level="8.2.4" data-path="dldnn.html"><a href="dldnn.html#evaluate-dnn"><i class="fa fa-check"></i><b>8.2.4</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="dldnn.html"><a href="dldnn.html#using-bag-of-words-features"><i class="fa fa-check"></i><b>8.3</b> Using bag-of-words features</a></li>
<li class="chapter" data-level="8.4" data-path="dldnn.html"><a href="dldnn.html#using-pre-trained-word-embeddings"><i class="fa fa-check"></i><b>8.4</b> Using pre-trained word embeddings</a></li>
<li class="chapter" data-level="8.5" data-path="dldnn.html"><a href="dldnn.html#dnncross"><i class="fa fa-check"></i><b>8.5</b> Cross-validation for deep learning models</a></li>
<li class="chapter" data-level="8.6" data-path="dldnn.html"><a href="dldnn.html#compare-and-evaluate-dnn-models"><i class="fa fa-check"></i><b>8.6</b> Compare and evaluate DNN models</a></li>
<li class="chapter" data-level="8.7" data-path="dldnn.html"><a href="dldnn.html#dllimitations"><i class="fa fa-check"></i><b>8.7</b> Limitations of deep learning</a></li>
<li class="chapter" data-level="8.8" data-path="dldnn.html"><a href="dldnn.html#dldnnsummary"><i class="fa fa-check"></i><b>8.8</b> Summary</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="dldnn.html"><a href="dldnn.html#in-this-chapter-you-learned-7"><i class="fa fa-check"></i><b>8.8.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dllstm.html"><a href="dllstm.html"><i class="fa fa-check"></i><b>9</b> Long short-term memory (LSTM) networks</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dllstm.html"><a href="dllstm.html#firstlstm"><i class="fa fa-check"></i><b>9.1</b> A first LSTM model</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="dllstm.html"><a href="dllstm.html#building-an-lstm"><i class="fa fa-check"></i><b>9.1.1</b> Building an LSTM</a></li>
<li class="chapter" data-level="9.1.2" data-path="dllstm.html"><a href="dllstm.html#lstmevaluation"><i class="fa fa-check"></i><b>9.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dllstm.html"><a href="dllstm.html#compare-to-a-recurrent-neural-network"><i class="fa fa-check"></i><b>9.2</b> Compare to a recurrent neural network</a></li>
<li class="chapter" data-level="9.3" data-path="dllstm.html"><a href="dllstm.html#bilstm"><i class="fa fa-check"></i><b>9.3</b> Case study: bidirectional LSTM</a></li>
<li class="chapter" data-level="9.4" data-path="dllstm.html"><a href="dllstm.html#case-study-stacking-lstm-layers"><i class="fa fa-check"></i><b>9.4</b> Case study: stacking LSTM layers</a></li>
<li class="chapter" data-level="9.5" data-path="dllstm.html"><a href="dllstm.html#lstmpadding"><i class="fa fa-check"></i><b>9.5</b> Case study: padding</a></li>
<li class="chapter" data-level="9.6" data-path="dllstm.html"><a href="dllstm.html#case-study-training-a-regression-model"><i class="fa fa-check"></i><b>9.6</b> Case study: training a regression model</a></li>
<li class="chapter" data-level="9.7" data-path="dllstm.html"><a href="dllstm.html#case-study-vocabulary-size"><i class="fa fa-check"></i><b>9.7</b> Case study: vocabulary size</a></li>
<li class="chapter" data-level="9.8" data-path="dllstm.html"><a href="dllstm.html#lstmfull"><i class="fa fa-check"></i><b>9.8</b> The full game: LSTM</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="dllstm.html"><a href="dllstm.html#lstmfullpreprocess"><i class="fa fa-check"></i><b>9.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="9.8.2" data-path="dllstm.html"><a href="dllstm.html#lstmfullmodel"><i class="fa fa-check"></i><b>9.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="dllstm.html"><a href="dllstm.html#dllstmsummary"><i class="fa fa-check"></i><b>9.9</b> Summary</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="dllstm.html"><a href="dllstm.html#in-this-chapter-you-learned-8"><i class="fa fa-check"></i><b>9.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dlcnn.html"><a href="dlcnn.html"><i class="fa fa-check"></i><b>10</b> Convolutional neural networks</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dlcnn.html"><a href="dlcnn.html#what-are-cnns"><i class="fa fa-check"></i><b>10.1</b> What are CNNs?</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="dlcnn.html"><a href="dlcnn.html#kernel"><i class="fa fa-check"></i><b>10.1.1</b> Kernel</a></li>
<li class="chapter" data-level="10.1.2" data-path="dlcnn.html"><a href="dlcnn.html#kernel-size"><i class="fa fa-check"></i><b>10.1.2</b> Kernel size</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="dlcnn.html"><a href="dlcnn.html#firstcnn"><i class="fa fa-check"></i><b>10.2</b> A first CNN model</a></li>
<li class="chapter" data-level="10.3" data-path="dlcnn.html"><a href="dlcnn.html#case-study-adding-more-layers"><i class="fa fa-check"></i><b>10.3</b> Case study: adding more layers</a></li>
<li class="chapter" data-level="10.4" data-path="dlcnn.html"><a href="dlcnn.html#case-study-byte-pair-encoding"><i class="fa fa-check"></i><b>10.4</b> Case study: byte pair encoding</a></li>
<li class="chapter" data-level="10.5" data-path="dlcnn.html"><a href="dlcnn.html#lime"><i class="fa fa-check"></i><b>10.5</b> Case study: explainability with LIME</a></li>
<li class="chapter" data-level="10.6" data-path="dlcnn.html"><a href="dlcnn.html#keras-hyperparameter"><i class="fa fa-check"></i><b>10.6</b> Case study: hyperparameter search</a></li>
<li class="chapter" data-level="10.7" data-path="dlcnn.html"><a href="dlcnn.html#cross-validation-for-evaluation"><i class="fa fa-check"></i><b>10.7</b> Cross-validation for evaluation</a></li>
<li class="chapter" data-level="10.8" data-path="dlcnn.html"><a href="dlcnn.html#cnnfull"><i class="fa fa-check"></i><b>10.8</b> The full game: CNN</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="dlcnn.html"><a href="dlcnn.html#cnnfullpreprocess"><i class="fa fa-check"></i><b>10.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="10.8.2" data-path="dlcnn.html"><a href="dlcnn.html#cnnfullmodel"><i class="fa fa-check"></i><b>10.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="dlcnn.html"><a href="dlcnn.html#dlcnnsummary"><i class="fa fa-check"></i><b>10.9</b> Summary</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="dlcnn.html"><a href="dlcnn.html#in-this-chapter-you-learned-9"><i class="fa fa-check"></i><b>10.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Conclusion</b></span></li>
<li class="chapter" data-level="" data-path="text-models-in-the-real-world.html"><a href="text-models-in-the-real-world.html"><i class="fa fa-check"></i>Text models in the real world</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="regexp.html"><a href="regexp.html"><i class="fa fa-check"></i><b>A</b> Regular expressions</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regexp.html"><a href="regexp.html#literal-characters"><i class="fa fa-check"></i><b>A.1</b> Literal characters</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="regexp.html"><a href="regexp.html#meta-characters"><i class="fa fa-check"></i><b>A.1.1</b> Meta characters</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="regexp.html"><a href="regexp.html#full-stop-the-wildcard"><i class="fa fa-check"></i><b>A.2</b> Full stop, the wildcard</a></li>
<li class="chapter" data-level="A.3" data-path="regexp.html"><a href="regexp.html#character-classes"><i class="fa fa-check"></i><b>A.3</b> Character classes</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="regexp.html"><a href="regexp.html#shorthand-character-classes"><i class="fa fa-check"></i><b>A.3.1</b> Shorthand character classes</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="regexp.html"><a href="regexp.html#quantifiers"><i class="fa fa-check"></i><b>A.4</b> Quantifiers</a></li>
<li class="chapter" data-level="A.5" data-path="regexp.html"><a href="regexp.html#anchors"><i class="fa fa-check"></i><b>A.5</b> Anchors</a></li>
<li class="chapter" data-level="A.6" data-path="regexp.html"><a href="regexp.html#additional-resources"><i class="fa fa-check"></i><b>A.6</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixdata.html"><a href="appendixdata.html"><i class="fa fa-check"></i><b>B</b> Data</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixdata.html"><a href="appendixdata.html#hcandersen"><i class="fa fa-check"></i><b>B.1</b> Hans Christian Andersen fairy tales</a></li>
<li class="chapter" data-level="B.2" data-path="appendixdata.html"><a href="appendixdata.html#scotus-opinions"><i class="fa fa-check"></i><b>B.2</b> Opinions of the Supreme Court of the United States</a></li>
<li class="chapter" data-level="B.3" data-path="appendixdata.html"><a href="appendixdata.html#cfpb-complaints"><i class="fa fa-check"></i><b>B.3</b> Consumer Financial Protection Bureau (CFPB) complaints</a></li>
<li class="chapter" data-level="B.4" data-path="appendixdata.html"><a href="appendixdata.html#kickstarter-blurbs"><i class="fa fa-check"></i><b>B.4</b> Kickstarter campaign blurbs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendixbaseline.html"><a href="appendixbaseline.html"><i class="fa fa-check"></i><b>C</b> Baseline linear classifier</a>
<ul>
<li class="chapter" data-level="C.1" data-path="appendixbaseline.html"><a href="appendixbaseline.html#read-in-the-data"><i class="fa fa-check"></i><b>C.1</b> Read in the data</a></li>
<li class="chapter" data-level="C.2" data-path="appendixbaseline.html"><a href="appendixbaseline.html#split-into-testtrain-and-create-resampling-folds"><i class="fa fa-check"></i><b>C.2</b> Split into test/train and create resampling folds</a></li>
<li class="chapter" data-level="C.3" data-path="appendixbaseline.html"><a href="appendixbaseline.html#recipe-for-data-preprocessing"><i class="fa fa-check"></i><b>C.3</b> Recipe for data preprocessing</a></li>
<li class="chapter" data-level="C.4" data-path="appendixbaseline.html"><a href="appendixbaseline.html#lasso-regularized-classification-model"><i class="fa fa-check"></i><b>C.4</b> Lasso regularized classification model</a></li>
<li class="chapter" data-level="C.5" data-path="appendixbaseline.html"><a href="appendixbaseline.html#a-model-workflow"><i class="fa fa-check"></i><b>C.5</b> A model workflow</a></li>
<li class="chapter" data-level="C.6" data-path="appendixbaseline.html"><a href="appendixbaseline.html#tune-the-workflow"><i class="fa fa-check"></i><b>C.6</b> Tune the workflow</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning for Text Analysis in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dllstm" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Long short-term memory (LSTM) networks</h1>
<p>In Chapter <a href="dldnn.html#dldnn">8</a>, we trained our first deep learning models, with straightforward dense network architectures that provide a bridge for our understanding as we move from shallow learning algorithms to more complex network architectures. Those first neural network architectures are not simple compared to the kinds of models we used in Chapters <a href="mlregression.html#mlregression">6</a> and <a href="mlclassification.html#mlclassification">7</a>, but it is possible to build many more different and more complex kinds of networks for prediction with text data. This chapter will focus on the family of <strong>long short-term memory</strong> networks (LSTMs) <span class="citation">(<a href="references.html#ref-Hochreiter1997" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>)</span>.</p>
<div id="firstlstm" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> A first LSTM model</h2>
<p>We will be using the same data from the previous chapter, described in Sections <a href="dldnn.html#kickstarter">8.1</a> and <a href="appendixdata.html#kickstarter-blurbs">B.4</a>. This data contains short text blurbs for prospective crowdfunding campaigns and whether those campaigns were successful. Our modeling goal is to predict whether a Kickstarter crowdfunding campaign was successful or not, based on the text blurb describing the campaign. Let’s start by splitting our data into training and testing sets.</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="dllstm.html#cb513-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb513-2"><a href="dllstm.html#cb513-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb513-3"><a href="dllstm.html#cb513-3" aria-hidden="true" tabindex="-1"></a>kickstarter <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/kickstarter.csv.gz&quot;</span>)</span>
<span id="cb513-4"><a href="dllstm.html#cb513-4" aria-hidden="true" tabindex="-1"></a>kickstarter</span></code></pre></div>
<pre><code>#&gt; # A tibble: 269,790 x 3
#&gt;    blurb                                                        state created_at
#&gt;    &lt;chr&gt;                                                        &lt;dbl&gt; &lt;date&gt;    
#&gt;  1 Exploring paint and its place in a digital world.                0 2015-03-17
#&gt;  2 Mike Fassio wants a side-by-side photo of me and Hazel eati…     0 2014-07-11
#&gt;  3 I need your help to get a nice graphics tablet and Photosho…     0 2014-07-30
#&gt;  4 I want to create a Nature Photograph Series of photos of wi…     0 2015-05-08
#&gt;  5 I want to bring colour to the world in my own artistic skil…     0 2015-02-01
#&gt;  6 We start from some lovely pictures made by us and we decide…     0 2015-11-18
#&gt;  7 Help me raise money to get a drawing tablet                      0 2015-04-03
#&gt;  8 I would like to share my art with the world and to do that …     0 2014-10-15
#&gt;  9 Post Card don’t set out to simply decorate stories. Our goa…     0 2015-06-25
#&gt; 10 My name is Siu Lon Liu and I am an illustrator seeking fund…     0 2014-07-19
#&gt; # … with 269,780 more rows</code></pre>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="dllstm.html#cb515-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb515-2"><a href="dllstm.html#cb515-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb515-3"><a href="dllstm.html#cb515-3" aria-hidden="true" tabindex="-1"></a>kickstarter_split <span class="ot">&lt;-</span> kickstarter <span class="sc">%&gt;%</span></span>
<span id="cb515-4"><a href="dllstm.html#cb515-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">nchar</span>(blurb) <span class="sc">&gt;=</span> <span class="dv">15</span>) <span class="sc">%&gt;%</span></span>
<span id="cb515-5"><a href="dllstm.html#cb515-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">state =</span> <span class="fu">as.integer</span>(state)) <span class="sc">%&gt;%</span></span>
<span id="cb515-6"><a href="dllstm.html#cb515-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">initial_split</span>()</span>
<span id="cb515-7"><a href="dllstm.html#cb515-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb515-8"><a href="dllstm.html#cb515-8" aria-hidden="true" tabindex="-1"></a>kickstarter_train <span class="ot">&lt;-</span> <span class="fu">training</span>(kickstarter_split)</span>
<span id="cb515-9"><a href="dllstm.html#cb515-9" aria-hidden="true" tabindex="-1"></a>kickstarter_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(kickstarter_split)</span></code></pre></div>
<p>Just as described in Chapter <a href="dldnn.html#dldnn">8</a>, the preprocessing needed for deep learning network architectures is somewhat different than for the models we used in Chapters <a href="mlregression.html#mlregression">6</a> and <a href="mlclassification.html#mlclassification">7</a>. The first step is still to tokenize the text, as described in Chapter <a href="tokenization.html#tokenization">2</a>. After we tokenize, we filter to keep only how many words we’ll include in the analysis; <code>step_tokenfilter()</code> keeps the top tokens based on frequency in this data set.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="dllstm.html#cb516-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(textrecipes)</span>
<span id="cb516-2"><a href="dllstm.html#cb516-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb516-3"><a href="dllstm.html#cb516-3" aria-hidden="true" tabindex="-1"></a>max_words <span class="ot">&lt;-</span> <span class="fl">2e4</span></span>
<span id="cb516-4"><a href="dllstm.html#cb516-4" aria-hidden="true" tabindex="-1"></a>max_length <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb516-5"><a href="dllstm.html#cb516-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb516-6"><a href="dllstm.html#cb516-6" aria-hidden="true" tabindex="-1"></a>kick_rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span> blurb, <span class="at">data =</span> kickstarter_train) <span class="sc">%&gt;%</span></span>
<span id="cb516-7"><a href="dllstm.html#cb516-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenize</span>(blurb) <span class="sc">%&gt;%</span></span>
<span id="cb516-8"><a href="dllstm.html#cb516-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenfilter</span>(blurb, <span class="at">max_tokens =</span> max_words) <span class="sc">%&gt;%</span></span>
<span id="cb516-9"><a href="dllstm.html#cb516-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_sequence_onehot</span>(blurb, <span class="at">sequence_length =</span> max_length)</span></code></pre></div>
<p>After tokenizing, the preprocessing is different. We use <code>step_sequence_onehot()</code> to encode the sequences of words as integers representing each token in the vocabulary of 20,000 words, as described in detail in Section <a href="dldnn.html#onehotsequence">8.2.2</a>. This is different than the representations we used in Chapters <a href="mlregression.html#mlregression">6</a> and <a href="mlclassification.html#mlclassification">7</a>, mainly because information about word sequence is encoded in this representation.</p>

<div class="rmdnote">
Using <code>step_sequence_onehot()</code> to preprocess text data records and encodes <em>sequence</em> information, unlike the document-term matrix and/or bag-of-tokens approaches we used in Chapters <a href="mlclassification.html#mlclassification">7</a> and <a href="mlregression.html#mlregression">6</a>.
</div>
<p>There are 202,093 blurbs in the training set and 67,364 in the testing set.</p>
<div class="rmdpackage">
<p>
Like we discussed in the last chapter, we are using <strong>recipes</strong> and <strong>textrecipes</strong> for preprocessing before modeling. When we <code>prep()</code> a recipe, we compute or estimate statistics from the training set; the output of <code>prep()</code> is a recipe. When we <code>bake()</code> a recipe, we apply the preprocessing to a data set, either the training set that we started with or another set like the testing data or new data. The output of <code>bake()</code> is a data set like a tibble or a matrix.
</p>
</div>
<p>We could have applied these <code>prep()</code> and <code>bake()</code> functions to any preprocessing recipes throughout this book, but we typically didn’t need to because our modeling workflows automated these steps.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="dllstm.html#cb517-1" aria-hidden="true" tabindex="-1"></a>kick_prep <span class="ot">&lt;-</span> <span class="fu">prep</span>(kick_rec)</span>
<span id="cb517-2"><a href="dllstm.html#cb517-2" aria-hidden="true" tabindex="-1"></a>kick_matrix <span class="ot">&lt;-</span> <span class="fu">bake</span>(kick_prep, <span class="at">new_data =</span> <span class="cn">NULL</span>, <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb517-3"><a href="dllstm.html#cb517-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb517-4"><a href="dllstm.html#cb517-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(kick_matrix)</span></code></pre></div>
<pre><code>#&gt; [1] 202093     30</code></pre>
<p>Here we use <code>composition = "matrix"</code> because the Keras modeling functions operate on matrices, rather than a dataframe or tibble.</p>
<div id="building-an-lstm" class="section level3" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Building an LSTM</h3>
<p>An LSTM is a specific kind of network architecture with feedback loops that allow information to persist through steps<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> and memory cells that can learn to “remember” and “forget” information through sequences. LSTMs are well-suited for text because of this ability to process text as a long sequence of words or characters, and can model structures within text like word dependencies. LSTMs are useful in text modeling because of this memory through long sequences; they are also used for time series, machine translation, and similar problems.</p>
<p>Figure <a href="dllstm.html#fig:rnndiag">9.1</a> depicts a high-level diagram of how the LSTM unit of a network works. In the diagram, part of the neural network, <span class="math inline">\(A\)</span>, operates on some of the input and outputs a value. During this process, some information is held inside <span class="math inline">\(A\)</span> to make the network “remember” this updated network. Network <span class="math inline">\(A\)</span> is then applied to the next input where it predicts new output and its memory is updated.</p>
<div class="figure" style="text-align: center"><span id="fig:rnndiag"></span>
<img src="diagram-files/rnn-architecture.png" alt="High-level diagram of an unrolled recurrent neural network. The recurrent neural network is the backbone of LSTM networks." width="90%" />
<p class="caption">
FIGURE 9.1: High-level diagram of an unrolled recurrent neural network. The recurrent neural network is the backbone of LSTM networks.
</p>
</div>
<p>The exact shape and function of network <span class="math inline">\(A\)</span> are beyond the reach of this book. For further study, Christopher Olah’s blog post <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">“Understanding LSTM Networks”</a> gives a more technical overview of how LSTM networks work.</p>
<p>The Keras library has convenient functions for broadly used architectures like LSTMs so we don’t have to build it from scratch using layers; we can instead use <code>layer_lstm()</code>. This comes <em>after</em> an embedding layer that makes dense vectors from our word sequences and <em>before</em> a densely-connected layer for output.</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="dllstm.html#cb519-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb519-2"><a href="dllstm.html#cb519-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-3"><a href="dllstm.html#cb519-3" aria-hidden="true" tabindex="-1"></a>lstm_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb519-4"><a href="dllstm.html#cb519-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb519-5"><a href="dllstm.html#cb519-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb519-6"><a href="dllstm.html#cb519-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb519-7"><a href="dllstm.html#cb519-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb519-8"><a href="dllstm.html#cb519-8" aria-hidden="true" tabindex="-1"></a>lstm_mod</span></code></pre></div>
<pre><code>#&gt; Model
#&gt; Model: &quot;sequential&quot;
#&gt; ________________________________________________________________________________
#&gt; Layer (type)                        Output Shape                    Param #     
#&gt; ================================================================================
#&gt; embedding (Embedding)               (None, None, 32)                640032      
#&gt; ________________________________________________________________________________
#&gt; lstm (LSTM)                         (None, 32)                      8320        
#&gt; ________________________________________________________________________________
#&gt; dense (Dense)                       (None, 1)                       33          
#&gt; ================================================================================
#&gt; Total params: 648,385
#&gt; Trainable params: 648,385
#&gt; Non-trainable params: 0
#&gt; ________________________________________________________________________________</code></pre>

<div class="rmdwarning">
Notice the number of parameters in this LSTM model, about twice as many as the dense neural networks in Chapter <a href="dldnn.html#dldnn">8</a>. It is easier to overfit an LSTM model, and it takes more time and memory to train, because of the large number of parameters.
</div>
<p>Because we are training a binary classification model, we use <code>activation = "sigmoid"</code> for the last layer; we want to fit and predict to class probabilities.</p>
<p>Next we <code>compile()</code> the model, which configures the model for training with a specific optimizer and set of metrics.</p>
<div class="rmdnote">
<p>
A good default optimizer for many problems is <code>“adam”</code> <span class="citation"><span class="citation">(<a href="references.html#ref-kingma2017adam" role="doc-biblioref">Kingma and Ba 2017</a>)</span></span>, and a good loss function for binary classification is <code>“binary_crossentropy”</code>.
</p>
</div>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="dllstm.html#cb521-1" aria-hidden="true" tabindex="-1"></a>lstm_mod <span class="sc">%&gt;%</span></span>
<span id="cb521-2"><a href="dllstm.html#cb521-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb521-3"><a href="dllstm.html#cb521-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb521-4"><a href="dllstm.html#cb521-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb521-5"><a href="dllstm.html#cb521-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb521-6"><a href="dllstm.html#cb521-6" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>

<div class="rmdwarning">
As we noted in Chapter <a href="dldnn.html#dldnn">8</a>, the neural network model is modified <strong>in place</strong>; the object <code>lstm_mod</code> is different after we compile it, even though we didn’t assign the object to anything. This is different from how most objects in R work, so pay special attention to the state of your model objects.
</div>
<p>After the model is compiled, we can fit it. The <code>fit()</code> method for Keras models has an argument <code>validation_split</code> that will set apart a fraction of the training data for evaluation and assessment. The performance metrics are evaluated on the validation set at the <em>end</em> of each epoch.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="dllstm.html#cb522-1" aria-hidden="true" tabindex="-1"></a>lstm_history <span class="ot">&lt;-</span> lstm_mod <span class="sc">%&gt;%</span></span>
<span id="cb522-2"><a href="dllstm.html#cb522-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb522-3"><a href="dllstm.html#cb522-3" aria-hidden="true" tabindex="-1"></a>    kick_matrix,</span>
<span id="cb522-4"><a href="dllstm.html#cb522-4" aria-hidden="true" tabindex="-1"></a>    kickstarter_train<span class="sc">$</span>state,</span>
<span id="cb522-5"><a href="dllstm.html#cb522-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb522-6"><a href="dllstm.html#cb522-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_split =</span> <span class="fl">0.25</span>,</span>
<span id="cb522-7"><a href="dllstm.html#cb522-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb522-8"><a href="dllstm.html#cb522-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb522-9"><a href="dllstm.html#cb522-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb522-10"><a href="dllstm.html#cb522-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb522-11"><a href="dllstm.html#cb522-11" aria-hidden="true" tabindex="-1"></a>lstm_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.2636
#&gt;     accuracy: 0.8776
#&gt;     val_loss: 0.5831
#&gt; val_accuracy: 0.7825</code></pre>
<p>The loss on the training data (called <code>loss</code> here) is much better than the loss on the validation data (<code>val_loss</code>), indicating that we are overfitting pretty dramatically. We can see this by plotting the history as well in Figure <a href="dllstm.html#fig:firstlstmhistory">9.2</a>.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="dllstm.html#cb524-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstm_history)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:firstlstmhistory"></span>
<img src="09_dl_lstm_files/figure-html/firstlstmhistory-1.png" alt="Training and validation metrics for LSTM" width="672" />
<p class="caption">
FIGURE 9.2: Training and validation metrics for LSTM
</p>
</div>
<div class="rmdnote">
<p>
Remember that lower loss indicates a better fitting model, and higher accuracy (closer to 1) indicates a better model.
</p>
</div>
<p>This model continues to improve epoch after epoch on the training data, but performs worse on the validation set than the training set after the first few epochs and eventually starts to exhibit <em>worsening</em> performance on the validation set as epochs pass, demonstrating how extremely it is overfitting to the training data. This is very common for powerful deep learning models, including LSTMs.</p>
</div>
<div id="lstmevaluation" class="section level3" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Evaluation</h3>
<p>We used some Keras defaults for model evaluation in the previous section, but just like we demonstrated in Section <a href="dldnn.html#evaluate-dnn">8.2.4</a>, we can take more control if we want or need to. Instead of using the <code>validation_split</code> argument, we can use the <code>validation_data</code> argument and send in our own validation set creating with rsample.</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="dllstm.html#cb525-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">234</span>)</span>
<span id="cb525-2"><a href="dllstm.html#cb525-2" aria-hidden="true" tabindex="-1"></a>kick_val <span class="ot">&lt;-</span> <span class="fu">validation_split</span>(kickstarter_train, <span class="at">strata =</span> state)</span>
<span id="cb525-3"><a href="dllstm.html#cb525-3" aria-hidden="true" tabindex="-1"></a>kick_val</span></code></pre></div>
<pre><code>#&gt; # Validation Set Split (0.75/0.25)  using stratification 
#&gt; # A tibble: 1 x 2
#&gt;   splits                 id        
#&gt;   &lt;list&gt;                 &lt;chr&gt;     
#&gt; 1 &lt;split [151571/50522]&gt; validation</code></pre>
<p>We can access the two data sets specified by this <code>split</code> via the functions <code>analysis()</code> (the analog to training) and <code>assessment()</code> (the analog to testing). We need to apply our prepped preprocessing recipe <code>kick_prep</code> to both to transform this data to the appropriate format for our neural network architecture.</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="dllstm.html#cb527-1" aria-hidden="true" tabindex="-1"></a>kick_analysis <span class="ot">&lt;-</span> <span class="fu">bake</span>(kick_prep, <span class="at">new_data =</span> <span class="fu">analysis</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]),</span>
<span id="cb527-2"><a href="dllstm.html#cb527-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb527-3"><a href="dllstm.html#cb527-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(kick_analysis)</span></code></pre></div>
<pre><code>#&gt; [1] 151571     30</code></pre>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="dllstm.html#cb529-1" aria-hidden="true" tabindex="-1"></a>kick_assess <span class="ot">&lt;-</span> <span class="fu">bake</span>(kick_prep, <span class="at">new_data =</span> <span class="fu">assessment</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]),</span>
<span id="cb529-2"><a href="dllstm.html#cb529-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb529-3"><a href="dllstm.html#cb529-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(kick_assess)</span></code></pre></div>
<pre><code>#&gt; [1] 50522    30</code></pre>
<p>These are each matrices appropriate for a Keras model. We will also need the outcome variables for both sets.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="dllstm.html#cb531-1" aria-hidden="true" tabindex="-1"></a>state_analysis <span class="ot">&lt;-</span> <span class="fu">analysis</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(state)</span>
<span id="cb531-2"><a href="dllstm.html#cb531-2" aria-hidden="true" tabindex="-1"></a>state_assess <span class="ot">&lt;-</span> <span class="fu">assessment</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(state)</span></code></pre></div>
<p>Let’s also think about our LSTM model architecture. We saw evidence for significant overfitting with our first LSTM, and we can counteract that by including dropout, both in the regular sense (<code>dropout</code>) and in the feedback loops (<code>recurrent_dropout</code>).</p>
<div class="rmdwarning">
<p>
When we include some dropout, we temporarily remove some units together with their connections from the network. The purpose of this is typically to reduce overfitting <span class="citation"><span class="citation">(<a href="references.html#ref-Srivastava2014" role="doc-biblioref">Srivastava et al. 2014</a>)</span></span>. Dropout is not exclusive to LSTM models, and can also be used in many other kinds of network architectures. Another way to add dropout to a network is with <code>layer_dropout()</code>.
</p>
</div>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="dllstm.html#cb532-1" aria-hidden="true" tabindex="-1"></a>lstm_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb532-2"><a href="dllstm.html#cb532-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb532-3"><a href="dllstm.html#cb532-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb532-4"><a href="dllstm.html#cb532-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb532-5"><a href="dllstm.html#cb532-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb532-6"><a href="dllstm.html#cb532-6" aria-hidden="true" tabindex="-1"></a>lstm_mod <span class="sc">%&gt;%</span></span>
<span id="cb532-7"><a href="dllstm.html#cb532-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb532-8"><a href="dllstm.html#cb532-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb532-9"><a href="dllstm.html#cb532-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb532-10"><a href="dllstm.html#cb532-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb532-11"><a href="dllstm.html#cb532-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb532-12"><a href="dllstm.html#cb532-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb532-13"><a href="dllstm.html#cb532-13" aria-hidden="true" tabindex="-1"></a>val_history <span class="ot">&lt;-</span> lstm_mod <span class="sc">%&gt;%</span></span>
<span id="cb532-14"><a href="dllstm.html#cb532-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb532-15"><a href="dllstm.html#cb532-15" aria-hidden="true" tabindex="-1"></a>    kick_analysis,</span>
<span id="cb532-16"><a href="dllstm.html#cb532-16" aria-hidden="true" tabindex="-1"></a>    state_analysis,</span>
<span id="cb532-17"><a href="dllstm.html#cb532-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb532-18"><a href="dllstm.html#cb532-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(kick_assess, state_assess),</span>
<span id="cb532-19"><a href="dllstm.html#cb532-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb532-20"><a href="dllstm.html#cb532-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb532-21"><a href="dllstm.html#cb532-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb532-22"><a href="dllstm.html#cb532-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb532-23"><a href="dllstm.html#cb532-23" aria-hidden="true" tabindex="-1"></a>val_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.3756
#&gt;     accuracy: 0.824
#&gt;     val_loss: 0.6022
#&gt; val_accuracy: 0.7385</code></pre>
<p>The overfitting has been reduced, and Figure <a href="dllstm.html#fig:lstmvalhistory">9.3</a> shows that the difference between our model’s performance on training and validation data is now smaller.</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="dllstm.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(val_history)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:lstmvalhistory"></span>
<img src="09_dl_lstm_files/figure-html/lstmvalhistory-1.png" alt="Training and validation metrics for LSTM with dropout" width="672" />
<p class="caption">
FIGURE 9.3: Training and validation metrics for LSTM with dropout
</p>
</div>
<p>Remember that this is specific validation data that we have chosen ahead of time, so we can evaluate metrics flexibly in any way we need to, for example, using yardstick functions. We can create a tibble with the true and predicted values for the validation set.</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="dllstm.html#cb535-1" aria-hidden="true" tabindex="-1"></a>val_res <span class="ot">&lt;-</span> <span class="fu">keras_predict</span>(lstm_mod, kick_assess, state_assess)</span>
<span id="cb535-2"><a href="dllstm.html#cb535-2" aria-hidden="true" tabindex="-1"></a>val_res <span class="sc">%&gt;%</span> <span class="fu">metrics</span>(state, .pred_class, .pred_1)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 3
#&gt;   .metric     .estimator .estimate
#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 accuracy    binary         0.739
#&gt; 2 kap         binary         0.475
#&gt; 3 mn_log_loss binary         0.602
#&gt; 4 roc_auc     binary         0.808</code></pre>
<p>A regularized linear model trained on this data set achieved results of accuracy of 0.684 and an AUC for the ROC curve of 0.752 (Appendix <a href="appendixbaseline.html#appendixbaseline">C</a>). This first LSTM with dropout is already performing better than such a linear model. We can plot the ROC curve in Figure <a href="dllstm.html#fig:lstmvalroc">9.4</a> to evaluate the performance across the range of thresholds.</p>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="dllstm.html#cb537-1" aria-hidden="true" tabindex="-1"></a>val_res <span class="sc">%&gt;%</span></span>
<span id="cb537-2"><a href="dllstm.html#cb537-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">roc_curve</span>(state, .pred_1) <span class="sc">%&gt;%</span></span>
<span id="cb537-3"><a href="dllstm.html#cb537-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:lstmvalroc"></span>
<img src="09_dl_lstm_files/figure-html/lstmvalroc-1.png" alt="ROC curve for LSTM with dropout predictions of Kickstarter campaign success" width="672" />
<p class="caption">
FIGURE 9.4: ROC curve for LSTM with dropout predictions of Kickstarter campaign success
</p>
</div>
</div>
</div>
<div id="compare-to-a-recurrent-neural-network" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Compare to a recurrent neural network</h2>
<p>An LSTM is actually a specific kind of recurrent neural network (RNN) <span class="citation">(<a href="references.html#ref-ELMAN1990179" role="doc-biblioref">Elman 1990</a>)</span>. Simple RNNs have feedback loops and hidden state that allow information to persist through steps but do not have memory cells like LSTMs. This difference between RNNs and LSTMs amounts to what happens in network <span class="math inline">\(A\)</span> in Figure <a href="dllstm.html#fig:rnndiag">9.1</a>. RNNs tend to have a very simple structure, typically just a single <code>tanh()</code> layer, much simpler than what happens in LSTMs.</p>
<div class="rmdwarning">
<p>
Simple RNNs can only connect very recent information and structure in sequences, but LSTMS can learn long-range dependencies and broader context.
</p>
</div>
<p>Let’s train an RNN to see how it compares to the LSTM.</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="dllstm.html#cb538-1" aria-hidden="true" tabindex="-1"></a>rnn_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb538-2"><a href="dllstm.html#cb538-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb538-3"><a href="dllstm.html#cb538-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb538-4"><a href="dllstm.html#cb538-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb538-5"><a href="dllstm.html#cb538-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb538-6"><a href="dllstm.html#cb538-6" aria-hidden="true" tabindex="-1"></a>rnn_mod <span class="sc">%&gt;%</span></span>
<span id="cb538-7"><a href="dllstm.html#cb538-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb538-8"><a href="dllstm.html#cb538-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb538-9"><a href="dllstm.html#cb538-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb538-10"><a href="dllstm.html#cb538-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb538-11"><a href="dllstm.html#cb538-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb538-12"><a href="dllstm.html#cb538-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb538-13"><a href="dllstm.html#cb538-13" aria-hidden="true" tabindex="-1"></a>rnn_history <span class="ot">&lt;-</span> rnn_mod <span class="sc">%&gt;%</span></span>
<span id="cb538-14"><a href="dllstm.html#cb538-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb538-15"><a href="dllstm.html#cb538-15" aria-hidden="true" tabindex="-1"></a>    kick_analysis,</span>
<span id="cb538-16"><a href="dllstm.html#cb538-16" aria-hidden="true" tabindex="-1"></a>    state_analysis,</span>
<span id="cb538-17"><a href="dllstm.html#cb538-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb538-18"><a href="dllstm.html#cb538-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(kick_assess, state_assess),</span>
<span id="cb538-19"><a href="dllstm.html#cb538-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb538-20"><a href="dllstm.html#cb538-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb538-21"><a href="dllstm.html#cb538-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb538-22"><a href="dllstm.html#cb538-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb538-23"><a href="dllstm.html#cb538-23" aria-hidden="true" tabindex="-1"></a>rnn_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.4996
#&gt;     accuracy: 0.7663
#&gt;     val_loss: 0.6204
#&gt; val_accuracy: 0.6949</code></pre>
<p>Looks like more overfitting! We can see this by plotting the history as well in Figure <a href="dllstm.html#fig:rnnhistory">9.5</a>.</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="dllstm.html#cb540-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rnn_history)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:rnnhistory"></span>
<img src="09_dl_lstm_files/figure-html/rnnhistory-1.png" alt="Training and validation metrics for RNN" width="672" />
<p class="caption">
FIGURE 9.5: Training and validation metrics for RNN
</p>
</div>
<p>These results are pretty disappointing overall, with worse performance than our first LSTM. Simple RNNs like the ones in this section can be challenging to train well, and just cranking up the number of embedding dimensions, units, or other network characteristics usually does not fix the problem. Often, RNNs just don’t work well compared to simpler deep learning architectures like the dense network introduced in Section <a href="dldnn.html#firstdlclassification">8.2</a> <span class="citation">(<a href="references.html#ref-Minaee2020" role="doc-biblioref">Minaee et al. 2020</a>)</span>, or even other machine learning approaches like regularized linear models with good preprocessing.</p>
<p>Fortunately, we can build on the ideas of a simple RNN with more complex architectures like LSTMs to build better performing models.</p>
</div>
<div id="bilstm" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Case study: bidirectional LSTM</h2>
<p>The RNNs and LSTMs that we have fit so far have modeled text as sequences, specifically sequences where information and memory persists moving forward. These kinds of models can learn structures and dependencies moving forward <em>only</em>. In language, the structures move both directions, though; the words that come <em>after</em> a given structure or word can be just as important for understanding it as the ones that come before it.</p>
<p>We can build this into our neural network architecture with a <strong>bidirectional</strong> wrapper for RNNs or LSTMs.</p>
<div class="rmdnote">
<p>
A bidirectional LSTM allows the network to have both the forward and backward information about the sequences at each step.
</p>
</div>
<p>The input sequences are passed through the network in two directions, both forward and backward, allowing the network to learn more context, structures, and dependencies.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="dllstm.html#cb541-1" aria-hidden="true" tabindex="-1"></a>bilstm_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb541-2"><a href="dllstm.html#cb541-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb541-3"><a href="dllstm.html#cb541-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bidirectional</span>(<span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>,</span>
<span id="cb541-4"><a href="dllstm.html#cb541-4" aria-hidden="true" tabindex="-1"></a>                           <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb541-5"><a href="dllstm.html#cb541-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb541-6"><a href="dllstm.html#cb541-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb541-7"><a href="dllstm.html#cb541-7" aria-hidden="true" tabindex="-1"></a>bilstm_mod <span class="sc">%&gt;%</span></span>
<span id="cb541-8"><a href="dllstm.html#cb541-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb541-9"><a href="dllstm.html#cb541-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb541-10"><a href="dllstm.html#cb541-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb541-11"><a href="dllstm.html#cb541-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb541-12"><a href="dllstm.html#cb541-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb541-13"><a href="dllstm.html#cb541-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb541-14"><a href="dllstm.html#cb541-14" aria-hidden="true" tabindex="-1"></a>bilstm_history <span class="ot">&lt;-</span> bilstm_mod <span class="sc">%&gt;%</span></span>
<span id="cb541-15"><a href="dllstm.html#cb541-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb541-16"><a href="dllstm.html#cb541-16" aria-hidden="true" tabindex="-1"></a>    kick_analysis,</span>
<span id="cb541-17"><a href="dllstm.html#cb541-17" aria-hidden="true" tabindex="-1"></a>    state_analysis,</span>
<span id="cb541-18"><a href="dllstm.html#cb541-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb541-19"><a href="dllstm.html#cb541-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(kick_assess, state_assess),</span>
<span id="cb541-20"><a href="dllstm.html#cb541-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb541-21"><a href="dllstm.html#cb541-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb541-22"><a href="dllstm.html#cb541-22" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb541-23"><a href="dllstm.html#cb541-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb541-24"><a href="dllstm.html#cb541-24" aria-hidden="true" tabindex="-1"></a>bilstm_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.3659
#&gt;     accuracy: 0.8294
#&gt;     val_loss: 0.6045
#&gt; val_accuracy: 0.7396</code></pre>
<p>The bidirectional LSTM is more able to represent the data well, but with the same amount of dropout, we do see more dramatic overfitting. Still, there is some improvement on the validation set as well.</p>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="dllstm.html#cb543-1" aria-hidden="true" tabindex="-1"></a>bilstm_res <span class="ot">&lt;-</span> <span class="fu">keras_predict</span>(bilstm_mod, kick_assess, state_assess)</span>
<span id="cb543-2"><a href="dllstm.html#cb543-2" aria-hidden="true" tabindex="-1"></a>bilstm_res <span class="sc">%&gt;%</span> <span class="fu">metrics</span>(state, .pred_class, .pred_1)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 3
#&gt;   .metric     .estimator .estimate
#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 accuracy    binary         0.740
#&gt; 2 kap         binary         0.478
#&gt; 3 mn_log_loss binary         0.604
#&gt; 4 roc_auc     binary         0.809</code></pre>
<p>This bidirectional LSTM, able to learn both forward and backward text structures, provides some improvement over the regular LSTM on the validation set (which had an accuracy of 0.739).</p>
</div>
<div id="case-study-stacking-lstm-layers" class="section level2" number="9.4">
<h2><span class="header-section-number">9.4</span> Case study: stacking LSTM layers</h2>
<p>Deep learning architectures can be built up to create extremely complex networks. For example, RNN and/or LSTM layers can be stacked on top of each other, or together with other kinds of layers. The idea of this stacking is to increase the ability of a network to represent the data well.</p>
<div class="rmdwarning">
<p>
Intermediate layers must be set up to return sequences (with <code>return_sequences = TRUE</code>) instead of the last output for each sequence.
</p>
</div>
<p>Let’s start by adding one single additional layer.</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="dllstm.html#cb545-1" aria-hidden="true" tabindex="-1"></a>stacked_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb545-2"><a href="dllstm.html#cb545-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb545-3"><a href="dllstm.html#cb545-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>,</span>
<span id="cb545-4"><a href="dllstm.html#cb545-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">return_sequences =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb545-5"><a href="dllstm.html#cb545-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb545-6"><a href="dllstm.html#cb545-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb545-7"><a href="dllstm.html#cb545-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb545-8"><a href="dllstm.html#cb545-8" aria-hidden="true" tabindex="-1"></a>stacked_mod <span class="sc">%&gt;%</span></span>
<span id="cb545-9"><a href="dllstm.html#cb545-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb545-10"><a href="dllstm.html#cb545-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb545-11"><a href="dllstm.html#cb545-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb545-12"><a href="dllstm.html#cb545-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb545-13"><a href="dllstm.html#cb545-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb545-14"><a href="dllstm.html#cb545-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb545-15"><a href="dllstm.html#cb545-15" aria-hidden="true" tabindex="-1"></a>stacked_history <span class="ot">&lt;-</span> stacked_mod <span class="sc">%&gt;%</span></span>
<span id="cb545-16"><a href="dllstm.html#cb545-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb545-17"><a href="dllstm.html#cb545-17" aria-hidden="true" tabindex="-1"></a>    kick_analysis,</span>
<span id="cb545-18"><a href="dllstm.html#cb545-18" aria-hidden="true" tabindex="-1"></a>    state_analysis,</span>
<span id="cb545-19"><a href="dllstm.html#cb545-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb545-20"><a href="dllstm.html#cb545-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(kick_assess, state_assess),</span>
<span id="cb545-21"><a href="dllstm.html#cb545-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb545-22"><a href="dllstm.html#cb545-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb545-23"><a href="dllstm.html#cb545-23" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb545-24"><a href="dllstm.html#cb545-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb545-25"><a href="dllstm.html#cb545-25" aria-hidden="true" tabindex="-1"></a>stacked_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.3812
#&gt;     accuracy: 0.8226
#&gt;     val_loss: 0.5919
#&gt; val_accuracy: 0.7387</code></pre>
<p>Adding another separate layer in the forward direction appears to have improved the network, about as much as extending the LSTM layer to handle information in the backward direction via the bidirectional LSTM.</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="dllstm.html#cb547-1" aria-hidden="true" tabindex="-1"></a>stacked_res <span class="ot">&lt;-</span> <span class="fu">keras_predict</span>(stacked_mod, kick_assess, state_assess)</span>
<span id="cb547-2"><a href="dllstm.html#cb547-2" aria-hidden="true" tabindex="-1"></a>stacked_res <span class="sc">%&gt;%</span> <span class="fu">metrics</span>(state, .pred_class, .pred_1)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 3
#&gt;   .metric     .estimator .estimate
#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 accuracy    binary         0.739
#&gt; 2 kap         binary         0.475
#&gt; 3 mn_log_loss binary         0.592
#&gt; 4 roc_auc     binary         0.806</code></pre>
<p>We can gradually improve a model by changing and adding to its architecture.</p>
</div>
<div id="lstmpadding" class="section level2" number="9.5">
<h2><span class="header-section-number">9.5</span> Case study: padding</h2>
<p>One of the most important themes of this book is that text must be heavily preprocessed in order to be useful for machine learning algorithms, and these preprocessing decisions have big effects on model results. One decision that seems like it may not be all that important is how sequences are <em>padded</em> for a deep learning model. The matrix that is used as input for a neural network must be rectangular, but the training data documents are typically all different lengths. Sometimes, like the case of the Supreme Court opinions, the lengths vary a lot; sometimes, like with the Kickstarter data, the lengths vary a little bit.</p>
<p>Either way, the sequences that are too long must be truncated and the sequences that are too short must be padded, typically with zeroes. This does literally mean that words or tokens are thrown away for the long documents and zeroes are added to the shorter documents, with the goal of creating a rectangular matrix that can be used for computation.</p>
<div class="rmdnote">
<p>
It is possible to set up an LSTM network that works with sequences of varied length; this can sometimes improve performance but takes more work to set up and is outside the scope of this book.
</p>
</div>
<p>The default in textrecipes, as well as most deep learning for text, is <code>padding = "pre"</code>, where zeroes are added at the beginning, and <code>truncating = "pre"</code>, where values at the beginning are removed. What happens if we change one of these defaults?</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="dllstm.html#cb549-1" aria-hidden="true" tabindex="-1"></a>padding_rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span> blurb, <span class="at">data =</span> kickstarter_train) <span class="sc">%&gt;%</span></span>
<span id="cb549-2"><a href="dllstm.html#cb549-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenize</span>(blurb) <span class="sc">%&gt;%</span></span>
<span id="cb549-3"><a href="dllstm.html#cb549-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenfilter</span>(blurb, <span class="at">max_tokens =</span> max_words) <span class="sc">%&gt;%</span></span>
<span id="cb549-4"><a href="dllstm.html#cb549-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_sequence_onehot</span>(blurb, <span class="at">sequence_length =</span> max_length, <span class="at">padding =</span> <span class="st">&quot;post&quot;</span>)</span>
<span id="cb549-5"><a href="dllstm.html#cb549-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb549-6"><a href="dllstm.html#cb549-6" aria-hidden="true" tabindex="-1"></a>padding_prep <span class="ot">&lt;-</span> <span class="fu">prep</span>(padding_rec)</span>
<span id="cb549-7"><a href="dllstm.html#cb549-7" aria-hidden="true" tabindex="-1"></a>padding_matrix <span class="ot">&lt;-</span> <span class="fu">bake</span>(padding_prep, <span class="at">new_data =</span> <span class="cn">NULL</span>, <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb549-8"><a href="dllstm.html#cb549-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(padding_matrix)</span></code></pre></div>
<pre><code>#&gt; [1] 202093     30</code></pre>
<p>This matrix has the same dimensions as <code>kick_matrix</code> but instead of padding with zeroes at the beginning of these Kickstarter blurbs, this matrix is padded with zeroes at the end. (This preprocessing strategy still truncates longer sequences in the same way.)</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="dllstm.html#cb551-1" aria-hidden="true" tabindex="-1"></a>pad_analysis <span class="ot">&lt;-</span> <span class="fu">bake</span>(padding_prep, <span class="at">new_data =</span> <span class="fu">analysis</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]),</span>
<span id="cb551-2"><a href="dllstm.html#cb551-2" aria-hidden="true" tabindex="-1"></a>                     <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb551-3"><a href="dllstm.html#cb551-3" aria-hidden="true" tabindex="-1"></a>pad_assess <span class="ot">&lt;-</span> <span class="fu">bake</span>(padding_prep, <span class="at">new_data =</span> <span class="fu">assessment</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]),</span>
<span id="cb551-4"><a href="dllstm.html#cb551-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span></code></pre></div>
<p>Now, let’s create and fit an LSTM to this preprocessed data.</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="dllstm.html#cb552-1" aria-hidden="true" tabindex="-1"></a>padding_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb552-2"><a href="dllstm.html#cb552-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb552-3"><a href="dllstm.html#cb552-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb552-4"><a href="dllstm.html#cb552-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb552-5"><a href="dllstm.html#cb552-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb552-6"><a href="dllstm.html#cb552-6" aria-hidden="true" tabindex="-1"></a>padding_mod <span class="sc">%&gt;%</span></span>
<span id="cb552-7"><a href="dllstm.html#cb552-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb552-8"><a href="dllstm.html#cb552-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb552-9"><a href="dllstm.html#cb552-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb552-10"><a href="dllstm.html#cb552-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb552-11"><a href="dllstm.html#cb552-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb552-12"><a href="dllstm.html#cb552-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb552-13"><a href="dllstm.html#cb552-13" aria-hidden="true" tabindex="-1"></a>padding_history <span class="ot">&lt;-</span> padding_mod <span class="sc">%&gt;%</span></span>
<span id="cb552-14"><a href="dllstm.html#cb552-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb552-15"><a href="dllstm.html#cb552-15" aria-hidden="true" tabindex="-1"></a>    pad_analysis,</span>
<span id="cb552-16"><a href="dllstm.html#cb552-16" aria-hidden="true" tabindex="-1"></a>    state_analysis,</span>
<span id="cb552-17"><a href="dllstm.html#cb552-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb552-18"><a href="dllstm.html#cb552-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(pad_assess, state_assess),</span>
<span id="cb552-19"><a href="dllstm.html#cb552-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb552-20"><a href="dllstm.html#cb552-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb552-21"><a href="dllstm.html#cb552-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb552-22"><a href="dllstm.html#cb552-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb552-23"><a href="dllstm.html#cb552-23" aria-hidden="true" tabindex="-1"></a>padding_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.4404
#&gt;     accuracy: 0.7811
#&gt;     val_loss: 0.5868
#&gt; val_accuracy: 0.7178</code></pre>
<p>This padding strategy results in noticeably worse performance than the default option!</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="dllstm.html#cb554-1" aria-hidden="true" tabindex="-1"></a>padding_res <span class="ot">&lt;-</span> <span class="fu">keras_predict</span>(padding_mod, pad_assess, state_assess)</span>
<span id="cb554-2"><a href="dllstm.html#cb554-2" aria-hidden="true" tabindex="-1"></a>padding_res <span class="sc">%&gt;%</span> <span class="fu">metrics</span>(state, .pred_class, .pred_1)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 3
#&gt;   .metric     .estimator .estimate
#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 accuracy    binary         0.718
#&gt; 2 kap         binary         0.433
#&gt; 3 mn_log_loss binary         0.587
#&gt; 4 roc_auc     binary         0.791</code></pre>
<p>The same model architecture with default padding preprocessing resulted in an accuracy of 0.739 and an AUC of 0.808; changing to <code>padding = "post"</code> has resulted in a remarkable degrading of predictive capacity. This result is typically attributed to the RNN/LSTM’s hidden states being flushed out by the added zeroes, before getting to the text itself.</p>
<div class="rmdwarning">
<p>
Different preprocessing strategies have a huge impact on deep learning results.
</p>
</div>
</div>
<div id="case-study-training-a-regression-model" class="section level2" number="9.6">
<h2><span class="header-section-number">9.6</span> Case study: training a regression model</h2>
<p>All our deep learning models for text so far have used the Kickstarter crowdfunding blurbs to predict whether the campaigns were successful or not, a classification problem. In our experience, classification is more common than regression tasks with text data, but these techniques can be used for either kind of supervised machine learning question. Let’s return to the regression problem of Chapter <a href="mlregression.html#mlregression">6</a> and predict the year of United States Supreme Court decisions, starting out by splitting into training and testing sets.</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="dllstm.html#cb556-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(scotus)</span>
<span id="cb556-2"><a href="dllstm.html#cb556-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb556-3"><a href="dllstm.html#cb556-3" aria-hidden="true" tabindex="-1"></a>scotus_split <span class="ot">&lt;-</span> scotus_filtered <span class="sc">%&gt;%</span></span>
<span id="cb556-4"><a href="dllstm.html#cb556-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb556-5"><a href="dllstm.html#cb556-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">year =</span> (<span class="fu">as.numeric</span>(year) <span class="sc">-</span> <span class="dv">1920</span>) <span class="sc">/</span> <span class="dv">50</span>,</span>
<span id="cb556-6"><a href="dllstm.html#cb556-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">text =</span> <span class="fu">str_remove_all</span>(text, <span class="st">&quot;&#39;&quot;</span>)</span>
<span id="cb556-7"><a href="dllstm.html#cb556-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb556-8"><a href="dllstm.html#cb556-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">initial_split</span>(<span class="at">strata =</span> year)</span>
<span id="cb556-9"><a href="dllstm.html#cb556-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb556-10"><a href="dllstm.html#cb556-10" aria-hidden="true" tabindex="-1"></a>scotus_train <span class="ot">&lt;-</span> <span class="fu">training</span>(scotus_split)</span>
<span id="cb556-11"><a href="dllstm.html#cb556-11" aria-hidden="true" tabindex="-1"></a>scotus_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(scotus_split)</span></code></pre></div>
<div class="rmdwarning">
<p>
Notice that we also shifted (subtracted) and scaled (divided) the <code>year</code> outcome by constant factors so all the values are centered around zero and not too large. Neural networks for regression problems typically behave better when dealing with outcomes that are roughly between -1 and 1.
</p>
</div>
<p>Next, let’s build a preprocessing recipe for these Supreme Court decisions. These documents are much longer than the Kickstarter blurbs, many thousands of words long instead of just a handful. Let’s try keeping the size of our vocabulary the same (<code>max_words</code>) but we will need to increase the sequence length information we store (<code>max_length</code>) by a great deal.</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="dllstm.html#cb557-1" aria-hidden="true" tabindex="-1"></a>max_words <span class="ot">&lt;-</span> <span class="fl">2e4</span></span>
<span id="cb557-2"><a href="dllstm.html#cb557-2" aria-hidden="true" tabindex="-1"></a>max_length <span class="ot">&lt;-</span> <span class="fl">1e3</span></span>
<span id="cb557-3"><a href="dllstm.html#cb557-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb557-4"><a href="dllstm.html#cb557-4" aria-hidden="true" tabindex="-1"></a>scotus_rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span> text, <span class="at">data =</span> scotus_train) <span class="sc">%&gt;%</span></span>
<span id="cb557-5"><a href="dllstm.html#cb557-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenize</span>(text) <span class="sc">%&gt;%</span></span>
<span id="cb557-6"><a href="dllstm.html#cb557-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenfilter</span>(text, <span class="at">max_tokens =</span> max_words) <span class="sc">%&gt;%</span></span>
<span id="cb557-7"><a href="dllstm.html#cb557-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_sequence_onehot</span>(text, <span class="at">sequence_length =</span> max_length)</span>
<span id="cb557-8"><a href="dllstm.html#cb557-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb557-9"><a href="dllstm.html#cb557-9" aria-hidden="true" tabindex="-1"></a>scotus_prep <span class="ot">&lt;-</span> <span class="fu">prep</span>(scotus_rec)</span>
<span id="cb557-10"><a href="dllstm.html#cb557-10" aria-hidden="true" tabindex="-1"></a>scotus_train_baked <span class="ot">&lt;-</span> <span class="fu">bake</span>(scotus_prep,</span>
<span id="cb557-11"><a href="dllstm.html#cb557-11" aria-hidden="true" tabindex="-1"></a>                           <span class="at">new_data =</span> scotus_train,</span>
<span id="cb557-12"><a href="dllstm.html#cb557-12" aria-hidden="true" tabindex="-1"></a>                           <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb557-13"><a href="dllstm.html#cb557-13" aria-hidden="true" tabindex="-1"></a>scotus_test_baked <span class="ot">&lt;-</span> <span class="fu">bake</span>(scotus_prep,</span>
<span id="cb557-14"><a href="dllstm.html#cb557-14" aria-hidden="true" tabindex="-1"></a>                          <span class="at">new_data =</span> scotus_test,</span>
<span id="cb557-15"><a href="dllstm.html#cb557-15" aria-hidden="true" tabindex="-1"></a>                          <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span></code></pre></div>
<p>What does our training data look like now?</p>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="dllstm.html#cb558-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(scotus_train_baked)</span></code></pre></div>
<pre><code>#&gt; [1] 7502 1000</code></pre>
<p>We only have 7502 rows of training data, and because these documents are so long and we want to keep more of each sequence, the training data has 1000 columns. You are probably starting to guess that we are going to run into problems.</p>
<p>Let’s create an LSTM and see what we can do. We will need to use higher-dimensional embeddings, since our sequences are much longer (we may want to increase the number of <code>units</code> as well, but will leave that out for the time being). Because we are training a regression model, there is no activation function for the last layer; we want to fit and predict to arbitrary values for the year.</p>
<div class="rmdnote">
<p>
A good default loss function for regression is mean squared error, <code>“mse”</code>.
</p>
</div>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="dllstm.html#cb560-1" aria-hidden="true" tabindex="-1"></a>scotus_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb560-2"><a href="dllstm.html#cb560-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">64</span>) <span class="sc">%&gt;%</span></span>
<span id="cb560-3"><a href="dllstm.html#cb560-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb560-4"><a href="dllstm.html#cb560-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>)</span>
<span id="cb560-5"><a href="dllstm.html#cb560-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb560-6"><a href="dllstm.html#cb560-6" aria-hidden="true" tabindex="-1"></a>scotus_mod <span class="sc">%&gt;%</span></span>
<span id="cb560-7"><a href="dllstm.html#cb560-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb560-8"><a href="dllstm.html#cb560-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb560-9"><a href="dllstm.html#cb560-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;mse&quot;</span>,</span>
<span id="cb560-10"><a href="dllstm.html#cb560-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;mean_squared_error&quot;</span>)</span>
<span id="cb560-11"><a href="dllstm.html#cb560-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb560-12"><a href="dllstm.html#cb560-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb560-13"><a href="dllstm.html#cb560-13" aria-hidden="true" tabindex="-1"></a>scotus_history <span class="ot">&lt;-</span> scotus_mod <span class="sc">%&gt;%</span></span>
<span id="cb560-14"><a href="dllstm.html#cb560-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb560-15"><a href="dllstm.html#cb560-15" aria-hidden="true" tabindex="-1"></a>    scotus_train_baked,</span>
<span id="cb560-16"><a href="dllstm.html#cb560-16" aria-hidden="true" tabindex="-1"></a>    scotus_train<span class="sc">$</span>year,</span>
<span id="cb560-17"><a href="dllstm.html#cb560-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb560-18"><a href="dllstm.html#cb560-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_split =</span> <span class="fl">0.25</span>,</span>
<span id="cb560-19"><a href="dllstm.html#cb560-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb560-20"><a href="dllstm.html#cb560-20" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>How does this model perform on the test data? Let’s transform back to real values for <code>year</code> so our metrics will be on the same scale as in Chapter <a href="mlregression.html#mlregression">6</a>.</p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="dllstm.html#cb561-1" aria-hidden="true" tabindex="-1"></a>scotus_res <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">year =</span> scotus_test<span class="sc">$</span>year,</span>
<span id="cb561-2"><a href="dllstm.html#cb561-2" aria-hidden="true" tabindex="-1"></a>                     <span class="at">.pred =</span> <span class="fu">predict</span>(scotus_mod, scotus_test_baked)[, <span class="dv">1</span>]) <span class="sc">%&gt;%</span></span>
<span id="cb561-3"><a href="dllstm.html#cb561-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">everything</span>(), <span class="sc">~</span> . <span class="sc">*</span> <span class="dv">50</span> <span class="sc">+</span> <span class="dv">1920</span>))</span>
<span id="cb561-4"><a href="dllstm.html#cb561-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb561-5"><a href="dllstm.html#cb561-5" aria-hidden="true" tabindex="-1"></a>scotus_res <span class="sc">%&gt;%</span> <span class="fu">metrics</span>(year, .pred)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 3 x 3
#&gt;   .metric .estimator .estimate
#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 rmse    standard      18.4  
#&gt; 2 rsq     standard       0.862
#&gt; 3 mae     standard      13.6</code></pre>
<p>This is much worse than the final regularized linear model trained in Section <a href="mlregression.html#mlregressionfull">6.9</a>, with an RMSE almost a decade worth of years worse. It’s possible we may be able to do a little better than this simple LSTM, but as this chapter has demonstrated, our improvements will likely not be enormous compared to the first LSTM baseline.</p>
<div class="rmdwarning">
<p>
The main problem with this regression model is that there isn’t that much data to start with; this is an example where a deep learning model is <em>not</em> a good choice and we should stick with a different machine learning algorithm like regularized regression.
</p>
</div>
</div>
<div id="case-study-vocabulary-size" class="section level2" number="9.7">
<h2><span class="header-section-number">9.7</span> Case study: vocabulary size</h2>
<p>In this chapter so far, we’ve worked with a vocabulary of 20,000 words or tokens. This is a <em>hyperparameter</em> of the model, and could be tuned, as we show in detail in Section <a href="dlcnn.html#keras-hyperparameter">10.6</a>. Instead of tuning in this chapter, let’s try a smaller value, corresponding to faster preprocessing and model fitting but a less powerful model, and explore whether and how much it affects model performance.</p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="dllstm.html#cb563-1" aria-hidden="true" tabindex="-1"></a>max_words <span class="ot">&lt;-</span> <span class="fl">1e4</span></span>
<span id="cb563-2"><a href="dllstm.html#cb563-2" aria-hidden="true" tabindex="-1"></a>max_length <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb563-3"><a href="dllstm.html#cb563-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb563-4"><a href="dllstm.html#cb563-4" aria-hidden="true" tabindex="-1"></a>smaller_rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span> blurb, <span class="at">data =</span> kickstarter_train) <span class="sc">%&gt;%</span></span>
<span id="cb563-5"><a href="dllstm.html#cb563-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenize</span>(blurb) <span class="sc">%&gt;%</span></span>
<span id="cb563-6"><a href="dllstm.html#cb563-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenfilter</span>(blurb, <span class="at">max_tokens =</span> max_words) <span class="sc">%&gt;%</span></span>
<span id="cb563-7"><a href="dllstm.html#cb563-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_sequence_onehot</span>(blurb, <span class="at">sequence_length =</span> max_length)</span>
<span id="cb563-8"><a href="dllstm.html#cb563-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb563-9"><a href="dllstm.html#cb563-9" aria-hidden="true" tabindex="-1"></a>kick_prep <span class="ot">&lt;-</span> <span class="fu">prep</span>(smaller_rec)</span>
<span id="cb563-10"><a href="dllstm.html#cb563-10" aria-hidden="true" tabindex="-1"></a>kick_analysis <span class="ot">&lt;-</span> <span class="fu">bake</span>(kick_prep, <span class="at">new_data =</span> <span class="fu">analysis</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]),</span>
<span id="cb563-11"><a href="dllstm.html#cb563-11" aria-hidden="true" tabindex="-1"></a>                      <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb563-12"><a href="dllstm.html#cb563-12" aria-hidden="true" tabindex="-1"></a>kick_assess <span class="ot">&lt;-</span> <span class="fu">bake</span>(kick_prep, <span class="at">new_data =</span> <span class="fu">assessment</span>(kick_val<span class="sc">$</span>splits[[<span class="dv">1</span>]]),</span>
<span id="cb563-13"><a href="dllstm.html#cb563-13" aria-hidden="true" tabindex="-1"></a>                    <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span></code></pre></div>
<p>Once our preprocessing is done and applied to our validation split <code>kick_val</code>, we can set up our model, another straightforward LSTM neural network.</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="dllstm.html#cb564-1" aria-hidden="true" tabindex="-1"></a>smaller_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb564-2"><a href="dllstm.html#cb564-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb564-3"><a href="dllstm.html#cb564-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb564-4"><a href="dllstm.html#cb564-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb564-5"><a href="dllstm.html#cb564-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb564-6"><a href="dllstm.html#cb564-6" aria-hidden="true" tabindex="-1"></a>smaller_mod <span class="sc">%&gt;%</span></span>
<span id="cb564-7"><a href="dllstm.html#cb564-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb564-8"><a href="dllstm.html#cb564-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb564-9"><a href="dllstm.html#cb564-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb564-10"><a href="dllstm.html#cb564-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb564-11"><a href="dllstm.html#cb564-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb564-12"><a href="dllstm.html#cb564-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb564-13"><a href="dllstm.html#cb564-13" aria-hidden="true" tabindex="-1"></a>smaller_history <span class="ot">&lt;-</span> smaller_mod <span class="sc">%&gt;%</span></span>
<span id="cb564-14"><a href="dllstm.html#cb564-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb564-15"><a href="dllstm.html#cb564-15" aria-hidden="true" tabindex="-1"></a>    kick_analysis,</span>
<span id="cb564-16"><a href="dllstm.html#cb564-16" aria-hidden="true" tabindex="-1"></a>    state_analysis,</span>
<span id="cb564-17"><a href="dllstm.html#cb564-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb564-18"><a href="dllstm.html#cb564-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_data =</span> <span class="fu">list</span>(kick_assess, state_assess),</span>
<span id="cb564-19"><a href="dllstm.html#cb564-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb564-20"><a href="dllstm.html#cb564-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb564-21"><a href="dllstm.html#cb564-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb564-22"><a href="dllstm.html#cb564-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb564-23"><a href="dllstm.html#cb564-23" aria-hidden="true" tabindex="-1"></a>smaller_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.4712
#&gt;     accuracy: 0.7669
#&gt;     val_loss: 0.582
#&gt; val_accuracy: 0.7093</code></pre>
<p>How did this smaller model, based on a smaller vocabulary in the model, perform?</p>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb566-1"><a href="dllstm.html#cb566-1" aria-hidden="true" tabindex="-1"></a>smaller_res <span class="ot">&lt;-</span> <span class="fu">keras_predict</span>(smaller_mod, kick_assess, state_assess)</span>
<span id="cb566-2"><a href="dllstm.html#cb566-2" aria-hidden="true" tabindex="-1"></a>smaller_res <span class="sc">%&gt;%</span> <span class="fu">metrics</span>(state, .pred_class, .pred_1)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 3
#&gt;   .metric     .estimator .estimate
#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 accuracy    binary         0.709
#&gt; 2 kap         binary         0.417
#&gt; 3 mn_log_loss binary         0.582
#&gt; 4 roc_auc     binary         0.782</code></pre>
<p>The original LSTM model with the larger vocabulary had an accuracy of 0.739 and an AUC of 0.808. Reducing the model’s capacity to capture and learn text meaning by restricting its access to vocabulary does result in a corresponding reduction in model performance, but a small one.</p>
<div class="rmdnote">
<p>
The relationship between this hyperparameter and model performance is weak over this range. Notice that we cut the vocabulary in half, and saw only modest reductions in accuracy.
</p>
</div>
</div>
<div id="lstmfull" class="section level2" number="9.8">
<h2><span class="header-section-number">9.8</span> The full game: LSTM</h2>
<p>We’ve come a long way in this chapter, even though we’ve focused on a very specific kind of recurrent neural network, the LSTM. Let’s step back and build one final model, incorporating what we have been able to learn.</p>
<div id="lstmfullpreprocess" class="section level3" number="9.8.1">
<h3><span class="header-section-number">9.8.1</span> Preprocess the data</h3>
<p>We know that we want to stick with the defaults for padding, and to use a larger vocabulary for our final model. For this final model, we are not going to use our validation split again, so we only need to preprocess the training data.</p>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="dllstm.html#cb568-1" aria-hidden="true" tabindex="-1"></a>max_words <span class="ot">&lt;-</span> <span class="fl">2e4</span></span>
<span id="cb568-2"><a href="dllstm.html#cb568-2" aria-hidden="true" tabindex="-1"></a>max_length <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb568-3"><a href="dllstm.html#cb568-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb568-4"><a href="dllstm.html#cb568-4" aria-hidden="true" tabindex="-1"></a>kick_rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(<span class="sc">~</span> blurb, <span class="at">data =</span> kickstarter_train) <span class="sc">%&gt;%</span></span>
<span id="cb568-5"><a href="dllstm.html#cb568-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenize</span>(blurb) <span class="sc">%&gt;%</span></span>
<span id="cb568-6"><a href="dllstm.html#cb568-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_tokenfilter</span>(blurb, <span class="at">max_tokens =</span> max_words) <span class="sc">%&gt;%</span></span>
<span id="cb568-7"><a href="dllstm.html#cb568-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">step_sequence_onehot</span>(blurb, <span class="at">sequence_length =</span> max_length)</span>
<span id="cb568-8"><a href="dllstm.html#cb568-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb568-9"><a href="dllstm.html#cb568-9" aria-hidden="true" tabindex="-1"></a>kick_prep <span class="ot">&lt;-</span> <span class="fu">prep</span>(kick_rec)</span>
<span id="cb568-10"><a href="dllstm.html#cb568-10" aria-hidden="true" tabindex="-1"></a>kick_matrix <span class="ot">&lt;-</span> <span class="fu">bake</span>(kick_prep, <span class="at">new_data =</span> <span class="cn">NULL</span>, <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb568-11"><a href="dllstm.html#cb568-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb568-12"><a href="dllstm.html#cb568-12" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(kick_matrix)</span></code></pre></div>
<pre><code>#&gt; [1] 202093     30</code></pre>
</div>
<div id="lstmfullmodel" class="section level3" number="9.8.2">
<h3><span class="header-section-number">9.8.2</span> Specify the model</h3>
<p>We’ve learned a lot about how to model this data set over the course of this chapter.</p>
<ul>
<li>We can use <code>dropout</code> to reduce overfitting.</li>
<li>Let’s stack several layers together, and in fact increase the number of LSTM layers to three.</li>
<li>The bidirectional LSTM performed better than the regular LSTM, so let’s set up each LSTM layer to be able to learn sequences in both directions.</li>
</ul>
<p>Instead of using specific validation data that we can then compute performance metrics for, let’s go back to specifying <code>validation_split = 0.1</code> and let the Keras model choose the validation set.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="dllstm.html#cb570-1" aria-hidden="true" tabindex="-1"></a>final_mod <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() <span class="sc">%&gt;%</span></span>
<span id="cb570-2"><a href="dllstm.html#cb570-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_words <span class="sc">+</span> <span class="dv">1</span>, <span class="at">output_dim =</span> <span class="dv">32</span>) <span class="sc">%&gt;%</span></span>
<span id="cb570-3"><a href="dllstm.html#cb570-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bidirectional</span>(<span class="fu">layer_lstm</span>(</span>
<span id="cb570-4"><a href="dllstm.html#cb570-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>,</span>
<span id="cb570-5"><a href="dllstm.html#cb570-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">return_sequences =</span> <span class="cn">TRUE</span></span>
<span id="cb570-6"><a href="dllstm.html#cb570-6" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb570-7"><a href="dllstm.html#cb570-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bidirectional</span>(<span class="fu">layer_lstm</span>(</span>
<span id="cb570-8"><a href="dllstm.html#cb570-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span>,</span>
<span id="cb570-9"><a href="dllstm.html#cb570-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">return_sequences =</span> <span class="cn">TRUE</span></span>
<span id="cb570-10"><a href="dllstm.html#cb570-10" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb570-11"><a href="dllstm.html#cb570-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bidirectional</span>(<span class="fu">layer_lstm</span>(</span>
<span id="cb570-12"><a href="dllstm.html#cb570-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">units =</span> <span class="dv">32</span>, <span class="at">dropout =</span> <span class="fl">0.4</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.4</span></span>
<span id="cb570-13"><a href="dllstm.html#cb570-13" aria-hidden="true" tabindex="-1"></a>  )) <span class="sc">%&gt;%</span></span>
<span id="cb570-14"><a href="dllstm.html#cb570-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&quot;sigmoid&quot;</span>)</span>
<span id="cb570-15"><a href="dllstm.html#cb570-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb570-16"><a href="dllstm.html#cb570-16" aria-hidden="true" tabindex="-1"></a>final_mod <span class="sc">%&gt;%</span></span>
<span id="cb570-17"><a href="dllstm.html#cb570-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compile</span>(</span>
<span id="cb570-18"><a href="dllstm.html#cb570-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">optimizer =</span> <span class="st">&quot;adam&quot;</span>,</span>
<span id="cb570-19"><a href="dllstm.html#cb570-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">loss =</span> <span class="st">&quot;binary_crossentropy&quot;</span>,</span>
<span id="cb570-20"><a href="dllstm.html#cb570-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>)</span>
<span id="cb570-21"><a href="dllstm.html#cb570-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb570-22"><a href="dllstm.html#cb570-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb570-23"><a href="dllstm.html#cb570-23" aria-hidden="true" tabindex="-1"></a>final_history <span class="ot">&lt;-</span> final_mod <span class="sc">%&gt;%</span></span>
<span id="cb570-24"><a href="dllstm.html#cb570-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(</span>
<span id="cb570-25"><a href="dllstm.html#cb570-25" aria-hidden="true" tabindex="-1"></a>    kick_matrix,</span>
<span id="cb570-26"><a href="dllstm.html#cb570-26" aria-hidden="true" tabindex="-1"></a>    kickstarter_train<span class="sc">$</span>state,</span>
<span id="cb570-27"><a href="dllstm.html#cb570-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">epochs =</span> <span class="dv">10</span>,</span>
<span id="cb570-28"><a href="dllstm.html#cb570-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">validation_split =</span> <span class="fl">0.1</span>,</span>
<span id="cb570-29"><a href="dllstm.html#cb570-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">batch_size =</span> <span class="dv">512</span>,</span>
<span id="cb570-30"><a href="dllstm.html#cb570-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">verbose =</span> <span class="cn">FALSE</span></span>
<span id="cb570-31"><a href="dllstm.html#cb570-31" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb570-32"><a href="dllstm.html#cb570-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb570-33"><a href="dllstm.html#cb570-33" aria-hidden="true" tabindex="-1"></a>final_history</span></code></pre></div>
<pre><code>#&gt; 
#&gt; Final epoch (plot to see history):
#&gt;         loss: 0.3358
#&gt;     accuracy: 0.8487
#&gt;     val_loss: 0.5305
#&gt; val_accuracy: 0.7794</code></pre>
<p>This looks promising! Let’s finally turn to the testing set, for the first time during this chapter, to evaluate this last model on data that has never been touched as part of the fitting process.</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="dllstm.html#cb572-1" aria-hidden="true" tabindex="-1"></a>kick_matrix_test <span class="ot">&lt;-</span> <span class="fu">bake</span>(kick_prep, <span class="at">new_data =</span> kickstarter_test,</span>
<span id="cb572-2"><a href="dllstm.html#cb572-2" aria-hidden="true" tabindex="-1"></a>                         <span class="at">composition =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb572-3"><a href="dllstm.html#cb572-3" aria-hidden="true" tabindex="-1"></a>final_res <span class="ot">&lt;-</span> <span class="fu">keras_predict</span>(final_mod, kick_matrix_test, kickstarter_test<span class="sc">$</span>state)</span>
<span id="cb572-4"><a href="dllstm.html#cb572-4" aria-hidden="true" tabindex="-1"></a>final_res <span class="sc">%&gt;%</span> <span class="fu">metrics</span>(state, .pred_class, .pred_1)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4 x 3
#&gt;   .metric     .estimator .estimate
#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
#&gt; 1 accuracy    binary         0.763
#&gt; 2 kap         binary         0.524
#&gt; 3 mn_log_loss binary         0.565
#&gt; 4 roc_auc     binary         0.832</code></pre>
<p>This is our best performing model in this chapter on LSTM models, although not by much. We can again create an ROC curve, this time using the test data in Figure <a href="dllstm.html#fig:lstmfinalroc">9.6</a>.</p>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="dllstm.html#cb574-1" aria-hidden="true" tabindex="-1"></a>final_res <span class="sc">%&gt;%</span></span>
<span id="cb574-2"><a href="dllstm.html#cb574-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">roc_curve</span>(state, .pred_1) <span class="sc">%&gt;%</span></span>
<span id="cb574-3"><a href="dllstm.html#cb574-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:lstmfinalroc"></span>
<img src="09_dl_lstm_files/figure-html/lstmfinalroc-1.png" alt="ROC curve for final LSTM model predictions on testing set of Kickstarter campaign success" width="672" />
<p class="caption">
FIGURE 9.6: ROC curve for final LSTM model predictions on testing set of Kickstarter campaign success
</p>
</div>
<p>We have been able to incrementally improve our model by adding to the structure and making good choices about preprocessing. We can visualize this final LSTM model’s performance using a confusion matrix as well, in Figure <a href="dllstm.html#fig:lstmheatmap">9.7</a>.</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="dllstm.html#cb575-1" aria-hidden="true" tabindex="-1"></a>final_res <span class="sc">%&gt;%</span></span>
<span id="cb575-2"><a href="dllstm.html#cb575-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(state, .pred_class) <span class="sc">%&gt;%</span></span>
<span id="cb575-3"><a href="dllstm.html#cb575-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">autoplot</span>(<span class="at">type =</span> <span class="st">&quot;heatmap&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:lstmheatmap"></span>
<img src="09_dl_lstm_files/figure-html/lstmheatmap-1.png" alt="Confusion matrix for final LSTM model predictions on testing set of Kickstarter campaign success" width="672" />
<p class="caption">
FIGURE 9.7: Confusion matrix for final LSTM model predictions on testing set of Kickstarter campaign success
</p>
</div>
<p>Notice that this final model still does not perform as well as any of the best models of Chapter <a href="dldnn.html#dldnn">8</a>.</p>
<div class="rmdnote">
<p>
For this data set of Kickstarter campaign blurbs, an LSTM architecture is not turning out to give a great result compared to other options. However, LSTMs typically perform very well for text data and are an important piece of the text modeling toolkit.
</p>
</div>
<p>For the Kickstarter data, these less-than-spectacular results are likely due to the documents’ short lengths. LSTMs often work well for text data, but this is not universally true for all kinds of text. Also, keep in mind that LSTMs take both more time and memory to train, compared to the simpler models discussed in Chapter <a href="dldnn.html#dldnn">8</a>.</p>
</div>
</div>
<div id="dllstmsummary" class="section level2" number="9.9">
<h2><span class="header-section-number">9.9</span> Summary</h2>
<p>LSTMs are a specific kind of recurrent neural network that are capable of learning long-range dependencies and broader context. They are often an excellent choice for building supervised models for text because of this ability to model sequences and structures within text like word dependencies. Text must be heavily preprocessed for LSTMs in much the same way it needs to be preprocessed for dense neural networks, with tokenization and one-hot encoding of sequences. A major characteristic of LSTMs, like other deep learning architectures, is their tendency to memorize the features of training data; we can use strategies like dropout and ensuring that the batch size is large enough to reduce overfitting.</p>
<div id="in-this-chapter-you-learned-8" class="section level3" number="9.9.1">
<h3><span class="header-section-number">9.9.1</span> In this chapter, you learned:</h3>
<ul>
<li>how to preprocess text data for LSTM models</li>
<li>about RNN, LSTM, and bidirectional LSTM network architectures</li>
<li>how to use dropout to reduce overfitting for deep learning models</li>
<li>that network layers (including RNNs and LSTMs) can be stacked for greater model capacity</li>
<li>about the importance of centering and scaling regression outcomes for neural networks</li>
<li>how to evaluate LSTM models for text</li>
</ul>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>Vanilla neural networks do not have this ability for information to persist at all; they start learning from scratch at every step.<a href="dllstm.html#fnref15" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dldnn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dlcnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/EmilHvitfeldt/smltar/edit/master/09_dl_lstm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
