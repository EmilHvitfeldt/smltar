<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Word Embeddings | Supervised Machine Learning for Text Analysis in R</title>
  <meta name="description" content="Chapter 5 Word Embeddings | Supervised Machine Learning for Text Analysis in R" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Word Embeddings | Supervised Machine Learning for Text Analysis in R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://smltar.com" />
  <meta property="og:image" content="https://smltar.com/cover.jpg" />
  <meta property="og:description" content="Chapter 5 Word Embeddings | Supervised Machine Learning for Text Analysis in R" />
  <meta name="github-repo" content="EmilHvitfeldt/smltar" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Word Embeddings | Supervised Machine Learning for Text Analysis in R" />
  
  <meta name="twitter:description" content="Chapter 5 Word Embeddings | Supervised Machine Learning for Text Analysis in R" />
  <meta name="twitter:image" content="https://smltar.com/cover.jpg" />

<meta name="author" content="Emil Hvitfeldt and Julia Silge" />


<meta name="date" content="2021-11-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="stemming.html"/>
<link rel="next" href="mloverview.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/plot_text_explanations-0.1.0/plot_text_explanations.css" rel="stylesheet" />
<script src="libs/plot_text_explanations-binding-0.5.2/plot_text_explanations.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="smltar.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Supervised Machine Learning for Text Analysis in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Supervised Machine Learning for Text Analysis in R</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#outline"><i class="fa fa-check"></i>Outline</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#topics-this-book-will-not-cover"><i class="fa fa-check"></i>Topics this book will not cover</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#colophon"><i class="fa fa-check"></i>Colophon</a></li>
</ul></li>
<li class="part"><span><b>I Natural Language Features</b></span></li>
<li class="chapter" data-level="1" data-path="language.html"><a href="language.html"><i class="fa fa-check"></i><b>1</b> Language and modeling</a>
<ul>
<li class="chapter" data-level="1.1" data-path="language.html"><a href="language.html#linguistics-for-text-analysis"><i class="fa fa-check"></i><b>1.1</b> Linguistics for text analysis</a></li>
<li class="chapter" data-level="1.2" data-path="language.html"><a href="language.html#morphology"><i class="fa fa-check"></i><b>1.2</b> A glimpse into one area: morphology</a></li>
<li class="chapter" data-level="1.3" data-path="language.html"><a href="language.html#different-languages"><i class="fa fa-check"></i><b>1.3</b> Different languages</a></li>
<li class="chapter" data-level="1.4" data-path="language.html"><a href="language.html#other-ways-text-can-vary"><i class="fa fa-check"></i><b>1.4</b> Other ways text can vary</a></li>
<li class="chapter" data-level="1.5" data-path="language.html"><a href="language.html#languagesummary"><i class="fa fa-check"></i><b>1.5</b> Summary</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="language.html"><a href="language.html#in-this-chapter-you-learned"><i class="fa fa-check"></i><b>1.5.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tokenization.html"><a href="tokenization.html"><i class="fa fa-check"></i><b>2</b> Tokenization</a>
<ul>
<li class="chapter" data-level="2.1" data-path="tokenization.html"><a href="tokenization.html#what-is-a-token"><i class="fa fa-check"></i><b>2.1</b> What is a token?</a></li>
<li class="chapter" data-level="2.2" data-path="tokenization.html"><a href="tokenization.html#types-of-tokens"><i class="fa fa-check"></i><b>2.2</b> Types of tokens</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="tokenization.html"><a href="tokenization.html#character-tokens"><i class="fa fa-check"></i><b>2.2.1</b> Character tokens</a></li>
<li class="chapter" data-level="2.2.2" data-path="tokenization.html"><a href="tokenization.html#word-tokens"><i class="fa fa-check"></i><b>2.2.2</b> Word tokens</a></li>
<li class="chapter" data-level="2.2.3" data-path="tokenization.html"><a href="tokenization.html#tokenizingngrams"><i class="fa fa-check"></i><b>2.2.3</b> Tokenizing by n-grams</a></li>
<li class="chapter" data-level="2.2.4" data-path="tokenization.html"><a href="tokenization.html#lines-sentence-and-paragraph-tokens"><i class="fa fa-check"></i><b>2.2.4</b> Lines, sentence, and paragraph tokens</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="tokenization.html"><a href="tokenization.html#where-does-tokenization-break-down"><i class="fa fa-check"></i><b>2.3</b> Where does tokenization break down?</a></li>
<li class="chapter" data-level="2.4" data-path="tokenization.html"><a href="tokenization.html#building-your-own-tokenizer"><i class="fa fa-check"></i><b>2.4</b> Building your own tokenizer</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="tokenization.html"><a href="tokenization.html#tokenize-to-characters-only-keeping-letters"><i class="fa fa-check"></i><b>2.4.1</b> Tokenize to characters, only keeping letters</a></li>
<li class="chapter" data-level="2.4.2" data-path="tokenization.html"><a href="tokenization.html#allow-for-hyphenated-words"><i class="fa fa-check"></i><b>2.4.2</b> Allow for hyphenated words</a></li>
<li class="chapter" data-level="2.4.3" data-path="tokenization.html"><a href="tokenization.html#wrapping-it-in-a-function"><i class="fa fa-check"></i><b>2.4.3</b> Wrapping it in a function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="tokenization.html"><a href="tokenization.html#tokenization-for-non-latin-alphabets"><i class="fa fa-check"></i><b>2.5</b> Tokenization for non-Latin alphabets</a></li>
<li class="chapter" data-level="2.6" data-path="tokenization.html"><a href="tokenization.html#tokenization-benchmark"><i class="fa fa-check"></i><b>2.6</b> Tokenization benchmark</a></li>
<li class="chapter" data-level="2.7" data-path="tokenization.html"><a href="tokenization.html#tokensummary"><i class="fa fa-check"></i><b>2.7</b> Summary</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="tokenization.html"><a href="tokenization.html#in-this-chapter-you-learned-1"><i class="fa fa-check"></i><b>2.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="stopwords.html"><a href="stopwords.html"><i class="fa fa-check"></i><b>3</b> Stop words</a>
<ul>
<li class="chapter" data-level="3.1" data-path="stopwords.html"><a href="stopwords.html#premadestopwords"><i class="fa fa-check"></i><b>3.1</b> Using premade stop word lists</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="stopwords.html"><a href="stopwords.html#stop-word-removal-in-r"><i class="fa fa-check"></i><b>3.1.1</b> Stop word removal in R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="stopwords.html"><a href="stopwords.html#homemadestopwords"><i class="fa fa-check"></i><b>3.2</b> Creating your own stop words list</a></li>
<li class="chapter" data-level="3.3" data-path="stopwords.html"><a href="stopwords.html#all-stop-word-lists-are-context-specific"><i class="fa fa-check"></i><b>3.3</b> All stop word lists are context-specific</a></li>
<li class="chapter" data-level="3.4" data-path="stopwords.html"><a href="stopwords.html#what-happens-when-you-remove-stop-words"><i class="fa fa-check"></i><b>3.4</b> What happens when you remove stop words</a></li>
<li class="chapter" data-level="3.5" data-path="stopwords.html"><a href="stopwords.html#stop-words-in-languages-other-than-english"><i class="fa fa-check"></i><b>3.5</b> Stop words in languages other than English</a></li>
<li class="chapter" data-level="3.6" data-path="stopwords.html"><a href="stopwords.html#stopwordssummary"><i class="fa fa-check"></i><b>3.6</b> Summary</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="stopwords.html"><a href="stopwords.html#in-this-chapter-you-learned-2"><i class="fa fa-check"></i><b>3.6.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stemming.html"><a href="stemming.html"><i class="fa fa-check"></i><b>4</b> Stemming</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stemming.html"><a href="stemming.html#how-to-stem-text-in-r"><i class="fa fa-check"></i><b>4.1</b> How to stem text in R</a></li>
<li class="chapter" data-level="4.2" data-path="stemming.html"><a href="stemming.html#should-you-use-stemming-at-all"><i class="fa fa-check"></i><b>4.2</b> Should you use stemming at all?</a></li>
<li class="chapter" data-level="4.3" data-path="stemming.html"><a href="stemming.html#understand-a-stemming-algorithm"><i class="fa fa-check"></i><b>4.3</b> Understand a stemming algorithm</a></li>
<li class="chapter" data-level="4.4" data-path="stemming.html"><a href="stemming.html#handling-punctuation-when-stemming"><i class="fa fa-check"></i><b>4.4</b> Handling punctuation when stemming</a></li>
<li class="chapter" data-level="4.5" data-path="stemming.html"><a href="stemming.html#compare-some-stemming-options"><i class="fa fa-check"></i><b>4.5</b> Compare some stemming options</a></li>
<li class="chapter" data-level="4.6" data-path="stemming.html"><a href="stemming.html#lemmatization"><i class="fa fa-check"></i><b>4.6</b> Lemmatization and stemming</a></li>
<li class="chapter" data-level="4.7" data-path="stemming.html"><a href="stemming.html#stemming-and-stop-words"><i class="fa fa-check"></i><b>4.7</b> Stemming and stop words</a></li>
<li class="chapter" data-level="4.8" data-path="stemming.html"><a href="stemming.html#stemmingsummary"><i class="fa fa-check"></i><b>4.8</b> Summary</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="stemming.html"><a href="stemming.html#in-this-chapter-you-learned-3"><i class="fa fa-check"></i><b>4.8.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="embeddings.html"><a href="embeddings.html"><i class="fa fa-check"></i><b>5</b> Word Embeddings</a>
<ul>
<li class="chapter" data-level="5.1" data-path="embeddings.html"><a href="embeddings.html#motivatingsparse"><i class="fa fa-check"></i><b>5.1</b> Motivating embeddings for sparse, high-dimensional data</a></li>
<li class="chapter" data-level="5.2" data-path="embeddings.html"><a href="embeddings.html#understand-word-embeddings-by-finding-them-yourself"><i class="fa fa-check"></i><b>5.2</b> Understand word embeddings by finding them yourself</a></li>
<li class="chapter" data-level="5.3" data-path="embeddings.html"><a href="embeddings.html#exploring-cfpb-word-embeddings"><i class="fa fa-check"></i><b>5.3</b> Exploring CFPB word embeddings</a></li>
<li class="chapter" data-level="5.4" data-path="embeddings.html"><a href="embeddings.html#glove"><i class="fa fa-check"></i><b>5.4</b> Use pre-trained word embeddings</a></li>
<li class="chapter" data-level="5.5" data-path="embeddings.html"><a href="embeddings.html#fairnessembeddings"><i class="fa fa-check"></i><b>5.5</b> Fairness and word embeddings</a></li>
<li class="chapter" data-level="5.6" data-path="embeddings.html"><a href="embeddings.html#using-word-embeddings-in-the-real-world"><i class="fa fa-check"></i><b>5.6</b> Using word embeddings in the real world</a></li>
<li class="chapter" data-level="5.7" data-path="embeddings.html"><a href="embeddings.html#embeddingssummary"><i class="fa fa-check"></i><b>5.7</b> Summary</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="embeddings.html"><a href="embeddings.html#in-this-chapter-you-learned-4"><i class="fa fa-check"></i><b>5.7.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Machine Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html#should-we-even-be-doing-this"><i class="fa fa-check"></i>Should we even be doing this?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html#what-bias-is-already-in-the-data"><i class="fa fa-check"></i>What bias is already in the data?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html#can-the-code-and-data-be-audited"><i class="fa fa-check"></i>Can the code and data be audited?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html#what-are-the-error-rates-for-sub-groups"><i class="fa fa-check"></i>What are the error rates for sub-groups?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html#what-is-the-accuracy-of-a-simple-rule-based-alternative"><i class="fa fa-check"></i>What is the accuracy of a simple rule-based alternative?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html#what-processes-are-in-place-to-handle-appeals-or-mistakes"><i class="fa fa-check"></i>What processes are in place to handle appeals or mistakes?</a></li>
<li class="chapter" data-level="" data-path="mloverview.html"><a href="mloverview.html#how-diverse-is-the-team-that-built-it"><i class="fa fa-check"></i>How diverse is the team that built it?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlregression.html"><a href="mlregression.html"><i class="fa fa-check"></i><b>6</b> Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mlregression.html"><a href="mlregression.html#firstmlregression"><i class="fa fa-check"></i><b>6.1</b> A first regression model</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="mlregression.html"><a href="mlregression.html#firstregression"><i class="fa fa-check"></i><b>6.1.1</b> Building our first regression model</a></li>
<li class="chapter" data-level="6.1.2" data-path="mlregression.html"><a href="mlregression.html#firstregressionevaluation"><i class="fa fa-check"></i><b>6.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="mlregression.html"><a href="mlregression.html#regnull"><i class="fa fa-check"></i><b>6.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="6.3" data-path="mlregression.html"><a href="mlregression.html#comparerf"><i class="fa fa-check"></i><b>6.3</b> Compare to a random forest model</a></li>
<li class="chapter" data-level="6.4" data-path="mlregression.html"><a href="mlregression.html#casestudystopwords"><i class="fa fa-check"></i><b>6.4</b> Case study: removing stop words</a></li>
<li class="chapter" data-level="6.5" data-path="mlregression.html"><a href="mlregression.html#casestudyngrams"><i class="fa fa-check"></i><b>6.5</b> Case study: varying n-grams</a></li>
<li class="chapter" data-level="6.6" data-path="mlregression.html"><a href="mlregression.html#mlregressionlemmatization"><i class="fa fa-check"></i><b>6.6</b> Case study: lemmatization</a></li>
<li class="chapter" data-level="6.7" data-path="mlregression.html"><a href="mlregression.html#case-study-feature-hashing"><i class="fa fa-check"></i><b>6.7</b> Case study: feature hashing</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="mlregression.html"><a href="mlregression.html#text-normalization"><i class="fa fa-check"></i><b>6.7.1</b> Text normalization</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="mlregression.html"><a href="mlregression.html#what-evaluation-metrics-are-appropriate"><i class="fa fa-check"></i><b>6.8</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="6.9" data-path="mlregression.html"><a href="mlregression.html#mlregressionfull"><i class="fa fa-check"></i><b>6.9</b> The full game: regression</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="mlregression.html"><a href="mlregression.html#preprocess-the-data"><i class="fa fa-check"></i><b>6.9.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="6.9.2" data-path="mlregression.html"><a href="mlregression.html#specify-the-model"><i class="fa fa-check"></i><b>6.9.2</b> Specify the model</a></li>
<li class="chapter" data-level="6.9.3" data-path="mlregression.html"><a href="mlregression.html#tune-the-model"><i class="fa fa-check"></i><b>6.9.3</b> Tune the model</a></li>
<li class="chapter" data-level="6.9.4" data-path="mlregression.html"><a href="mlregression.html#regression-final-evaluation"><i class="fa fa-check"></i><b>6.9.4</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="mlregression.html"><a href="mlregression.html#mlregressionsummary"><i class="fa fa-check"></i><b>6.10</b> Summary</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="mlregression.html"><a href="mlregression.html#in-this-chapter-you-learned-5"><i class="fa fa-check"></i><b>6.10.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mlclassification.html"><a href="mlclassification.html"><i class="fa fa-check"></i><b>7</b> Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mlclassification.html"><a href="mlclassification.html#classfirstattemptlookatdata"><i class="fa fa-check"></i><b>7.1</b> A first classification model</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="mlclassification.html"><a href="mlclassification.html#classfirstmodel"><i class="fa fa-check"></i><b>7.1.1</b> Building our first classification model</a></li>
<li class="chapter" data-level="7.1.2" data-path="mlclassification.html"><a href="mlclassification.html#evaluation"><i class="fa fa-check"></i><b>7.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mlclassification.html"><a href="mlclassification.html#classnull"><i class="fa fa-check"></i><b>7.2</b> Compare to the null model</a></li>
<li class="chapter" data-level="7.3" data-path="mlclassification.html"><a href="mlclassification.html#comparetolasso"><i class="fa fa-check"></i><b>7.3</b> Compare to a lasso classification model</a></li>
<li class="chapter" data-level="7.4" data-path="mlclassification.html"><a href="mlclassification.html#tunelasso"><i class="fa fa-check"></i><b>7.4</b> Tuning lasso hyperparameters</a></li>
<li class="chapter" data-level="7.5" data-path="mlclassification.html"><a href="mlclassification.html#casestudysparseencoding"><i class="fa fa-check"></i><b>7.5</b> Case study: sparse encoding</a></li>
<li class="chapter" data-level="7.6" data-path="mlclassification.html"><a href="mlclassification.html#mlmulticlass"><i class="fa fa-check"></i><b>7.6</b> Two-class or multiclass?</a></li>
<li class="chapter" data-level="7.7" data-path="mlclassification.html"><a href="mlclassification.html#case-study-including-non-text-data"><i class="fa fa-check"></i><b>7.7</b> Case study: including non-text data</a></li>
<li class="chapter" data-level="7.8" data-path="mlclassification.html"><a href="mlclassification.html#case-study-data-censoring"><i class="fa fa-check"></i><b>7.8</b> Case study: data censoring</a></li>
<li class="chapter" data-level="7.9" data-path="mlclassification.html"><a href="mlclassification.html#customfeatures"><i class="fa fa-check"></i><b>7.9</b> Case study: custom features</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="mlclassification.html"><a href="mlclassification.html#detect-credit-cards"><i class="fa fa-check"></i><b>7.9.1</b> Detect credit cards</a></li>
<li class="chapter" data-level="7.9.2" data-path="mlclassification.html"><a href="mlclassification.html#calculate-percentage-censoring"><i class="fa fa-check"></i><b>7.9.2</b> Calculate percentage censoring</a></li>
<li class="chapter" data-level="7.9.3" data-path="mlclassification.html"><a href="mlclassification.html#detect-monetary-amounts"><i class="fa fa-check"></i><b>7.9.3</b> Detect monetary amounts</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="mlclassification.html"><a href="mlclassification.html#what-evaluation-metrics-are-appropriate-1"><i class="fa fa-check"></i><b>7.10</b> What evaluation metrics are appropriate?</a></li>
<li class="chapter" data-level="7.11" data-path="mlclassification.html"><a href="mlclassification.html#mlclassificationfull"><i class="fa fa-check"></i><b>7.11</b> The full game: classification</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="mlclassification.html"><a href="mlclassification.html#feature-selection"><i class="fa fa-check"></i><b>7.11.1</b> Feature selection</a></li>
<li class="chapter" data-level="7.11.2" data-path="mlclassification.html"><a href="mlclassification.html#specify-the-model-1"><i class="fa fa-check"></i><b>7.11.2</b> Specify the model</a></li>
<li class="chapter" data-level="7.11.3" data-path="mlclassification.html"><a href="mlclassification.html#classification-final-evaluation"><i class="fa fa-check"></i><b>7.11.3</b> Evaluate the modeling</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="mlclassification.html"><a href="mlclassification.html#mlclassificationsummary"><i class="fa fa-check"></i><b>7.12</b> Summary</a>
<ul>
<li class="chapter" data-level="7.12.1" data-path="mlclassification.html"><a href="mlclassification.html#in-this-chapter-you-learned-6"><i class="fa fa-check"></i><b>7.12.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Deep Learning Methods</b></span></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="dloverview.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="dloverview.html#spending-your-data-budget"><i class="fa fa-check"></i>Spending your data budget</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="dloverview.html#feature-engineering"><i class="fa fa-check"></i>Feature engineering</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="dloverview.html#fitting-and-tuning"><i class="fa fa-check"></i>Fitting and tuning</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="dloverview.html#model-evaluation"><i class="fa fa-check"></i>Model evaluation</a></li>
<li class="chapter" data-level="" data-path="dloverview.html"><a href="dloverview.html#putting-the-model-process-in-context"><i class="fa fa-check"></i>Putting the model process in context</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="dldnn.html"><a href="dldnn.html"><i class="fa fa-check"></i><b>8</b> Dense neural networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dldnn.html"><a href="dldnn.html#kickstarter"><i class="fa fa-check"></i><b>8.1</b> Kickstarter data</a></li>
<li class="chapter" data-level="8.2" data-path="dldnn.html"><a href="dldnn.html#firstdlclassification"><i class="fa fa-check"></i><b>8.2</b> A first deep learning model</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="dldnn.html"><a href="dldnn.html#dnnrecipe"><i class="fa fa-check"></i><b>8.2.1</b> Preprocessing for deep learning</a></li>
<li class="chapter" data-level="8.2.2" data-path="dldnn.html"><a href="dldnn.html#onehotsequence"><i class="fa fa-check"></i><b>8.2.2</b> One-hot sequence embedding of text</a></li>
<li class="chapter" data-level="8.2.3" data-path="dldnn.html"><a href="dldnn.html#simple-flattened-dense-network"><i class="fa fa-check"></i><b>8.2.3</b> Simple flattened dense network</a></li>
<li class="chapter" data-level="8.2.4" data-path="dldnn.html"><a href="dldnn.html#evaluate-dnn"><i class="fa fa-check"></i><b>8.2.4</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="dldnn.html"><a href="dldnn.html#using-bag-of-words-features"><i class="fa fa-check"></i><b>8.3</b> Using bag-of-words features</a></li>
<li class="chapter" data-level="8.4" data-path="dldnn.html"><a href="dldnn.html#using-pre-trained-word-embeddings"><i class="fa fa-check"></i><b>8.4</b> Using pre-trained word embeddings</a></li>
<li class="chapter" data-level="8.5" data-path="dldnn.html"><a href="dldnn.html#dnncross"><i class="fa fa-check"></i><b>8.5</b> Cross-validation for deep learning models</a></li>
<li class="chapter" data-level="8.6" data-path="dldnn.html"><a href="dldnn.html#compare-and-evaluate-dnn-models"><i class="fa fa-check"></i><b>8.6</b> Compare and evaluate DNN models</a></li>
<li class="chapter" data-level="8.7" data-path="dldnn.html"><a href="dldnn.html#dllimitations"><i class="fa fa-check"></i><b>8.7</b> Limitations of deep learning</a></li>
<li class="chapter" data-level="8.8" data-path="dldnn.html"><a href="dldnn.html#dldnnsummary"><i class="fa fa-check"></i><b>8.8</b> Summary</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="dldnn.html"><a href="dldnn.html#in-this-chapter-you-learned-7"><i class="fa fa-check"></i><b>8.8.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dllstm.html"><a href="dllstm.html"><i class="fa fa-check"></i><b>9</b> Long short-term memory (LSTM) networks</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dllstm.html"><a href="dllstm.html#firstlstm"><i class="fa fa-check"></i><b>9.1</b> A first LSTM model</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="dllstm.html"><a href="dllstm.html#building-an-lstm"><i class="fa fa-check"></i><b>9.1.1</b> Building an LSTM</a></li>
<li class="chapter" data-level="9.1.2" data-path="dllstm.html"><a href="dllstm.html#lstmevaluation"><i class="fa fa-check"></i><b>9.1.2</b> Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dllstm.html"><a href="dllstm.html#compare-to-a-recurrent-neural-network"><i class="fa fa-check"></i><b>9.2</b> Compare to a recurrent neural network</a></li>
<li class="chapter" data-level="9.3" data-path="dllstm.html"><a href="dllstm.html#bilstm"><i class="fa fa-check"></i><b>9.3</b> Case study: bidirectional LSTM</a></li>
<li class="chapter" data-level="9.4" data-path="dllstm.html"><a href="dllstm.html#case-study-stacking-lstm-layers"><i class="fa fa-check"></i><b>9.4</b> Case study: stacking LSTM layers</a></li>
<li class="chapter" data-level="9.5" data-path="dllstm.html"><a href="dllstm.html#lstmpadding"><i class="fa fa-check"></i><b>9.5</b> Case study: padding</a></li>
<li class="chapter" data-level="9.6" data-path="dllstm.html"><a href="dllstm.html#case-study-training-a-regression-model"><i class="fa fa-check"></i><b>9.6</b> Case study: training a regression model</a></li>
<li class="chapter" data-level="9.7" data-path="dllstm.html"><a href="dllstm.html#case-study-vocabulary-size"><i class="fa fa-check"></i><b>9.7</b> Case study: vocabulary size</a></li>
<li class="chapter" data-level="9.8" data-path="dllstm.html"><a href="dllstm.html#lstmfull"><i class="fa fa-check"></i><b>9.8</b> The full game: LSTM</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="dllstm.html"><a href="dllstm.html#lstmfullpreprocess"><i class="fa fa-check"></i><b>9.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="9.8.2" data-path="dllstm.html"><a href="dllstm.html#lstmfullmodel"><i class="fa fa-check"></i><b>9.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="dllstm.html"><a href="dllstm.html#dllstmsummary"><i class="fa fa-check"></i><b>9.9</b> Summary</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="dllstm.html"><a href="dllstm.html#in-this-chapter-you-learned-8"><i class="fa fa-check"></i><b>9.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dlcnn.html"><a href="dlcnn.html"><i class="fa fa-check"></i><b>10</b> Convolutional neural networks</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dlcnn.html"><a href="dlcnn.html#what-are-cnns"><i class="fa fa-check"></i><b>10.1</b> What are CNNs?</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="dlcnn.html"><a href="dlcnn.html#kernel"><i class="fa fa-check"></i><b>10.1.1</b> Kernel</a></li>
<li class="chapter" data-level="10.1.2" data-path="dlcnn.html"><a href="dlcnn.html#kernel-size"><i class="fa fa-check"></i><b>10.1.2</b> Kernel size</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="dlcnn.html"><a href="dlcnn.html#firstcnn"><i class="fa fa-check"></i><b>10.2</b> A first CNN model</a></li>
<li class="chapter" data-level="10.3" data-path="dlcnn.html"><a href="dlcnn.html#case-study-adding-more-layers"><i class="fa fa-check"></i><b>10.3</b> Case study: adding more layers</a></li>
<li class="chapter" data-level="10.4" data-path="dlcnn.html"><a href="dlcnn.html#case-study-byte-pair-encoding"><i class="fa fa-check"></i><b>10.4</b> Case study: byte pair encoding</a></li>
<li class="chapter" data-level="10.5" data-path="dlcnn.html"><a href="dlcnn.html#lime"><i class="fa fa-check"></i><b>10.5</b> Case study: explainability with LIME</a></li>
<li class="chapter" data-level="10.6" data-path="dlcnn.html"><a href="dlcnn.html#keras-hyperparameter"><i class="fa fa-check"></i><b>10.6</b> Case study: hyperparameter search</a></li>
<li class="chapter" data-level="10.7" data-path="dlcnn.html"><a href="dlcnn.html#cross-validation-for-evaluation"><i class="fa fa-check"></i><b>10.7</b> Cross-validation for evaluation</a></li>
<li class="chapter" data-level="10.8" data-path="dlcnn.html"><a href="dlcnn.html#cnnfull"><i class="fa fa-check"></i><b>10.8</b> The full game: CNN</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="dlcnn.html"><a href="dlcnn.html#cnnfullpreprocess"><i class="fa fa-check"></i><b>10.8.1</b> Preprocess the data</a></li>
<li class="chapter" data-level="10.8.2" data-path="dlcnn.html"><a href="dlcnn.html#cnnfullmodel"><i class="fa fa-check"></i><b>10.8.2</b> Specify the model</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="dlcnn.html"><a href="dlcnn.html#dlcnnsummary"><i class="fa fa-check"></i><b>10.9</b> Summary</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="dlcnn.html"><a href="dlcnn.html#in-this-chapter-you-learned-9"><i class="fa fa-check"></i><b>10.9.1</b> In this chapter, you learned:</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Conclusion</b></span></li>
<li class="chapter" data-level="" data-path="text-models-in-the-real-world.html"><a href="text-models-in-the-real-world.html"><i class="fa fa-check"></i>Text models in the real world</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="regexp.html"><a href="regexp.html"><i class="fa fa-check"></i><b>A</b> Regular expressions</a>
<ul>
<li class="chapter" data-level="A.1" data-path="regexp.html"><a href="regexp.html#literal-characters"><i class="fa fa-check"></i><b>A.1</b> Literal characters</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="regexp.html"><a href="regexp.html#meta-characters"><i class="fa fa-check"></i><b>A.1.1</b> Meta characters</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="regexp.html"><a href="regexp.html#full-stop-the-wildcard"><i class="fa fa-check"></i><b>A.2</b> Full stop, the wildcard</a></li>
<li class="chapter" data-level="A.3" data-path="regexp.html"><a href="regexp.html#character-classes"><i class="fa fa-check"></i><b>A.3</b> Character classes</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="regexp.html"><a href="regexp.html#shorthand-character-classes"><i class="fa fa-check"></i><b>A.3.1</b> Shorthand character classes</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="regexp.html"><a href="regexp.html#quantifiers"><i class="fa fa-check"></i><b>A.4</b> Quantifiers</a></li>
<li class="chapter" data-level="A.5" data-path="regexp.html"><a href="regexp.html#anchors"><i class="fa fa-check"></i><b>A.5</b> Anchors</a></li>
<li class="chapter" data-level="A.6" data-path="regexp.html"><a href="regexp.html#additional-resources"><i class="fa fa-check"></i><b>A.6</b> Additional resources</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixdata.html"><a href="appendixdata.html"><i class="fa fa-check"></i><b>B</b> Data</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixdata.html"><a href="appendixdata.html#hcandersen"><i class="fa fa-check"></i><b>B.1</b> Hans Christian Andersen fairy tales</a></li>
<li class="chapter" data-level="B.2" data-path="appendixdata.html"><a href="appendixdata.html#scotus-opinions"><i class="fa fa-check"></i><b>B.2</b> Opinions of the Supreme Court of the United States</a></li>
<li class="chapter" data-level="B.3" data-path="appendixdata.html"><a href="appendixdata.html#cfpb-complaints"><i class="fa fa-check"></i><b>B.3</b> Consumer Financial Protection Bureau (CFPB) complaints</a></li>
<li class="chapter" data-level="B.4" data-path="appendixdata.html"><a href="appendixdata.html#kickstarter-blurbs"><i class="fa fa-check"></i><b>B.4</b> Kickstarter campaign blurbs</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appendixbaseline.html"><a href="appendixbaseline.html"><i class="fa fa-check"></i><b>C</b> Baseline linear classifier</a>
<ul>
<li class="chapter" data-level="C.1" data-path="appendixbaseline.html"><a href="appendixbaseline.html#read-in-the-data"><i class="fa fa-check"></i><b>C.1</b> Read in the data</a></li>
<li class="chapter" data-level="C.2" data-path="appendixbaseline.html"><a href="appendixbaseline.html#split-into-testtrain-and-create-resampling-folds"><i class="fa fa-check"></i><b>C.2</b> Split into test/train and create resampling folds</a></li>
<li class="chapter" data-level="C.3" data-path="appendixbaseline.html"><a href="appendixbaseline.html#recipe-for-data-preprocessing"><i class="fa fa-check"></i><b>C.3</b> Recipe for data preprocessing</a></li>
<li class="chapter" data-level="C.4" data-path="appendixbaseline.html"><a href="appendixbaseline.html#lasso-regularized-classification-model"><i class="fa fa-check"></i><b>C.4</b> Lasso regularized classification model</a></li>
<li class="chapter" data-level="C.5" data-path="appendixbaseline.html"><a href="appendixbaseline.html#a-model-workflow"><i class="fa fa-check"></i><b>C.5</b> A model workflow</a></li>
<li class="chapter" data-level="C.6" data-path="appendixbaseline.html"><a href="appendixbaseline.html#tune-the-workflow"><i class="fa fa-check"></i><b>C.6</b> Tune the workflow</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Supervised Machine Learning for Text Analysis in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="embeddings" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Word Embeddings</h1>
<blockquote>
You shall know a word by the company it keeps.<br />

<footer>
— <a href="https://en.wikiquote.org/wiki/John_Rupert_Firth">John Rupert Firth</a>
</footer>
</blockquote>
<p>So far in our discussion of natural language features, we have discussed preprocessing steps such as tokenization, removing stop words, and stemming in detail. We implement these types of preprocessing steps to be able to represent our text data in some data structure that is a good fit for modeling.</p>
<div id="motivatingsparse" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Motivating embeddings for sparse, high-dimensional data</h2>
<p>What kind of data structure might work well for typical text data? Perhaps, if we wanted to analyze or build a model for consumer complaints to the <a href="https://www.consumerfinance.gov/data-research/consumer-complaints/">United States Consumer Financial Protection Bureau (CFPB)</a>, described in Section <a href="appendixdata.html#cfpb-complaints">B.3</a>, we would start with straightforward word counts. Let’s create a sparse matrix, where the matrix elements are the counts of words in each document.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="embeddings.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb110-2"><a href="embeddings.html#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidytext)</span>
<span id="cb110-3"><a href="embeddings.html#cb110-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(SnowballC)</span>
<span id="cb110-4"><a href="embeddings.html#cb110-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-5"><a href="embeddings.html#cb110-5" aria-hidden="true" tabindex="-1"></a>complaints <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/complaints.csv.gz&quot;</span>)</span>
<span id="cb110-6"><a href="embeddings.html#cb110-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-7"><a href="embeddings.html#cb110-7" aria-hidden="true" tabindex="-1"></a>complaints <span class="sc">%&gt;%</span></span>
<span id="cb110-8"><a href="embeddings.html#cb110-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, consumer_complaint_narrative) <span class="sc">%&gt;%</span></span>
<span id="cb110-9"><a href="embeddings.html#cb110-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(<span class="fu">get_stopwords</span>(), <span class="at">by =</span> <span class="st">&quot;word&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb110-10"><a href="embeddings.html#cb110-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb110-11"><a href="embeddings.html#cb110-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(complaint_id, stem) <span class="sc">%&gt;%</span></span>
<span id="cb110-12"><a href="embeddings.html#cb110-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_dfm</span>(complaint_id, stem, n)</span></code></pre></div>
<pre><code>#&gt; Document-feature matrix of: 117,214 documents, 46,099
features (99.88% sparse) and 0 docvars.</code></pre>

<div class="rmdwarning">
<p>
A <em>sparse matrix</em> is a matrix where most of the elements are zero. When working with text data, we say our data is “sparse” because most documents do not contain most words, resulting in a representation of mostly zeroes. There are special data structures and algorithms for dealing with sparse data that can take advantage of their structure. For example, an array can more efficiently store the locations and values of only the non-zero elements instead of all elements.
</p>
</div>
<p>The data set of consumer complaints used in this book has been filtered to those submitted to the CFPB since January 1, 2019 that include a consumer complaint narrative (i.e., some submitted text).</p>
<p>Another way to represent our text data is to create a sparse matrix where the elements are weighted, rather than straightforward counts only. The <em>term frequency</em> of a word is how frequently a word occurs in a document, and the <em>inverse document frequency</em> of a word decreases the weight for commonly-used words and increases the weight for words that are not used often in a collection of documents. It is typically defined as:</p>
<p><span class="math display">\[idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}\]</span>

These two quantities can be combined to calculate a term’s tf-idf (the two quantities multiplied together). This statistic measures the frequency of a term adjusted for how rarely it is used, and it is an example of a weighting scheme that can often work better than counts for predictive modeling with text features.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="embeddings.html#cb112-1" aria-hidden="true" tabindex="-1"></a>complaints <span class="sc">%&gt;%</span></span>
<span id="cb112-2"><a href="embeddings.html#cb112-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, consumer_complaint_narrative) <span class="sc">%&gt;%</span></span>
<span id="cb112-3"><a href="embeddings.html#cb112-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">anti_join</span>(<span class="fu">get_stopwords</span>(), <span class="at">by =</span> <span class="st">&quot;word&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb112-4"><a href="embeddings.html#cb112-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb112-5"><a href="embeddings.html#cb112-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(complaint_id, stem) <span class="sc">%&gt;%</span></span>
<span id="cb112-6"><a href="embeddings.html#cb112-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_tf_idf</span>(stem, complaint_id, n) <span class="sc">%&gt;%</span></span>
<span id="cb112-7"><a href="embeddings.html#cb112-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_dfm</span>(complaint_id, stem, tf_idf)</span></code></pre></div>
<pre><code>#&gt; Document-feature matrix of: 117,214 documents, 46,099
features (99.88% sparse) and 0 docvars.</code></pre>
<p>Notice that, in either case, our final data structure is incredibly sparse and of high dimensionality with a huge number of features. Some modeling algorithms and the libraries that implement them can take advantage of the memory characteristics of sparse matrices for better performance; an example of this is regularized regression implemented in <strong>glmnet</strong> <span class="citation">(<a href="#ref-Friedman2010" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span>. Some modeling algorithms, including tree-based algorithms, do not perform better with sparse input, and then some libraries are not built to take advantage of sparse data structures, even if it would improve performance for those algorithms. We have some computational tools to take advantage of sparsity, but they don’t always solve all the problems that come along with big text data sets.</p>
<p>

As the size of a corpus increases in terms of words or other tokens, both the sparsity and RAM required to hold the corpus in memory increase. Figure <a href="embeddings.html#fig:sparsityram">5.1</a> shows how this works out; as the corpus grows, there are more words used just a few times included in the corpus. The sparsity increases and approaches 100%, but even more notably, the memory required to store the corpus increases with the square of the number of tokens.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="embeddings.html#cb114-1" aria-hidden="true" tabindex="-1"></a>get_dfm <span class="ot">&lt;-</span> <span class="cf">function</span>(frac) {</span>
<span id="cb114-2"><a href="embeddings.html#cb114-2" aria-hidden="true" tabindex="-1"></a>  complaints <span class="sc">%&gt;%</span></span>
<span id="cb114-3"><a href="embeddings.html#cb114-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sample_frac</span>(frac) <span class="sc">%&gt;%</span></span>
<span id="cb114-4"><a href="embeddings.html#cb114-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unnest_tokens</span>(word, consumer_complaint_narrative) <span class="sc">%&gt;%</span></span>
<span id="cb114-5"><a href="embeddings.html#cb114-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">anti_join</span>(<span class="fu">get_stopwords</span>(), <span class="at">by =</span> <span class="st">&quot;word&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb114-6"><a href="embeddings.html#cb114-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">stem =</span> <span class="fu">wordStem</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb114-7"><a href="embeddings.html#cb114-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">count</span>(complaint_id, stem) <span class="sc">%&gt;%</span></span>
<span id="cb114-8"><a href="embeddings.html#cb114-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cast_dfm</span>(complaint_id, stem, n)</span>
<span id="cb114-9"><a href="embeddings.html#cb114-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb114-10"><a href="embeddings.html#cb114-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-11"><a href="embeddings.html#cb114-11" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb114-12"><a href="embeddings.html#cb114-12" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">frac =</span> <span class="dv">2</span> <span class="sc">^</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">16</span>, <span class="sc">-</span><span class="dv">6</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb114-13"><a href="embeddings.html#cb114-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">dfm =</span> <span class="fu">map</span>(frac, get_dfm),</span>
<span id="cb114-14"><a href="embeddings.html#cb114-14" aria-hidden="true" tabindex="-1"></a>         <span class="at">words =</span> <span class="fu">map_dbl</span>(dfm, quanteda<span class="sc">::</span>nfeat),</span>
<span id="cb114-15"><a href="embeddings.html#cb114-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">sparsity =</span> <span class="fu">map_dbl</span>(dfm, quanteda<span class="sc">::</span>sparsity),</span>
<span id="cb114-16"><a href="embeddings.html#cb114-16" aria-hidden="true" tabindex="-1"></a>         <span class="st">`</span><span class="at">RAM (in bytes)</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">map_dbl</span>(dfm, lobstr<span class="sc">::</span>obj_size)) <span class="sc">%&gt;%</span></span>
<span id="cb114-17"><a href="embeddings.html#cb114-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(sparsity<span class="sc">:</span><span class="st">`</span><span class="at">RAM (in bytes)</span><span class="st">`</span>, <span class="at">names_to =</span> <span class="st">&quot;measure&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb114-18"><a href="embeddings.html#cb114-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(words, value, <span class="at">color =</span> measure)) <span class="sc">+</span></span>
<span id="cb114-19"><a href="embeddings.html#cb114-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.5</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb114-20"><a href="embeddings.html#cb114-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb114-21"><a href="embeddings.html#cb114-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>measure, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb114-22"><a href="embeddings.html#cb114-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>(<span class="at">labels =</span> scales<span class="sc">::</span><span class="fu">label_comma</span>()) <span class="sc">+</span></span>
<span id="cb114-23"><a href="embeddings.html#cb114-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> scales<span class="sc">::</span><span class="fu">label_comma</span>()) <span class="sc">+</span></span>
<span id="cb114-24"><a href="embeddings.html#cb114-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb114-25"><a href="embeddings.html#cb114-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Number of unique words in corpus (log scale)&quot;</span>,</span>
<span id="cb114-26"><a href="embeddings.html#cb114-26" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sparsityram"></span>
<img src="05_word_embeddings_files/figure-html/sparsityram-1.svg" alt="Memory requirements and sparsity increase with corpus size" width="672" />
<p class="caption">
FIGURE 5.1: Memory requirements and sparsity increase with corpus size
</p>
</div>
<p>Linguists have long worked on vector models for language that can reduce the number of dimensions representing text data based on how people use language; the quote that opened this chapter dates to 1957. These kinds of dense word vectors are often called <em>word embeddings</em>.</p>
</div>
<div id="understand-word-embeddings-by-finding-them-yourself" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Understand word embeddings by finding them yourself</h2>
<p>Word embeddings are a way to represent text data as vectors of numbers based on a huge corpus of text, capturing semantic meaning from words’ context.</p>
<div class="rmdnote">
<p>
Modern word embeddings are based on a statistical approach to modeling language, rather than a linguistics or rules-based approach.
</p>
</div>
<p>We can determine these vectors for a corpus of text using word counts and matrix factorization, as outlined by <span class="citation"><a href="#ref-Moody2017" role="doc-biblioref">Moody</a> (<a href="#ref-Moody2017" role="doc-biblioref">2017</a>)</span>. This approach is valuable because it allows practitioners to find word vectors for their own collections of text (with no need to rely on pre-trained vectors) using familiar techniques that are not difficult to understand. Let’s walk through how to do this using tidy data principles and sparse matrices, on the data set of CFPB complaints. First, let’s filter out words that are used only rarely in this data set and create a nested dataframe, with one row per complaint.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="embeddings.html#cb115-1" aria-hidden="true" tabindex="-1"></a>tidy_complaints <span class="ot">&lt;-</span> complaints <span class="sc">%&gt;%</span></span>
<span id="cb115-2"><a href="embeddings.html#cb115-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(complaint_id, consumer_complaint_narrative) <span class="sc">%&gt;%</span></span>
<span id="cb115-3"><a href="embeddings.html#cb115-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, consumer_complaint_narrative) <span class="sc">%&gt;%</span></span>
<span id="cb115-4"><a href="embeddings.html#cb115-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_count</span>(word) <span class="sc">%&gt;%</span></span>
<span id="cb115-5"><a href="embeddings.html#cb115-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(n <span class="sc">&gt;=</span> <span class="dv">50</span>) <span class="sc">%&gt;%</span></span>
<span id="cb115-6"><a href="embeddings.html#cb115-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>n)</span>
<span id="cb115-7"><a href="embeddings.html#cb115-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-8"><a href="embeddings.html#cb115-8" aria-hidden="true" tabindex="-1"></a>nested_words <span class="ot">&lt;-</span> tidy_complaints <span class="sc">%&gt;%</span></span>
<span id="cb115-9"><a href="embeddings.html#cb115-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">words =</span> <span class="fu">c</span>(word))</span>
<span id="cb115-10"><a href="embeddings.html#cb115-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-11"><a href="embeddings.html#cb115-11" aria-hidden="true" tabindex="-1"></a>nested_words</span></code></pre></div>
<pre><code>#&gt; # A tibble: 117,170 × 2
#&gt;    complaint_id words             
#&gt;           &lt;dbl&gt; &lt;list&gt;            
#&gt;  1      3384392 &lt;tibble [18 × 1]&gt; 
#&gt;  2      3417821 &lt;tibble [71 × 1]&gt; 
#&gt;  3      3433198 &lt;tibble [77 × 1]&gt; 
#&gt;  4      3366475 &lt;tibble [69 × 1]&gt; 
#&gt;  5      3385399 &lt;tibble [213 × 1]&gt;
#&gt;  6      3444592 &lt;tibble [19 × 1]&gt; 
#&gt;  7      3379924 &lt;tibble [121 × 1]&gt;
#&gt;  8      3446975 &lt;tibble [22 × 1]&gt; 
#&gt;  9      3214857 &lt;tibble [64 × 1]&gt; 
#&gt; 10      3417374 &lt;tibble [44 × 1]&gt; 
#&gt; # … with 117,160 more rows</code></pre>
<p>Next, let’s create a <code>slide_windows()</code> function, using the <code>slide()</code> function from the <strong>slider</strong> package <span class="citation">(<a href="#ref-Vaughan2020" role="doc-biblioref">Vaughan 2021a</a>)</span> that implements fast sliding window computations written in C. Our new function identifies skipgram windows in order to calculate the skipgram probabilities, how often we find each word near each other word. We do this by defining a fixed-size moving window that centers around each word. Do we see <code>word1</code> and <code>word2</code> together within this window? We can calculate probabilities based on when we do or do not.</p>
<p>One of the arguments to this function is the <code>window_size</code>, which determines the size of the sliding window that moves through the text, counting up words that we find within the window. The best choice for this window size depends on your analytical question because it determines what kind of semantic meaning the embeddings capture. A smaller window size, like three or four, focuses on how the word is used and learns what other words are functionally similar. A larger window size, like 10, captures more information about the domain or topic of each word, not constrained by how functionally similar the words are <span class="citation">(<a href="#ref-Levy2014" role="doc-biblioref">Levy and Goldberg 2014</a>)</span>. A smaller window size is also faster to compute.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="embeddings.html#cb117-1" aria-hidden="true" tabindex="-1"></a>slide_windows <span class="ot">&lt;-</span> <span class="cf">function</span>(tbl, window_size) {</span>
<span id="cb117-2"><a href="embeddings.html#cb117-2" aria-hidden="true" tabindex="-1"></a>  skipgrams <span class="ot">&lt;-</span> slider<span class="sc">::</span><span class="fu">slide</span>(</span>
<span id="cb117-3"><a href="embeddings.html#cb117-3" aria-hidden="true" tabindex="-1"></a>    tbl, </span>
<span id="cb117-4"><a href="embeddings.html#cb117-4" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span>.x, </span>
<span id="cb117-5"><a href="embeddings.html#cb117-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">.after =</span> window_size <span class="sc">-</span> <span class="dv">1</span>, </span>
<span id="cb117-6"><a href="embeddings.html#cb117-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">.step =</span> <span class="dv">1</span>, </span>
<span id="cb117-7"><a href="embeddings.html#cb117-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">.complete =</span> <span class="cn">TRUE</span></span>
<span id="cb117-8"><a href="embeddings.html#cb117-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb117-9"><a href="embeddings.html#cb117-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb117-10"><a href="embeddings.html#cb117-10" aria-hidden="true" tabindex="-1"></a>  safe_mutate <span class="ot">&lt;-</span> <span class="fu">safely</span>(mutate)</span>
<span id="cb117-11"><a href="embeddings.html#cb117-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb117-12"><a href="embeddings.html#cb117-12" aria-hidden="true" tabindex="-1"></a>  out <span class="ot">&lt;-</span> <span class="fu">map2</span>(skipgrams,</span>
<span id="cb117-13"><a href="embeddings.html#cb117-13" aria-hidden="true" tabindex="-1"></a>              <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(skipgrams),</span>
<span id="cb117-14"><a href="embeddings.html#cb117-14" aria-hidden="true" tabindex="-1"></a>              <span class="sc">~</span> <span class="fu">safe_mutate</span>(.x, <span class="at">window_id =</span> .y))</span>
<span id="cb117-15"><a href="embeddings.html#cb117-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb117-16"><a href="embeddings.html#cb117-16" aria-hidden="true" tabindex="-1"></a>  out <span class="sc">%&gt;%</span></span>
<span id="cb117-17"><a href="embeddings.html#cb117-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">transpose</span>() <span class="sc">%&gt;%</span></span>
<span id="cb117-18"><a href="embeddings.html#cb117-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pluck</span>(<span class="st">&quot;result&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb117-19"><a href="embeddings.html#cb117-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">compact</span>() <span class="sc">%&gt;%</span></span>
<span id="cb117-20"><a href="embeddings.html#cb117-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_rows</span>()</span>
<span id="cb117-21"><a href="embeddings.html#cb117-21" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now that we can find all the skipgram windows, we can calculate how often words occur on their own, and how often words occur together with other words. We do this using the point-wise mutual information (PMI), a measure of association that measures exactly what we described in the previous sentence; it’s the logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone. We use PMI to measure which words occur together more often than expected based on how often they occurred on their own.</p>
<p>For this example, let’s use a window size of <em>four</em>.</p>

<div class="rmdpackage">
This next step is the computationally expensive part of finding word embeddings with this method, and can take a while to run. Fortunately, we can use the <strong>furrr</strong> package <span class="citation">(<a href="#ref-Vaughan2018" role="doc-biblioref">Vaughan and Dancho 2021</a>)</span> to take advantage of parallel processing because identifying skipgram windows in one document is independent from all the other documents.
</div>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="embeddings.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(widyr)</span>
<span id="cb118-2"><a href="embeddings.html#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(furrr)</span>
<span id="cb118-3"><a href="embeddings.html#cb118-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-4"><a href="embeddings.html#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plan</span>(multisession)  <span class="do">## for parallel processing</span></span>
<span id="cb118-5"><a href="embeddings.html#cb118-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-6"><a href="embeddings.html#cb118-6" aria-hidden="true" tabindex="-1"></a>tidy_pmi <span class="ot">&lt;-</span> nested_words <span class="sc">%&gt;%</span></span>
<span id="cb118-7"><a href="embeddings.html#cb118-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">words =</span> <span class="fu">future_map</span>(words, slide_windows, 4L)) <span class="sc">%&gt;%</span></span>
<span id="cb118-8"><a href="embeddings.html#cb118-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(words) <span class="sc">%&gt;%</span></span>
<span id="cb118-9"><a href="embeddings.html#cb118-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unite</span>(window_id, complaint_id, window_id) <span class="sc">%&gt;%</span></span>
<span id="cb118-10"><a href="embeddings.html#cb118-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pairwise_pmi</span>(word, window_id)</span>
<span id="cb118-11"><a href="embeddings.html#cb118-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-12"><a href="embeddings.html#cb118-12" aria-hidden="true" tabindex="-1"></a>tidy_pmi</span></code></pre></div>
<pre><code>#&gt; # A tibble: 4,818,402 × 3
#&gt;    item1   item2           pmi
#&gt;    &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;
#&gt;  1 systems transworld  7.09   
#&gt;  2 inc     transworld  5.96   
#&gt;  3 is      transworld -0.135  
#&gt;  4 trying  transworld -0.107  
#&gt;  5 to      transworld -0.00206
#&gt;  6 collect transworld  1.07   
#&gt;  7 a       transworld -0.516  
#&gt;  8 debt    transworld  0.919  
#&gt;  9 that    transworld -0.542  
#&gt; 10 not     transworld -1.17   
#&gt; # … with 4,818,392 more rows</code></pre>
<p>When PMI is high, the two words are associated with each other, i.e., likely to occur together. When PMI is low, the two words are not associated with each other, unlikely to occur together.</p>
<div class="rmdwarning">
<p>
The step above used <code>unite()</code>, a function from <strong>tidyr</strong> that pastes multiple columns into one, to make a new column for <code>window_id</code> from the old <code>window_id</code> plus the <code>complaint_id</code>. This new column tells us which combination of window and complaint each word belongs to.
</p>
</div>
<p>We can next determine the word vectors from the PMI values using singular value decomposition (SVD).
SVD is a method for dimensionality reduction via matrix factorization <span class="citation">(<a href="#ref-Golub1970" role="doc-biblioref">Golub and Reinsch 1970</a>)</span> that works by taking our data and decomposing it onto special orthogonal axes. The first axis is chosen to capture as much of the variance as possible. Keeping that first axis fixed, the remaining orthogonal axes are rotated to maximize the variance in the second. This is repeated for all the remaining axes.</p>
<p>In our application, we will use SVD to factor the PMI matrix into a set of smaller matrices containing the word embeddings with a size we get to choose. The embedding size is typically chosen to be in the low hundreds. Thus we get a matrix of dimension (<code>n_vocabulary * n_dim</code>) instead of dimension (<code>n_vocabulary * n_vocabulary</code>), which can be a vast reduction in size for large vocabularies.
Let’s use the <code>widely_svd()</code> function in <strong>widyr</strong> <span class="citation">(<a href="#ref-R-widyr" role="doc-biblioref">Robinson 2020</a>)</span>, creating 100-dimensional word embeddings. This matrix factorization is much faster than the previous step of identifying the skipgram windows and calculating PMI.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="embeddings.html#cb120-1" aria-hidden="true" tabindex="-1"></a>tidy_word_vectors <span class="ot">&lt;-</span> tidy_pmi <span class="sc">%&gt;%</span></span>
<span id="cb120-2"><a href="embeddings.html#cb120-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">widely_svd</span>(</span>
<span id="cb120-3"><a href="embeddings.html#cb120-3" aria-hidden="true" tabindex="-1"></a>    item1, item2, pmi,</span>
<span id="cb120-4"><a href="embeddings.html#cb120-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">nv =</span> <span class="dv">100</span>, <span class="at">maxit =</span> <span class="dv">1000</span></span>
<span id="cb120-5"><a href="embeddings.html#cb120-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb120-6"><a href="embeddings.html#cb120-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-7"><a href="embeddings.html#cb120-7" aria-hidden="true" tabindex="-1"></a>tidy_word_vectors</span></code></pre></div>
<pre><code>#&gt; # A tibble: 747,500 × 3
#&gt;    item1   dimension   value
#&gt;    &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;
#&gt;  1 systems         1 0.0165 
#&gt;  2 inc             1 0.0191 
#&gt;  3 is              1 0.0202 
#&gt;  4 trying          1 0.0423 
#&gt;  5 to              1 0.00904
#&gt;  6 collect         1 0.0370 
#&gt;  7 a               1 0.0126 
#&gt;  8 debt            1 0.0430 
#&gt;  9 that            1 0.0136 
#&gt; 10 not             1 0.0213 
#&gt; # … with 747,490 more rows</code></pre>
<div class="rmdnote">
<p>
<code>tidy_word_vectors</code> is not drastically smaller than <code>tidy_pmi</code> since the vocabulary is not enormous and <code>tidy_pmi</code> is represented in a sparse format.
</p>
</div>
<p>We have now successfully found word embeddings, with clear and understandable code. This is a real benefit of this approach; this approach is based on counting, dividing, and matrix decomposition and is thus easier to understand and implement than options based on deep learning. Training word vectors or embeddings, even with this straightforward method, still requires a large data set (ideally, hundreds of thousands of documents or more) and a not insignificant investment of time and computational power.</p>
</div>
<div id="exploring-cfpb-word-embeddings" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Exploring CFPB word embeddings</h2>
<p>Now that we have determined word embeddings for the data set of CFPB complaints, let’s explore them and talk about how they are used in modeling. We have projected the sparse, high-dimensional set of word features into a more dense, 100-dimensional set of features.</p>
<div class="rmdwarn">
<p>
Each word can be represented as a numeric vector in this new feature space. A single word is mapped to only one vector, so be aware that all senses of a word are conflated in word embeddings. Because of this, word embeddings are limited for understanding lexical semantics.
</p>
</div>
<p>Which words are close to each other in this new feature space of word embeddings? Let’s create a simple function that will find the nearest words to any given example in using our newly created word embeddings.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="embeddings.html#cb122-1" aria-hidden="true" tabindex="-1"></a>nearest_neighbors <span class="ot">&lt;-</span> <span class="cf">function</span>(df, token) {</span>
<span id="cb122-2"><a href="embeddings.html#cb122-2" aria-hidden="true" tabindex="-1"></a>  df <span class="sc">%&gt;%</span></span>
<span id="cb122-3"><a href="embeddings.html#cb122-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">widely</span>(</span>
<span id="cb122-4"><a href="embeddings.html#cb122-4" aria-hidden="true" tabindex="-1"></a>      <span class="sc">~</span> {</span>
<span id="cb122-5"><a href="embeddings.html#cb122-5" aria-hidden="true" tabindex="-1"></a>        y <span class="ot">&lt;-</span> .[<span class="fu">rep</span>(token, <span class="fu">nrow</span>(.)), ]</span>
<span id="cb122-6"><a href="embeddings.html#cb122-6" aria-hidden="true" tabindex="-1"></a>        res <span class="ot">&lt;-</span> <span class="fu">rowSums</span>(. <span class="sc">*</span> y) <span class="sc">/</span> </span>
<span id="cb122-7"><a href="embeddings.html#cb122-7" aria-hidden="true" tabindex="-1"></a>          (<span class="fu">sqrt</span>(<span class="fu">rowSums</span>(. <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(.[token, ] <span class="sc">^</span> <span class="dv">2</span>)))</span>
<span id="cb122-8"><a href="embeddings.html#cb122-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb122-9"><a href="embeddings.html#cb122-9" aria-hidden="true" tabindex="-1"></a>        <span class="fu">matrix</span>(res, <span class="at">ncol =</span> <span class="dv">1</span>, <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fu">names</span>(res)))</span>
<span id="cb122-10"><a href="embeddings.html#cb122-10" aria-hidden="true" tabindex="-1"></a>      },</span>
<span id="cb122-11"><a href="embeddings.html#cb122-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">sort =</span> <span class="cn">TRUE</span></span>
<span id="cb122-12"><a href="embeddings.html#cb122-12" aria-hidden="true" tabindex="-1"></a>    )(item1, dimension, value) <span class="sc">%&gt;%</span></span>
<span id="cb122-13"><a href="embeddings.html#cb122-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span>item2)</span>
<span id="cb122-14"><a href="embeddings.html#cb122-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>This function takes the tidy word embeddings as input, along with a word (or token, more strictly) as a string. It uses matrix multiplication and sums to calculate the cosine similarity between the word and all the words in the embedding to find which words are closer or farther to the input word, and returns a dataframe sorted by similarity.</p>
<p>What words are closest to <code>"error"</code> in the data set of CFPB complaints, as determined by our word embeddings?</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="embeddings.html#cb123-1" aria-hidden="true" tabindex="-1"></a>tidy_word_vectors <span class="sc">%&gt;%</span></span>
<span id="cb123-2"><a href="embeddings.html#cb123-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nearest_neighbors</span>(<span class="st">&quot;error&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 7,475 × 2
#&gt;    item1            value
#&gt;    &lt;chr&gt;            &lt;dbl&gt;
#&gt;  1 error            1    
#&gt;  2 mistake          0.683
#&gt;  3 clerical         0.627
#&gt;  4 problem          0.582
#&gt;  5 glitch           0.580
#&gt;  6 errors           0.571
#&gt;  7 miscommunication 0.512
#&gt;  8 misunderstanding 0.486
#&gt;  9 issue            0.478
#&gt; 10 discrepancy      0.474
#&gt; # … with 7,465 more rows</code></pre>
<p>Mistakes, problems, glitches – sounds bad!</p>
<p>What is closest to the word <code>"month"</code>?</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="embeddings.html#cb125-1" aria-hidden="true" tabindex="-1"></a>tidy_word_vectors <span class="sc">%&gt;%</span></span>
<span id="cb125-2"><a href="embeddings.html#cb125-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nearest_neighbors</span>(<span class="st">&quot;month&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 7,475 × 2
#&gt;    item1        value
#&gt;    &lt;chr&gt;        &lt;dbl&gt;
#&gt;  1 month        1    
#&gt;  2 year         0.607
#&gt;  3 months       0.593
#&gt;  4 monthly      0.454
#&gt;  5 installments 0.446
#&gt;  6 payment      0.429
#&gt;  7 week         0.406
#&gt;  8 weeks        0.400
#&gt;  9 85.00        0.399
#&gt; 10 bill         0.396
#&gt; # … with 7,465 more rows</code></pre>
<p>We see words about installments and payments, along with other time periods such as years and weeks. Notice that we did not stem this text data (see Chapter <a href="stemming.html#stemming">4</a>), but the word embeddings learned that “month,” “months,” and “monthly” belong together.</p>
<p>What words are closest in this embedding space to <code>"fee"</code>?</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="embeddings.html#cb127-1" aria-hidden="true" tabindex="-1"></a>tidy_word_vectors <span class="sc">%&gt;%</span></span>
<span id="cb127-2"><a href="embeddings.html#cb127-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nearest_neighbors</span>(<span class="st">&quot;fee&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 7,475 × 2
#&gt;    item1     value
#&gt;    &lt;chr&gt;     &lt;dbl&gt;
#&gt;  1 fee       1    
#&gt;  2 fees      0.746
#&gt;  3 overdraft 0.678
#&gt;  4 12.00     0.675
#&gt;  5 14.00     0.645
#&gt;  6 37.00     0.632
#&gt;  7 charge    0.630
#&gt;  8 11.00     0.630
#&gt;  9 36.00     0.627
#&gt; 10 28.00     0.624
#&gt; # … with 7,465 more rows</code></pre>
<p>We find a lot of dollar amounts, which makes sense. Let us filter out the numbers to see what non-dollar words are similar to “fee.”</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="embeddings.html#cb129-1" aria-hidden="true" tabindex="-1"></a>tidy_word_vectors <span class="sc">%&gt;%</span></span>
<span id="cb129-2"><a href="embeddings.html#cb129-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nearest_neighbors</span>(<span class="st">&quot;fee&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb129-3"><a href="embeddings.html#cb129-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(item1, <span class="st">&quot;[0-9]*.[0-9]{2}&quot;</span>, <span class="at">negate =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<pre><code>#&gt; # A tibble: 7,047 × 2
#&gt;    item1     value
#&gt;    &lt;chr&gt;     &lt;dbl&gt;
#&gt;  1 fee       1    
#&gt;  2 fees      0.746
#&gt;  3 overdraft 0.678
#&gt;  4 charge    0.630
#&gt;  5 nsf       0.609
#&gt;  6 charged   0.594
#&gt;  7 od        0.552
#&gt;  8 waived    0.547
#&gt;  9 assessed  0.538
#&gt; 10 charges   0.530
#&gt; # … with 7,037 more rows</code></pre>
<p>We now find words about overdrafts and charges. The top two words are “fee” and “fees”; word embeddings can learn that singular and plural forms of words are related and belong together. In fact, word embeddings can accomplish many of the same goals of tasks like stemming (Chapter <a href="stemming.html#stemming">4</a>) but more reliably and less arbitrarily.</p>
<p>Since we have found word embeddings via singular value decomposition, we can use these vectors to understand what principal components explain the most variation in the CFPB complaints. The orthogonal axes that SVD used to represent our data were chosen so that the first axis accounts for the most variance, the second axis accounts for the next most variance, and so on. We can now explore which and how much each <em>original</em> dimension (tokens in this case) contributed to each of the resulting principal components produced using SVD.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="embeddings.html#cb131-1" aria-hidden="true" tabindex="-1"></a>tidy_word_vectors <span class="sc">%&gt;%</span></span>
<span id="cb131-2"><a href="embeddings.html#cb131-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(dimension <span class="sc">&lt;=</span> <span class="dv">24</span>) <span class="sc">%&gt;%</span></span>
<span id="cb131-3"><a href="embeddings.html#cb131-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(dimension) <span class="sc">%&gt;%</span></span>
<span id="cb131-4"><a href="embeddings.html#cb131-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">top_n</span>(<span class="dv">12</span>, <span class="fu">abs</span>(value)) <span class="sc">%&gt;%</span></span>
<span id="cb131-5"><a href="embeddings.html#cb131-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb131-6"><a href="embeddings.html#cb131-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">item1 =</span> <span class="fu">reorder_within</span>(item1, value, dimension)) <span class="sc">%&gt;%</span></span>
<span id="cb131-7"><a href="embeddings.html#cb131-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(item1, value, <span class="at">fill =</span> dimension)) <span class="sc">+</span></span>
<span id="cb131-8"><a href="embeddings.html#cb131-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb131-9"><a href="embeddings.html#cb131-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>dimension, <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>, <span class="at">ncol =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb131-10"><a href="embeddings.html#cb131-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_reordered</span>() <span class="sc">+</span></span>
<span id="cb131-11"><a href="embeddings.html#cb131-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb131-12"><a href="embeddings.html#cb131-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb131-13"><a href="embeddings.html#cb131-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="cn">NULL</span>,</span>
<span id="cb131-14"><a href="embeddings.html#cb131-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Value&quot;</span>,</span>
<span id="cb131-15"><a href="embeddings.html#cb131-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;First 24 principal components for text of CFPB complaints&quot;</span>,</span>
<span id="cb131-16"><a href="embeddings.html#cb131-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="fu">paste</span>(<span class="st">&quot;Top words contributing to the components that explain&quot;</span>,</span>
<span id="cb131-17"><a href="embeddings.html#cb131-17" aria-hidden="true" tabindex="-1"></a>                     <span class="st">&quot;the most variation&quot;</span>)</span>
<span id="cb131-18"><a href="embeddings.html#cb131-18" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:embeddingpca"></span>
<img src="05_word_embeddings_files/figure-html/embeddingpca-1.svg" alt="Word embeddings for Consumer Finance Protection Bureau complaints" width="768" />
<p class="caption">
FIGURE 5.2: Word embeddings for Consumer Finance Protection Bureau complaints
</p>
</div>
<p>It becomes very clear in Figure <a href="embeddings.html#fig:embeddingpca">5.2</a> that stop words have not been removed, but notice that we can learn meaningful relationships in how very common words are used. Component 12 shows us how common prepositions are often used with words like <code>"regarding"</code>, <code>"contacted"</code>, and <code>"called"</code>, while component 9 highlights the use of <em>different</em> common words when submitting a complaint about unethical, predatory, and/or deceptive practices. Stop words do carry information, and methods like determining word embeddings can make that information usable.</p>
<p>We created word embeddings and can explore them to understand our text data set, but how do we use this vector representation in modeling? The classic and simplest approach is to treat each document as a collection of words and summarize the word embeddings into <em>document embeddings</em>, either using a mean or sum. This approach loses information about word order but is straightforward to implement. Let’s <code>count()</code> to find the sum here in our example.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="embeddings.html#cb132-1" aria-hidden="true" tabindex="-1"></a>word_matrix <span class="ot">&lt;-</span> tidy_complaints <span class="sc">%&gt;%</span></span>
<span id="cb132-2"><a href="embeddings.html#cb132-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(complaint_id, word) <span class="sc">%&gt;%</span></span>
<span id="cb132-3"><a href="embeddings.html#cb132-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_sparse</span>(complaint_id, word, n)</span>
<span id="cb132-4"><a href="embeddings.html#cb132-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-5"><a href="embeddings.html#cb132-5" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="ot">&lt;-</span> tidy_word_vectors <span class="sc">%&gt;%</span></span>
<span id="cb132-6"><a href="embeddings.html#cb132-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_sparse</span>(item1, dimension, value)</span>
<span id="cb132-7"><a href="embeddings.html#cb132-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-8"><a href="embeddings.html#cb132-8" aria-hidden="true" tabindex="-1"></a>doc_matrix <span class="ot">&lt;-</span> word_matrix <span class="sc">%*%</span> embedding_matrix</span>
<span id="cb132-9"><a href="embeddings.html#cb132-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-10"><a href="embeddings.html#cb132-10" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(doc_matrix)</span></code></pre></div>
<pre><code>#&gt; [1] 117170    100</code></pre>
<p>We have a new matrix here that we can use as the input for modeling. Notice that we still have over 100,000 documents (we did lose a few complaints, compared to our example sparse matrices at the beginning of the chapter, when we filtered out rarely used words) but instead of tens of thousands of features, we have exactly 100 features.</p>
<div class="rmdnote">
<p>
These hundred features are the word embeddings we learned from the text data itself.
</p>
</div>
<p>If our word embeddings are of high quality, this translation of the high-dimensional space of words to the lower-dimensional space of the word embeddings allows our modeling based on such an input matrix to take advantage of the semantic meaning captured in the embeddings.</p>
<p>This is a straightforward method for finding and using word embeddings, based on counting and linear algebra. It is valuable both for understanding what word embeddings are and how they work, but also in many real-world applications. This is not the method to reach for if you want to publish an academic NLP paper, but is excellent for many applied purposes. Other methods for determining word embeddings include GloVe <span class="citation">(<a href="#ref-Pennington2014" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span>, implemented in R in the <strong>text2vec</strong> package <span class="citation">(<a href="#ref-Selivanov2018" role="doc-biblioref">Selivanov, Bickel, and Wang 2020</a>)</span>, word2vec <span class="citation">(<a href="#ref-Mikolov2013" role="doc-biblioref">Mikolov et al. 2013</a>)</span>, and FastText <span class="citation">(<a href="#ref-Bojanowski2016" role="doc-biblioref">Bojanowski et al. 2017</a>)</span>.


</p>
</div>
<div id="glove" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Use pre-trained word embeddings</h2>
<p>If your data set is too small, you typically cannot train reliable word embeddings.</p>
<div class="rmdwarning">
<p>
How small is too small? It is hard to make definitive statements because being able to determine useful word embeddings depends on the semantic and pragmatic details of <em>how</em> words are used in any given data set. However, it may be unreasonable to expect good results with data sets smaller than about a million words or tokens. (Here, we do not mean about a million unique tokens, i.e., the vocabulary size, but instead about that many observations in the text data.)
</p>
</div>
<p>In such situations, we can still use word embeddings for feature creation in modeling, just not embeddings that we determine ourselves from our own data set. Instead, we can turn to <em>pre-trained</em> word embeddings, such as the GloVe word vectors trained on six billion tokens from Wikipedia and news sources. Several pre-trained GloVe vector representations are available in R via the <strong>textdata</strong> package <span class="citation">(<a href="#ref-Hvitfeldt2020" role="doc-biblioref">Hvitfeldt 2020b</a>)</span>. Let’s use <code>dimensions = 100</code>, since we trained 100-dimensional word embeddings in the previous section.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="embeddings.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(textdata)</span>
<span id="cb134-2"><a href="embeddings.html#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="embeddings.html#cb134-3" aria-hidden="true" tabindex="-1"></a>glove6b <span class="ot">&lt;-</span> <span class="fu">embedding_glove6b</span>(<span class="at">dimensions =</span> <span class="dv">100</span>)</span>
<span id="cb134-4"><a href="embeddings.html#cb134-4" aria-hidden="true" tabindex="-1"></a>glove6b</span></code></pre></div>
<pre><code>#&gt; # A tibble: 400,000 × 101
#&gt;    token      d1      d2      d3      d4      d5      d6      d7      d8      d9
#&gt;    &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 &quot;the&quot; -0.0382 -0.245   0.728  -0.400   0.0832  0.0440 -0.391   0.334  -0.575 
#&gt;  2 &quot;,&quot;   -0.108   0.111   0.598  -0.544   0.674   0.107   0.0389  0.355   0.0635
#&gt;  3 &quot;.&quot;   -0.340   0.209   0.463  -0.648  -0.384   0.0380  0.171   0.160   0.466 
#&gt;  4 &quot;of&quot;  -0.153  -0.243   0.898   0.170   0.535   0.488  -0.588  -0.180  -1.36  
#&gt;  5 &quot;to&quot;  -0.190   0.0500  0.191  -0.0492 -0.0897  0.210  -0.550   0.0984 -0.201 
#&gt;  6 &quot;and&quot; -0.0720  0.231   0.0237 -0.506   0.339   0.196  -0.329   0.184  -0.181 
#&gt;  7 &quot;in&quot;   0.0857 -0.222   0.166   0.134   0.382   0.354   0.0129  0.225  -0.438 
#&gt;  8 &quot;a&quot;   -0.271   0.0440 -0.0203 -0.174   0.644   0.712   0.355   0.471  -0.296 
#&gt;  9 &quot;\&quot;&quot;  -0.305  -0.236   0.176  -0.729  -0.283  -0.256   0.266   0.0253 -0.0748
#&gt; 10 &quot;&#39;s&quot;   0.589  -0.202   0.735  -0.683  -0.197  -0.180  -0.392   0.342  -0.606 
#&gt; # … with 399,990 more rows, and 91 more variables: d10 &lt;dbl&gt;, d11 &lt;dbl&gt;,
#&gt; #   d12 &lt;dbl&gt;, d13 &lt;dbl&gt;, d14 &lt;dbl&gt;, d15 &lt;dbl&gt;, d16 &lt;dbl&gt;, d17 &lt;dbl&gt;,
#&gt; #   d18 &lt;dbl&gt;, …</code></pre>
<p>We can transform these word embeddings into a more tidy format, using <code>pivot_longer()</code> from <strong>tidyr</strong>. Let’s also give this tidied version the same column names as <code>tidy_word_vectors</code>, for convenience.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="embeddings.html#cb136-1" aria-hidden="true" tabindex="-1"></a>tidy_glove <span class="ot">&lt;-</span> glove6b <span class="sc">%&gt;%</span></span>
<span id="cb136-2"><a href="embeddings.html#cb136-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">contains</span>(<span class="st">&quot;d&quot;</span>),</span>
<span id="cb136-3"><a href="embeddings.html#cb136-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;dimension&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb136-4"><a href="embeddings.html#cb136-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">item1 =</span> token)</span>
<span id="cb136-5"><a href="embeddings.html#cb136-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-6"><a href="embeddings.html#cb136-6" aria-hidden="true" tabindex="-1"></a>tidy_glove</span></code></pre></div>
<pre><code>#&gt; # A tibble: 40,000,000 × 3
#&gt;    item1 dimension   value
#&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;
#&gt;  1 the   d1        -0.0382
#&gt;  2 the   d2        -0.245 
#&gt;  3 the   d3         0.728 
#&gt;  4 the   d4        -0.400 
#&gt;  5 the   d5         0.0832
#&gt;  6 the   d6         0.0440
#&gt;  7 the   d7        -0.391 
#&gt;  8 the   d8         0.334 
#&gt;  9 the   d9        -0.575 
#&gt; 10 the   d10        0.0875
#&gt; # … with 39,999,990 more rows</code></pre>
<p>We’ve already explored some sets of “synonyms” in the embedding space we determined ourselves from the CPFB complaints. What about this embedding space learned via the GloVe algorithm on a much larger data set? We just need to make one change to our <code>nearest_neighbors()</code> function and add <code>maximum_size = NULL</code>, because the matrices we are multiplying together are much larger this time.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="embeddings.html#cb138-1" aria-hidden="true" tabindex="-1"></a>nearest_neighbors <span class="ot">&lt;-</span> <span class="cf">function</span>(df, token) {</span>
<span id="cb138-2"><a href="embeddings.html#cb138-2" aria-hidden="true" tabindex="-1"></a>  df <span class="sc">%&gt;%</span></span>
<span id="cb138-3"><a href="embeddings.html#cb138-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">widely</span>(</span>
<span id="cb138-4"><a href="embeddings.html#cb138-4" aria-hidden="true" tabindex="-1"></a>      <span class="sc">~</span> {</span>
<span id="cb138-5"><a href="embeddings.html#cb138-5" aria-hidden="true" tabindex="-1"></a>        y <span class="ot">&lt;-</span> .[<span class="fu">rep</span>(token, <span class="fu">nrow</span>(.)), ]</span>
<span id="cb138-6"><a href="embeddings.html#cb138-6" aria-hidden="true" tabindex="-1"></a>        res <span class="ot">&lt;-</span> <span class="fu">rowSums</span>(. <span class="sc">*</span> y) <span class="sc">/</span> </span>
<span id="cb138-7"><a href="embeddings.html#cb138-7" aria-hidden="true" tabindex="-1"></a>          (<span class="fu">sqrt</span>(<span class="fu">rowSums</span>(. <span class="sc">^</span> <span class="dv">2</span>)) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(.[token, ] <span class="sc">^</span> <span class="dv">2</span>)))</span>
<span id="cb138-8"><a href="embeddings.html#cb138-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">matrix</span>(res, <span class="at">ncol =</span> <span class="dv">1</span>, <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="at">x =</span> <span class="fu">names</span>(res)))</span>
<span id="cb138-9"><a href="embeddings.html#cb138-9" aria-hidden="true" tabindex="-1"></a>      },</span>
<span id="cb138-10"><a href="embeddings.html#cb138-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">sort =</span> <span class="cn">TRUE</span>,</span>
<span id="cb138-11"><a href="embeddings.html#cb138-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">maximum_size =</span> <span class="cn">NULL</span></span>
<span id="cb138-12"><a href="embeddings.html#cb138-12" aria-hidden="true" tabindex="-1"></a>    )(item1, dimension, value) <span class="sc">%&gt;%</span></span>
<span id="cb138-13"><a href="embeddings.html#cb138-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span>item2)</span>
<span id="cb138-14"><a href="embeddings.html#cb138-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Pre-trained word embeddings are trained on very large, general purpose English language data sets. Commonly used <a href="https://code.google.com/archive/p/word2vec/">word2vec embeddings</a> are based on the Google News data set, and  <a href="https://nlp.stanford.edu/projects/glove/">GloVe embeddings</a> (what we are using here) and <a href="https://fasttext.cc/docs/en/english-vectors.html">FastText embeddings</a> are learned from the text of Wikipedia plus other sources. Keeping that in mind, what words are closest to <code>"error"</code> in the GloVe embeddings?</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="embeddings.html#cb139-1" aria-hidden="true" tabindex="-1"></a>tidy_glove <span class="sc">%&gt;%</span></span>
<span id="cb139-2"><a href="embeddings.html#cb139-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nearest_neighbors</span>(<span class="st">&quot;error&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 400,000 × 2
#&gt;    item1       value
#&gt;    &lt;chr&gt;       &lt;dbl&gt;
#&gt;  1 error       1    
#&gt;  2 errors      0.792
#&gt;  3 mistake     0.664
#&gt;  4 correct     0.621
#&gt;  5 incorrect   0.613
#&gt;  6 fault       0.607
#&gt;  7 difference  0.594
#&gt;  8 mistakes    0.586
#&gt;  9 calculation 0.584
#&gt; 10 probability 0.583
#&gt; # … with 399,990 more rows</code></pre>
<p>Instead of the more specific clerical mistakes and discrepancies in the CFPB embeddings, we now see more general words about being correct or incorrect. This could present a challenge for using the GloVe embeddings with the CFPB text data. Remember that different senses or uses of the same word are conflated in word embeddings; the high-dimensional space of any set of word embeddings cannot distinguish between different uses of a word, such as the word “error.”</p>
<p>What is closest to the word <code>"month"</code> in these pre-trained GloVe embeddings?</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="embeddings.html#cb141-1" aria-hidden="true" tabindex="-1"></a>tidy_glove <span class="sc">%&gt;%</span></span>
<span id="cb141-2"><a href="embeddings.html#cb141-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nearest_neighbors</span>(<span class="st">&quot;month&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 400,000 × 2
#&gt;    item1    value
#&gt;    &lt;chr&gt;    &lt;dbl&gt;
#&gt;  1 month    1    
#&gt;  2 week     0.939
#&gt;  3 last     0.924
#&gt;  4 months   0.898
#&gt;  5 year     0.893
#&gt;  6 weeks    0.865
#&gt;  7 earlier  0.859
#&gt;  8 tuesday  0.846
#&gt;  9 ago      0.844
#&gt; 10 thursday 0.841
#&gt; # … with 399,990 more rows</code></pre>
<p>Instead of words about payments, the GloVe results here focus on different time periods only.</p>
<p>What words are closest in the GloVe embedding space to <code>"fee"</code>?</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="embeddings.html#cb143-1" aria-hidden="true" tabindex="-1"></a>tidy_glove <span class="sc">%&gt;%</span></span>
<span id="cb143-2"><a href="embeddings.html#cb143-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nearest_neighbors</span>(<span class="st">&quot;fee&quot;</span>)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 400,000 × 2
#&gt;    item1        value
#&gt;    &lt;chr&gt;        &lt;dbl&gt;
#&gt;  1 fee          1    
#&gt;  2 fees         0.832
#&gt;  3 payment      0.741
#&gt;  4 pay          0.711
#&gt;  5 salary       0.700
#&gt;  6 paid         0.668
#&gt;  7 payments     0.653
#&gt;  8 subscription 0.647
#&gt;  9 paying       0.623
#&gt; 10 expenses     0.619
#&gt; # … with 399,990 more rows</code></pre>
<p>The most similar words are, like with the CPFB embeddings, generally financial, but they are largely about salary and pay instead of about charges and overdrafts.</p>
<p></p>
<div class="rmdnote">
<p>
These examples highlight how pre-trained word embeddings can be useful because of the incredibly rich semantic relationships they encode, but also how these vector representations are often less than ideal for specific tasks.
</p>
</div>
<p>If we do choose to use pre-trained word embeddings, how do we go about integrating them into a modeling workflow? Again, we can create simple document embeddings by treating each document as a collection of words and summarizing the word embeddings. The GloVe embeddings do not contain all the tokens in the CPFB complaints, and vice versa, so let’s use <code>inner_join()</code> to match up our data sets.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="embeddings.html#cb145-1" aria-hidden="true" tabindex="-1"></a>word_matrix <span class="ot">&lt;-</span> tidy_complaints <span class="sc">%&gt;%</span></span>
<span id="cb145-2"><a href="embeddings.html#cb145-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(<span class="at">by =</span> <span class="st">&quot;word&quot;</span>,</span>
<span id="cb145-3"><a href="embeddings.html#cb145-3" aria-hidden="true" tabindex="-1"></a>             tidy_glove <span class="sc">%&gt;%</span></span>
<span id="cb145-4"><a href="embeddings.html#cb145-4" aria-hidden="true" tabindex="-1"></a>               <span class="fu">distinct</span>(item1) <span class="sc">%&gt;%</span></span>
<span id="cb145-5"><a href="embeddings.html#cb145-5" aria-hidden="true" tabindex="-1"></a>               <span class="fu">rename</span>(<span class="at">word =</span> item1)) <span class="sc">%&gt;%</span></span>
<span id="cb145-6"><a href="embeddings.html#cb145-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(complaint_id, word) <span class="sc">%&gt;%</span></span>
<span id="cb145-7"><a href="embeddings.html#cb145-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_sparse</span>(complaint_id, word, n)</span>
<span id="cb145-8"><a href="embeddings.html#cb145-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-9"><a href="embeddings.html#cb145-9" aria-hidden="true" tabindex="-1"></a>glove_matrix <span class="ot">&lt;-</span> tidy_glove <span class="sc">%&gt;%</span></span>
<span id="cb145-10"><a href="embeddings.html#cb145-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">inner_join</span>(<span class="at">by =</span> <span class="st">&quot;item1&quot;</span>,</span>
<span id="cb145-11"><a href="embeddings.html#cb145-11" aria-hidden="true" tabindex="-1"></a>             tidy_complaints <span class="sc">%&gt;%</span></span>
<span id="cb145-12"><a href="embeddings.html#cb145-12" aria-hidden="true" tabindex="-1"></a>               <span class="fu">distinct</span>(word) <span class="sc">%&gt;%</span></span>
<span id="cb145-13"><a href="embeddings.html#cb145-13" aria-hidden="true" tabindex="-1"></a>               <span class="fu">rename</span>(<span class="at">item1 =</span> word)) <span class="sc">%&gt;%</span></span>
<span id="cb145-14"><a href="embeddings.html#cb145-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cast_sparse</span>(item1, dimension, value)</span>
<span id="cb145-15"><a href="embeddings.html#cb145-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-16"><a href="embeddings.html#cb145-16" aria-hidden="true" tabindex="-1"></a>doc_matrix <span class="ot">&lt;-</span> word_matrix <span class="sc">%*%</span> glove_matrix</span>
<span id="cb145-17"><a href="embeddings.html#cb145-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-18"><a href="embeddings.html#cb145-18" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(doc_matrix)</span></code></pre></div>
<pre><code>#&gt; [1] 117163    100</code></pre>
<p>Since these GloVe embeddings had the same number of dimensions as the word embeddings we found ourselves (100), we end up with the same number of columns as before but with slightly fewer documents in the data set. We have lost documents that contain only words not included in the GloVe embeddings.</p>

<div class="rmdpackage">
The package <strong>wordsalad</strong> <span class="citation">(<a href="#ref-R-wordsalad" role="doc-biblioref">Hvitfeldt 2020c</a>)</span> provides a unified interface for finding different kinds of word vectors from text using pre-trained embeddings. The options include fastText, GloVe, and word2vec.
</div>
<p>
</p>
</div>
<div id="fairnessembeddings" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Fairness and word embeddings</h2>
<p>Perhaps more than any of the other preprocessing steps this book has covered so far, using word embeddings opens an analysis or model up to the possibility of being influenced by systemic unfairness and bias.</p>

<div class="rmdwarning">
<p>
Embeddings are trained or learned from a large corpus of text data, and whatever human prejudice or bias exists in the corpus becomes imprinted into the vector data of the embeddings.
</p>
</div>
<p>This is true of all machine learning to some extent (models learn, reproduce, and often amplify whatever biases exist in training data), but this is literally, concretely true of word embeddings. <span class="citation"><a href="#ref-Caliskan2016" role="doc-biblioref">Caliskan, Bryson, and Narayanan</a> (<a href="#ref-Caliskan2016" role="doc-biblioref">2017</a>)</span> show how the GloVe word embeddings (the same embeddings we used in Section <a href="embeddings.html#glove">5.4</a>) replicate human-like semantic biases.</p>
<ul>
<li><p>Typically Black first names are associated with more unpleasant feelings than typically white first names.</p></li>
<li><p>Women’s first names are more associated with family and men’s first names are more associated with career.</p></li>
<li><p>Terms associated with women are more associated with the arts and terms associated with men are more associated with science.</p></li>
</ul>
<p>Results like these have been confirmed over and over again, such as when <span class="citation"><a href="#ref-Bolukbasi2016" role="doc-biblioref">Bolukbasi et al.</a> (<a href="#ref-Bolukbasi2016" role="doc-biblioref">2016</a>)</span> demonstrated gender stereotypes in how word embeddings encode professions or when Google Translate <a href="https://twitter.com/seyyedreza/status/935291317252493312">exhibited apparently sexist behavior when translating text from languages with no gendered pronouns</a>. Google has since <a href="https://www.blog.google/products/translate/reducing-gender-bias-google-translate/">worked to correct this problem</a>, but in 2021 the problem <a href="https://twitter.com/doravargha/status/1373211762108076034">still exists for some languages</a>. <span class="citation"><a href="#ref-Garg2018" role="doc-biblioref">Garg et al.</a> (<a href="#ref-Garg2018" role="doc-biblioref">2018</a>)</span> even used the way bias and stereotypes can be found in word embeddings to quantify how social attitudes towards women and minorities have changed over time.</p>
<p>Remember that word embeddings are <em>learned</em> or trained from some large data set of text; this training data is the source of the biases we observe when applying word embeddings to NLP tasks. <span class="citation"><a href="#ref-Bender2021" role="doc-biblioref">Bender et al.</a> (<a href="#ref-Bender2021" role="doc-biblioref">2021</a>)</span> outline how the very large data sets used in large language models do not mean that such models reflect representative or diverse viewpoints, or even can respond to changing social views. As one concrete example, a common data set used to train large embedding models is the text of Wikipedia, but Wikipedia <a href="https://en.wikipedia.org/wiki/Gender_bias_on_Wikipedia">itself has problems with, for example, gender bias</a>. Some of the gender discrepancies on Wikipedia can be attributed to social and historical factors, but some can be attributed to the site mechanics of Wikipedia itself <span class="citation">(<a href="#ref-Wagner2016" role="doc-biblioref">Wagner et al. 2016</a>)</span>.</p>
<div class="rmdnote">
<p>
It’s safe to assume that any large corpus of language will contain latent structure reflecting the biases of the people who generated that language.
</p>
</div>
<p>
</p>
<p>When embeddings with these kinds of stereotypes are used as a preprocessing step in training a predictive model, the final model can exhibit racist, sexist, or otherwise biased characteristics. <span class="citation"><a href="#ref-Speer2017" role="doc-biblioref">Speer</a> (<a href="#ref-Speer2017" role="doc-biblioref">2017</a>)</span> demonstrated how using pre-trained word embeddings to train a straightforward sentiment analysis model can result in text such as</p>
<blockquote>
<p>Let’s go get Italian food</p>
</blockquote>
<p>being scored much more positively than text such as</p>
<blockquote>
<p>Let’s go get Mexican food</p>
</blockquote>
<p>because of characteristics of the text the word embeddings were trained on.</p>
</div>
<div id="using-word-embeddings-in-the-real-world" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Using word embeddings in the real world</h2>
<p>Given these profound and fundamental challenges with word embeddings, what options are out there? First, consider not using word embeddings when building a text model. Depending on the particular analytical question you are trying to answer, another numerical representation of text data (such as word frequencies or tf-idf of single words or n-grams) may be more appropriate. Consider this option even more seriously if the model you want to train is already entangled with issues of bias, such as the sentiment analysis example in Section <a href="embeddings.html#fairnessembeddings">5.5</a>.</p>
<p>Consider whether finding your own word embeddings, instead of relying on pre-trained embeddings created using an algorithm like GloVe or word2vec, may help you. Building your own vectors is likely to be a good option when the text domain you are working in is <em>specific</em> rather than general purpose; some examples of such domains could include customer feedback for a clothing e-commerce site, comments posted on a coding Q&amp;A site, or legal documents.</p>
<p>Learning good quality word embeddings is only realistic when you have a large corpus of text data (say, a million tokens), but if you have that much data, it is possible that embeddings learned from scratch based on your own data may not exhibit the same kind of semantic biases that exist in pre-trained word embeddings. Almost certainly there will be some kind of bias latent in any large text corpus, but when you use your own training data for learning word embeddings, you avoid the problem of <em>adding</em> historic, systemic prejudice from general purpose language data sets.</p>
<div class="rmdnote">
<p>
You can use the same approaches discussed in this chapter to check any new embeddings for dangerous biases such as racism or sexism.
</p>
</div>
<p></p>
<p>NLP researchers have also proposed methods for debiasing embeddings. <span class="citation"><a href="#ref-Bolukbasi2016" role="doc-biblioref">Bolukbasi et al.</a> (<a href="#ref-Bolukbasi2016" role="doc-biblioref">2016</a>)</span> aim to remove stereotypes by postprocessing pre-trained word vectors, choosing specific sets of words that are reprojected in the vector space so that some specific bias, such as gender bias, is mitigated. This is the most established method for reducing bias in embeddings to date, although other methods have been proposed as well, such as augmenting data with counterfactuals <span class="citation">(<a href="#ref-Lu2018" role="doc-biblioref">Lu et al. 2020</a>)</span>. Recent work <span class="citation">(<a href="#ref-Ethayarajh2019" role="doc-biblioref">Ethayarajh, Duvenaud, and Hirst 2019</a>)</span> has explored whether the association tests used to measure bias are even useful, and under what conditions debiasing can be effective.</p>
<p>Other researchers, such as <span class="citation"><a href="#ref-Caliskan2016" role="doc-biblioref">Caliskan, Bryson, and Narayanan</a> (<a href="#ref-Caliskan2016" role="doc-biblioref">2017</a>)</span>, suggest that corrections for fairness should happen at the point of <em>decision</em> or action rather than earlier in the process of modeling, such as preprocessing steps like building word embeddings. The concern is that methods for debiasing word embeddings may allow the stereotypes to seep back in, and more recent work shows that this is exactly what can happen. <span class="citation"><a href="#ref-Gonen2019" role="doc-biblioref">Gonen and Goldberg</a> (<a href="#ref-Gonen2019" role="doc-biblioref">2019</a>)</span> highlight how pervasive and consistent gender bias is across different word embedding models, <em>even after</em> applying current debiasing methods.</p>
</div>
<div id="embeddingssummary" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Summary</h2>
<p>Mapping words (or other tokens) to an embedding in a special vector space is a powerful approach in natural language processing. This chapter started from fundamentals to demonstrate how to determine word embeddings from a text data set, but a whole host of highly sophisticated techniques have been built on this foundation. For example, document embeddings can be learned from text directly <span class="citation">(<a href="#ref-Le2014" role="doc-biblioref">Le and Mikolov 2014</a>)</span> rather than summarized from word embeddings. More recently, embeddings have acted as one part of language models with transformers like ULMFiT <span class="citation">(<a href="#ref-Howard2018" role="doc-biblioref">Howard and Ruder 2018</a>)</span> and ELMo <span class="citation">(<a href="#ref-Peters2018" role="doc-biblioref">Peters et al. 2018</a>)</span>. It’s important to keep in mind that even more advanced natural language algorithms, such as these language models with transformers, also exhibit such systemic biases <span class="citation">(<a href="#ref-Sheng2019" role="doc-biblioref">Sheng et al. 2019</a>)</span>.</p>
<div id="in-this-chapter-you-learned-4" class="section level3" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> In this chapter, you learned:</h3>
<ul>
<li><p>what word embeddings are and why we use them</p></li>
<li><p>how to determine word embeddings from a text data set</p></li>
<li><p>how the vector space of word embeddings encodes word similarity</p></li>
<li><p>about a simple strategy to find document similarity</p></li>
<li><p>how to handle pre-trained word embeddings</p></li>
<li><p>why word embeddings carry historic and systemic bias</p></li>
<li><p>about approaches for debiasing word embeddings</p></li>
</ul>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Bender2021" class="csl-entry">
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. 2021. <span>“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.”</span> In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 610–623. FAccT ’21. New York, NY: Association for Computing Machinery. <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>.
</div>
<div id="ref-Bojanowski2016" class="csl-entry">
Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. 2017. <span>“Enriching Word Vectors with Subword Information.”</span> <em>Transactions of the Association for Computational Linguistics</em> 5: 135–146. <a href="https://www.aclweb.org/anthology/Q17-1010">https://www.aclweb.org/anthology/Q17-1010</a>.
</div>
<div id="ref-Bolukbasi2016" class="csl-entry">
Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. 2016. <span>“Quantifying and Reducing Stereotypes in Word Embeddings.”</span> <em>CoRR</em> abs/1606.06121. <a href="http://arxiv.org/abs/1606.06121">http://arxiv.org/abs/1606.06121</a>.
</div>
<div id="ref-Caliskan2016" class="csl-entry">
Caliskan, A., Bryson, J. J., and Narayanan, A. 2017. <span>“Semantics Derived Automatically from Language Corpora Contain Human-Like Biases.”</span> <em>Science</em> 356 (6334). American Association for the Advancement of Science: 183–186. <a href="https://science.sciencemag.org/content/356/6334/183">https://science.sciencemag.org/content/356/6334/183</a>.
</div>
<div id="ref-Ethayarajh2019" class="csl-entry">
Ethayarajh, K., Duvenaud, D., and Hirst, G. 2019. <span>“Understanding Undesirable Word Embedding Associations.”</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 1696–1705. Florence, Italy: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/P19-1166">https://www.aclweb.org/anthology/P19-1166</a>.
</div>
<div id="ref-Friedman2010" class="csl-entry">
Friedman, J. H., Hastie, T., and Tibshirani, R. 2010. <span>“Regularization Paths for Generalized Linear Models via Coordinate Descent.”</span> <em>Journal of Statistical Software, Articles</em> 33 (1): 1–22. <a href="https://www.jstatsoft.org/v033/i01">https://www.jstatsoft.org/v033/i01</a>.
</div>
<div id="ref-Garg2018" class="csl-entry">
Garg, N., Schiebinger, L., Jurafsky, D., and Zou, J. 2018. <span>“Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes.”</span> <em>Proceedings of the National Academy of Sciences</em> 115 (16). National Academy of Sciences: E3635–E3644. <a href="https://www.pnas.org/content/115/16/E3635">https://www.pnas.org/content/115/16/E3635</a>.
</div>
<div id="ref-Golub1970" class="csl-entry">
Golub, G. H., and Reinsch, C. 1970. <span>“Singular Value Decomposition and Least Squares Solutions.”</span> <em>Numerische Mathematik</em> 14 (5). Berlin, Heidelberg: Springer-Verlag: 403–420. <a href="https://doi.org/10.1007/BF02163027">https://doi.org/10.1007/BF02163027</a>.
</div>
<div id="ref-Gonen2019" class="csl-entry">
Gonen, H., and Goldberg, Y. 2019. <span>“Lipstick on a Pig: <span>D</span>ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings but Do Not Remove Them.”</span> In <em>Proceedings of the 2019 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 609–614. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/N19-1061">https://www.aclweb.org/anthology/N19-1061</a>.
</div>
<div id="ref-Howard2018" class="csl-entry">
Howard, J., and Ruder, S. 2018. <span>“Universal Language Model Fine-Tuning for Text Classification.”</span> In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 328–339. Melbourne, Australia: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/P18-1031">https://www.aclweb.org/anthology/P18-1031</a>.
</div>
<div id="ref-Hvitfeldt2020" class="csl-entry">
Hvitfeldt, E. 2020b. <em><span class="nocase">textdata</span>: Download and Load Various Text Datasets</em>. R package version 0.4.1. <a href="https://CRAN.R-project.org/package=textdata">https://CRAN.R-project.org/package=textdata</a>.
</div>
<div id="ref-R-wordsalad" class="csl-entry">
Hvitfeldt, E. 2020c. <em><span class="nocase">wordsalad</span>: Provide Tools to Extract and Analyze Word Vectors</em>. R package version 0.2.0. <a href="https://CRAN.R-project.org/package=wordsalad">https://CRAN.R-project.org/package=wordsalad</a>.
</div>
<div id="ref-Le2014" class="csl-entry">
Le, Q., and Mikolov, T. 2014. <span>“Distributed Representations of Sentences and Documents.”</span> In <em>Proceedings of the 31st International Conference on Machine Learning</em>, edited by Eric P. Xing and Tony Jebara, 32:1188–1196. Proceedings of Machine Learning Research 2. Bejing, China: PMLR. <a href="http://proceedings.mlr.press/v32/le14.html">http://proceedings.mlr.press/v32/le14.html</a>.
</div>
<div id="ref-Levy2014" class="csl-entry">
Levy, O., and Goldberg, Y. 2014. <span>“Dependency-Based Word Embeddings.”</span> In <em>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, 302–308. Baltimore, Maryland: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/P14-2050">https://www.aclweb.org/anthology/P14-2050</a>.
</div>
<div id="ref-Lu2018" class="csl-entry">
Lu, K., Mardziel, P., Wu, F., Amancharla, P., and Datta, A. 2020. <span>“Gender Bias in Neural Natural Language Processing.”</span> In <em>Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday</em>, edited by Vivek Nigam, Tajana Ban Kirigin, Carolyn Talcott, Joshua Guttman, Stepan Kuznetsov, Boon Thau Loo, and Mitsuhiro Okada, 189–202. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-62077-6_14">https://doi.org/10.1007/978-3-030-62077-6_14</a>.
</div>
<div id="ref-Mikolov2013" class="csl-entry">
Mikolov, T., Chen, K., Corrado, G. S., and Dean, J. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-Moody2017" class="csl-entry">
Moody, C. 2017. <span>“Stop Using <span class="nocase">word2vec</span>.”</span> <em>Multithreaded</em>. StitchFix. <a href="https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/">https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/</a>.
</div>
<div id="ref-Pennington2014" class="csl-entry">
Pennington, J., Socher, R., and Manning, C. 2014. <span>“<span>G</span>lo<span>V</span>e: Global Vectors for Word Representation.”</span> In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (<span>EMNLP</span>)</em>, 1532–1543. Doha, Qatar: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/D14-1162">https://www.aclweb.org/anthology/D14-1162</a>.
</div>
<div id="ref-Peters2018" class="csl-entry">
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. <span>“Deep Contextualized Word Representations.”</span> In <em>Proceedings of the 2018 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, 2227–2237. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/N18-1202">https://www.aclweb.org/anthology/N18-1202</a>.
</div>
<div id="ref-R-widyr" class="csl-entry">
Robinson, D. 2020. <em><span class="nocase">widyr</span>: Widen, Process, Then Re-Tidy Data</em>. R package version 0.1.3. <a href="https://CRAN.R-project.org/package=widyr">https://CRAN.R-project.org/package=widyr</a>.
</div>
<div id="ref-Selivanov2018" class="csl-entry">
Selivanov, D., Bickel, M., and Wang, Q. 2020. <em><span class="nocase">text2vec</span>: Modern Text Mining Framework for <span>R</span></em>. R package version 0.6. <a href="https://CRAN.R-project.org/package=text2vec">https://CRAN.R-project.org/package=text2vec</a>.
</div>
<div id="ref-Sheng2019" class="csl-entry">
Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. 2019. <span>“The Woman Worked as a Babysitter: On Biases in Language Generation.”</span> In <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 3407–3412. Hong Kong: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/D19-1339">https://www.aclweb.org/anthology/D19-1339</a>.
</div>
<div id="ref-Speer2017" class="csl-entry">
Speer, R. 2017. <span>“How to Make a Racist <span>AI</span> Without Really Trying.”</span> <em>ConceptNet Blog</em>. <a href="http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/">http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/</a>.
</div>
<div id="ref-Vaughan2020" class="csl-entry">
Vaughan, D. 2021a. <em><span class="nocase">slider</span>: Sliding Window Functions</em>. R package version 0.2.1. <a href="https://CRAN.R-project.org/package=slider">https://CRAN.R-project.org/package=slider</a>.
</div>
<div id="ref-Vaughan2018" class="csl-entry">
Vaughan, D., and Dancho, M. 2021. <em><span class="nocase">furrr</span>: Apply Mapping Functions in Parallel Using Futures</em>. R package version 0.2.2. <a href="https://CRAN.R-project.org/package=furrr">https://CRAN.R-project.org/package=furrr</a>.
</div>
<div id="ref-Wagner2016" class="csl-entry">
Wagner, C., Graells-Garrido, E., Garcia, D., and Menczer, F. 2016. <span>“Women Through the Glass Ceiling: Gender Asymmetries in <span>W</span>ikipedia.”</span> <em>EPJ Data Science</em> 5 (1). SpringerOpen: 5. <a href="https://doi.org/10.1140/epjds/s13688-016-0066-4">https://doi.org/10.1140/epjds/s13688-016-0066-4</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="stemming.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mloverview.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/EmilHvitfeldt/smltar/edit/master/05_word_embeddings.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
